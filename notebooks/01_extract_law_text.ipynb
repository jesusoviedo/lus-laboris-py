{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dad38a4e-4836-417b-9f58-3a566961f189",
   "metadata": {},
   "source": [
    "# Downloading the Law HTML Page / Descargando la p√°gina HTML de la ley\n",
    "\n",
    "**Goal**: Download the Paraguay Labor Code HTML page and save it locally for further processing.  \n",
    "\n",
    "**Input**: The URL of the law page on the official government website.  \n",
    "\n",
    "**Approach**:  \n",
    "- Use the `requests` library to perform an HTTP GET request.  \n",
    "- Ensure the request is successful and handle possible errors.  \n",
    "- Create the target folder if it does not exist.  \n",
    "- Save the HTML content to a file at `../data/raw/codigo_trabajo_py.html`.\n",
    "\n",
    "**Output**: Local HTML file containing the law text.  \n",
    "\n",
    "**Why this matters**: Having a local copy ensures consistent processing and avoids repeated web requests, which is especially useful for parsing, indexing, or text analysis tasks.\n",
    "\n",
    "---\n",
    "\n",
    "**Objetivo**: Descargar la p√°gina HTML del C√≥digo Laboral de Paraguay y guardarla localmente para su posterior procesamiento.  \n",
    "\n",
    "**Entrada**: URL de la p√°gina de la ley en el sitio oficial del gobierno.  \n",
    "\n",
    "**Enfoque**:  \n",
    "- Usar la librer√≠a `requests` para hacer una petici√≥n HTTP GET.  \n",
    "- Verificar que la petici√≥n sea exitosa y manejar posibles errores.  \n",
    "- Crear la carpeta de destino si no existe.  \n",
    "- Guardar el contenido HTML en un archivo en `../data/raw/codigo_trabajo_py.html`.\n",
    "\n",
    "**Salida**: Archivo HTML local que contiene el texto de la ley.  \n",
    "\n",
    "**Por qu√© importa**: Tener una copia local garantiza un procesamiento consistente y evita solicitudes web repetidas, √∫til para parsing, indexaci√≥n o an√°lisis de texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f7f5989-0929-47bb-b755-6b5a585b6587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P√°gina descargada y guardada en: ../data/raw/codigo_trabajo_py.html\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def download_law_page(url, output_path=\"../data/raw/codigo_trabajo_py.html\"):\n",
    "    out_path = Path(output_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Lanza excepci√≥n si hay error en la descarga\n",
    "\n",
    "    # Guardar contenido en archivo\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "\n",
    "url = \"https://www.bacn.gov.py/leyes-paraguayas/2608/ley-n-213-establece-el-codigo-del-trabajo\"\n",
    "download_law_page(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c266d32e",
   "metadata": {},
   "source": [
    "# Transformation of downloaded data / Transformaci√≥n de los datos descargados\n",
    "\n",
    "**Goal**: Extract the Paraguay Labor Code text from a local HTML file (`../data/raw/codigo_trabajo_py.html`) for further processing, such as cleaning, indexing, or search.  \n",
    "\n",
    "**Input**: Local HTML file saved from the source website.  \n",
    "\n",
    "**Approach**:  \n",
    "- Open the file using a context manager with the selected encoding.  \n",
    "- Parse the HTML with BeautifulSoup using the built-in `html.parser`.  \n",
    "- Locate the main container `<div class=\"entry-content\">`.  \n",
    "- Extract the text while preserving line breaks.\n",
    "\n",
    "**Output**: Clean plain text in memory.  \n",
    "\n",
    "**Why this matters**: Ensures that the extracted text is complete and readable, making it suitable for downstream tasks like indexing, search, or QA.\n",
    "\n",
    "**File path and encoding**\n",
    "\n",
    "**Path**: `../data/raw/codigo_trabajo_py.html`. Adjust if your working directory changes.  \n",
    "\n",
    "**Encoding**: The file is likely in Latin-1 (`encoding='latin-1'`). If you notice encoding artifacts such as `√É¬≥`, try `utf-8` or `cp1252`.  \n",
    "\n",
    "**Reading the HTML**\n",
    "\n",
    "Use a context manager to safely open and read the file.  \n",
    "\n",
    "**Common pitfalls**:  \n",
    "- FileNotFoundError if the path is wrong.  \n",
    "- UnicodeDecodeError if the encoding does not match the file.\n",
    "\n",
    "**Parse and select content**\n",
    "\n",
    "**Parser**: `html.parser` is built-in and sufficient for this HTML.  \n",
    "\n",
    "**Target container**: The law text is inside `<div class=\"entry-content\">`, selected with `soup.find('div', class_='entry-content')`.  \n",
    "\n",
    "**Fallbacks**: If `None` is returned, inspect the DOM for alternative classes or IDs.\n",
    "\n",
    "**Extract text cleanly**\n",
    "\n",
    "Use `get_text(separator='\\n', strip=True)` to flatten the HTML while preserving line breaks.  \n",
    "\n",
    "**Tips**: Adjust `separator` for paragraph spacing or post-process the text to normalize multiple newlines or bullet points.\n",
    "\n",
    "---\n",
    "\n",
    "**Objetivo**: Extraer el texto del C√≥digo Laboral de Paraguay desde un archivo HTML local (`../data/raw/codigo_trabajo_py.html`) para procesamiento posterior, como limpieza, indexaci√≥n o b√∫squeda.  \n",
    "\n",
    "**Entrada**: Archivo HTML local guardado desde el sitio de origen.  \n",
    "\n",
    "**Enfoque**:  \n",
    "- Abrir el archivo con un gestor de contexto usando la codificaci√≥n correcta.  \n",
    "- Parsear el HTML con BeautifulSoup (`html.parser`).  \n",
    "- Localizar el contenedor principal `<div class=\"entry-content\">`.  \n",
    "- Extraer el texto preservando los saltos de l√≠nea.\n",
    "\n",
    "**Salida**: Texto plano limpio en memoria.  \n",
    "\n",
    "**Por qu√© importa**: Garantiza que el texto extra√≠do est√© completo y sea legible, apto para tareas posteriores como indexaci√≥n, b√∫squeda o QA.\n",
    "\n",
    "**Ruta y codificaci√≥n**\n",
    "\n",
    "**Ruta**: `../data/raw/codigo_trabajo_py.html`. Ajusta si cambia tu directorio de trabajo.  \n",
    "\n",
    "**Codificaci√≥n**: Probablemente el archivo est√° en Latin-1 (`encoding='latin-1'`). Si aparecen artefactos como `√É¬≥`, prueba con `utf-8` o `cp1252`.\n",
    "\n",
    "**Lectura del HTML**\n",
    "\n",
    "Usa un gestor de contexto para abrir y leer el archivo de forma segura.  \n",
    "\n",
    "**Errores comunes**:  \n",
    "- FileNotFoundError si la ruta es incorrecta.  \n",
    "- UnicodeDecodeError si la codificaci√≥n no coincide con el archivo.\n",
    "\n",
    "**Parseo y selecci√≥n de contenido**\n",
    "\n",
    "**Parser**: `html.parser` es integrado y suficiente para este HTML.  \n",
    "\n",
    "**Contenedor objetivo**: El texto de la ley est√° dentro de `<div class=\"entry-content\">`, seleccionado con `soup.find('div', class_='entry-content')`.  \n",
    "**Alternativas**: Si retorna `None`, inspecciona el DOM para otras clases o IDs posibles.\n",
    "\n",
    "**Extracci√≥n de texto limpio**\n",
    "\n",
    "Usa `get_text(separator='\\n', strip=True)` para aplanar el HTML preservando los saltos de l√≠nea.  \n",
    "\n",
    "**Consejos**: Ajusta `separator` para espaciar p√°rrafos o post-procesa el texto para normalizar saltos de l√≠nea m√∫ltiples o vi√±etas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eec1144-b275-4470-a7ea-1b5cdf2d660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from ftfy import fix_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80fc327e-667b-497b-8a3a-d50b5d86a657",
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_archivo_html = \"../data/raw/codigo_trabajo_py.html\"\n",
    "\n",
    "with open(nombre_archivo_html, encoding=\"latin-1\") as archivo:\n",
    "    contenido_html = archivo.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07d9a6e4-c2d0-4229-9b5f-1321119ac780",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(contenido_html, \"html.parser\")\n",
    "contenido_ley = soup.find(\"div\", class_=\"entry-content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "458873e2-e9b6-4ec9-b803-6a4a1e63f7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Contenido de la Ley extra√≠do exitosamente ---\n"
     ]
    }
   ],
   "source": [
    "if contenido_ley:\n",
    "    texto_limpio = contenido_ley.get_text(separator=\"\\n\", strip=True)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b9bbd9b-0830-49d9-a396-73fe71157116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(texto_limpio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6140f33-684d-4446-8f2e-4cd90e706df9",
   "metadata": {},
   "source": [
    "# Header and Article Patterns / Patrones de Encabezados y Art√≠culos\n",
    "\n",
    "This section defines regular expressions and mappings to identify different parts of the legal text:\n",
    "\n",
    "- `HEADER_PATTERNS`: A dictionary containing regex patterns for:\n",
    "  - `'libro'`: Matches \"LIBRO\" followed by its name (e.g., \"LIBRO PRIMERO\").\n",
    "  - `'titulo'`: Matches \"TITULO\" followed by its name (e.g., \"TITULO PRIMERO\").\n",
    "  - `'capitulo'`: Matches \"CAPITULO\" followed by Roman numerals (e.g., \"CAPITULO I\").\n",
    "  \n",
    "- `ARTICULO_PATTERN`: A regex to detect article headers like \"Art√≠culo 1¬∞.-\".\n",
    "\n",
    "- `ROMAN_MAP`: Maps Spanish ordinal words to integer numbers for easy conversion (e.g., \"PRIMERO\" ‚Üí 1).\n",
    "\n",
    "---\n",
    "\n",
    "Esta secci√≥n define expresiones regulares y mapeos para identificar diferentes partes del texto legal:\n",
    "\n",
    "- `HEADER_PATTERNS`: Un diccionario con patrones regex para:\n",
    "  - `'libro'`: Detecta \"LIBRO\" seguido de su nombre (ej.: \"LIBRO PRIMERO\").\n",
    "  - `'titulo'`: Detecta \"TITULO\" seguido de su nombre (ej.: \"TITULO PRIMERO\").\n",
    "  - `'capitulo'`: Detecta \"CAPITULO\" seguido de n√∫meros romanos (ej.: \"CAPITULO I\").\n",
    "  \n",
    "- `ARTICULO_PATTERN`: Regex para detectar encabezados de art√≠culos como \"Art√≠culo 1¬∞.-\".\n",
    "\n",
    "- `ROMAN_MAP`: Mapea palabras ordinales en espa√±ol a n√∫meros enteros para facilitar la conversi√≥n (ej.: \"PRIMERO\" ‚Üí 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81739ab9-d8ab-4315-8c79-453d6ad5f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER_PATTERNS = {\n",
    "    \"libro\": re.compile(r\"^LIBRO\\s+([A-Z√Å√â√ç√ì√ö√ë]+)\\s*$\", re.IGNORECASE),\n",
    "    \"titulo\": re.compile(r\"^TITULO\\s+([A-Z√Å√â√ç√ì√ö√ë]+)\\s*$\", re.IGNORECASE),\n",
    "    \"capitulo\": re.compile(r\"^CAPITULO\\s+([IVXLCDM]+)\\s*$\", re.IGNORECASE),\n",
    "}\n",
    "\n",
    "ARTICULO_PATTERN = re.compile(r\"^Art[√≠i]?t?culo\\s+(\\d+)\\s*(?:[¬∞¬∫])?\\s*\\.?\\s*-\\s*$\", re.IGNORECASE)\n",
    "\n",
    "ROMAN_MAP = {\n",
    "    \"PRIMERO\": 1,\n",
    "    \"SEGUNDO\": 2,\n",
    "    \"TERCERO\": 3,\n",
    "    \"CUARTO\": 4,\n",
    "    \"QUINTO\": 5,\n",
    "    \"SEXTO\": 6,\n",
    "    \"S√âPTIMO\": 7,\n",
    "    \"SEPTIMO\": 7,\n",
    "    \"OCTAVO\": 8,\n",
    "    \"NOVENO\": 9,\n",
    "    \"D√âCIMO\": 10,\n",
    "    \"DECIMO\": 10,\n",
    "    \"UND√âCIMO\": 11,\n",
    "    \"UNDECIMO\": 11,\n",
    "    \"DUOD√âCIMO\": 12,\n",
    "    \"DUODECIMO\": 12,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb30185-977b-425b-ad89-5281cf34bc18",
   "metadata": {},
   "source": [
    "# Roman Numeral Conversion / Conversi√≥n de N√∫meros Romanos\n",
    "\n",
    "This section defines a helper function to convert Roman numerals into integers:\n",
    "\n",
    "- `_ROMAN_VALUES`: A dictionary mapping Roman numeral characters to their integer values.\n",
    "- `roman_to_int(roman)`: Converts a Roman numeral string into an integer.\n",
    "  - The function iterates over the characters in reverse.\n",
    "  - If a smaller value precedes a larger one, it is subtracted.\n",
    "  - Otherwise, the value is added.\n",
    "\n",
    "---\n",
    "\n",
    "Esta secci√≥n define una funci√≥n auxiliar para convertir n√∫meros romanos en enteros:\n",
    "\n",
    "- `_ROMAN_VALUES`: Un diccionario que asigna valores enteros a los caracteres de n√∫meros romanos.\n",
    "- `roman_to_int(roman)`: Convierte una cadena de n√∫mero romano a un n√∫mero entero.\n",
    "  - La funci√≥n itera sobre los caracteres en orden inverso.\n",
    "  - Si un valor menor precede a un valor mayor, se resta.\n",
    "  - De lo contrario, se suma el valor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "313789e1-bab6-441b-a8dc-8acf430ae8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ROMAN_VALUES = {\"I\": 1, \"V\": 5, \"X\": 10, \"L\": 50, \"C\": 100, \"D\": 500, \"M\": 1000}\n",
    "\n",
    "\n",
    "def roman_to_int(roman):\n",
    "    roman = roman.strip().upper()\n",
    "    total = 0\n",
    "    prev = 0\n",
    "    for ch in reversed(roman):\n",
    "        val = _ROMAN_VALUES.get(ch, 0)\n",
    "        if val < prev:\n",
    "            total -= val\n",
    "        else:\n",
    "            total += val\n",
    "            prev = val\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6d9123-d81c-4bbb-86b3-e75233853baa",
   "metadata": {},
   "source": [
    "# Metadata Extraction / Extracci√≥n de Metadatos\n",
    "\n",
    "This function extracts key metadata from the legal text header, before the first chapter:\n",
    "\n",
    "- `extract_metadata(lines)`: Receives a list of text lines.\n",
    "  - Constructs the `encabezado` (header) until the first \"CAP√çTULO I\".\n",
    "  - Searches within this header for:\n",
    "    - Law number (`numero_ley`) using a regex that matches \"LEY N¬∞ ...\".\n",
    "    - Promulgation date (`fecha_promulgacion`) using a regex for \"Fecha de Promulgaci√≥n\".\n",
    "    - Publication date (`fecha_publicacion`) using a regex for \"Fecha de Publicaci√≥n\".\n",
    "  - Returns a dictionary `meta` containing these values.\n",
    "\n",
    "---\n",
    "\n",
    "Esta funci√≥n extrae metadatos clave del encabezado del texto legal, antes del primer cap√≠tulo:\n",
    "\n",
    "- `extract_metadata(lines)`: Recibe una lista de l√≠neas de texto.\n",
    "  - Construye el `encabezado` hasta el primer \"CAP√çTULO I\".\n",
    "  - Busca dentro de este encabezado:\n",
    "    - N√∫mero de ley (`numero_ley`) usando una expresi√≥n regular que detecta \"LEY N¬∞ ...\".\n",
    "    - Fecha de promulgaci√≥n (`fecha_promulgacion`) usando regex para \"Fecha de Promulgaci√≥n\".\n",
    "    - Fecha de publicaci√≥n (`fecha_publicacion`) usando regex para \"Fecha de Publicaci√≥n\".\n",
    "  - Devuelve un diccionario `meta` con estos valores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d630ebe-6e8e-4d07-9c75-eee7ee0a5da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata(lines):\n",
    "    \"\"\"Extrae n√∫mero de ley y fechas desde el encabezado (antes del primer CAPITULO I).\"\"\"\n",
    "    meta = {}\n",
    "\n",
    "    encabezado = []\n",
    "    for ln in lines:\n",
    "        if re.match(r\"CAP[I√ç]TULO\\s+I\\b\", ln, re.IGNORECASE):\n",
    "            break\n",
    "        encabezado.append(ln)\n",
    "\n",
    "    encabezado_text = \" \".join(encabezado)\n",
    "\n",
    "    ley_match = re.search(r\"LEY\\s*N[¬∞¬∫]?\\s*(\\d+)\", encabezado_text, re.IGNORECASE)\n",
    "    if ley_match:\n",
    "        meta[\"numero_ley\"] = ley_match.group(1)\n",
    "\n",
    "    promulg_match = re.search(\n",
    "        r\"Fecha\\s+de\\s+Promulgaci[o√≥]n:?\\s*(\\d{2}-\\d{2}-\\d{4})\", encabezado_text, re.IGNORECASE\n",
    "    )\n",
    "    if promulg_match:\n",
    "        meta[\"fecha_promulgacion\"] = promulg_match.group(1)\n",
    "\n",
    "    public_match = re.search(\n",
    "        r\"Fecha\\s+de\\s+Publicaci[o√≥]n:?\\s*(\\d{2}-\\d{2}-\\d{4})\", encabezado_text, re.IGNORECASE\n",
    "    )\n",
    "    if public_match:\n",
    "        meta[\"fecha_publicacion\"] = public_match.group(1)\n",
    "\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaae11fc-bd55-4cef-bbec-5f2529ce779d",
   "metadata": {},
   "source": [
    "# Article Extraction / Extracci√≥n de Art√≠culos\n",
    "\n",
    "This function segments the legal text into structured parts: books, titles, chapters, and articles in detail.\n",
    "\n",
    "- `extract_articles(lines)`: Receives a list of text lines.\n",
    "  - Maintains context variables for the current book, title, chapter, and chapter description.\n",
    "  - Uses the helper function `flush_article()` to finalize and store the current article when a new header or article starts.\n",
    "  - Iterates through the lines:\n",
    "    - Detects **LIBRO** headers using `HEADER_PATTERNS['libro']`.\n",
    "    - Detects **TITULO** headers using `HEADER_PATTERNS['titulo']`.\n",
    "    - Detects **CAPITULO** headers and optionally captures the next line as chapter description.\n",
    "    - Detects article headers using `ARTICULO_PATTERN` and accumulates the article text until the next header or article.\n",
    "  - Returns a list of dictionaries, each representing an article with metadata and cleaned text.\n",
    "\n",
    "---\n",
    "\n",
    "Esta funci√≥n segmenta el texto legal en partes estructuradas: libros, t√≠tulos, cap√≠tulos y art√≠culos en detalle.\n",
    "\n",
    "- `extract_articles(lines)`: Recibe una lista de l√≠neas de texto.\n",
    "  - Mantiene variables de contexto para el libro, t√≠tulo, cap√≠tulo y descripci√≥n del cap√≠tulo actual.\n",
    "  - Utiliza la funci√≥n auxiliar `flush_article()` para finalizar y almacenar el art√≠culo actual cuando empieza un nuevo encabezado o art√≠culo.\n",
    "  - Itera sobre las l√≠neas:\n",
    "    - Detecta encabezados de **LIBRO** con `HEADER_PATTERNS['libro']`.\n",
    "    - Detecta encabezados de **TITULO** con `HEADER_PATTERNS['titulo']`.\n",
    "    - Detecta encabezados de **CAPITULO** y opcionalmente captura la siguiente l√≠nea como descripci√≥n del cap√≠tulo.\n",
    "    - Detecta encabezados de art√≠culo con `ARTICULO_PATTERN` y acumula el texto del art√≠culo hasta el siguiente encabezado o art√≠culo.\n",
    "  - Devuelve una lista de diccionarios, cada uno representando un art√≠culo con sus metadatos y texto limpio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98f02ae9-3a21-4447-849d-3edefa82a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_articles(lines):\n",
    "    \"\"\"Segmenta libros, t√≠tulos, cap√≠tulos y art√≠culos en detalle.\"\"\"\n",
    "    # Contexto de encabezados\n",
    "    current_libro = None\n",
    "    current_libro_num = None\n",
    "    current_titulo = None\n",
    "    current_capitulo = None\n",
    "    current_capitulo_num = None\n",
    "    current_capitulo_desc = None\n",
    "\n",
    "    # Segmentaci√≥n de art√≠culos\n",
    "    articles = []\n",
    "    current_article_num = None\n",
    "    current_article_lines = []\n",
    "\n",
    "    def flush_article():\n",
    "        if current_article_num is None:\n",
    "            return\n",
    "        body = \"\\n\".join(current_article_lines).strip()\n",
    "        articles.append(\n",
    "            {\n",
    "                \"articulo_numero\": int(current_article_num),\n",
    "                \"libro\": current_libro.lower() if current_libro else None,\n",
    "                \"libro_numero\": current_libro_num,\n",
    "                \"titulo\": current_titulo.lower() if current_titulo else None,\n",
    "                \"capitulo\": current_capitulo.lower() if current_capitulo else None,\n",
    "                \"capitulo_numero\": current_capitulo_num,\n",
    "                \"capitulo_descripcion\": current_capitulo_desc.lower()\n",
    "                if current_capitulo_desc\n",
    "                else None,\n",
    "                \"articulo\": body.lower().replace(\"\\n\", \"\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        ln = lines[i]\n",
    "\n",
    "        # Detectar LIBRO\n",
    "        m_lib = HEADER_PATTERNS[\"libro\"].match(ln)\n",
    "        if m_lib:\n",
    "            current_libro = f\"LIBRO {m_lib.group(1).title()}\"\n",
    "            current_libro_num = ROMAN_MAP.get(m_lib.group(1).upper())\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Detectar TITULO\n",
    "        m_tit = HEADER_PATTERNS[\"titulo\"].match(ln)\n",
    "        if m_tit:\n",
    "            current_titulo = f\"TITULO {m_tit.group(1).title()}\"\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Detectar CAPITULO\n",
    "        m_cap = HEADER_PATTERNS[\"capitulo\"].match(ln)\n",
    "        if m_cap:\n",
    "            roman = m_cap.group(1)\n",
    "            current_capitulo = f\"CAPITULO {roman}\"\n",
    "            current_capitulo_num = roman_to_int(roman)\n",
    "            next_desc = None\n",
    "            if i + 1 < len(lines):\n",
    "                nxt = lines[i + 1]\n",
    "                if not (\n",
    "                    HEADER_PATTERNS[\"libro\"].match(nxt)\n",
    "                    or HEADER_PATTERNS[\"titulo\"].match(nxt)\n",
    "                    or HEADER_PATTERNS[\"capitulo\"].match(nxt)\n",
    "                    or ARTICULO_PATTERN.match(nxt)\n",
    "                ):\n",
    "                    next_desc = nxt\n",
    "            current_capitulo_desc = next_desc\n",
    "            i += 2 if next_desc else 1\n",
    "            continue\n",
    "\n",
    "        # Detectar inicio de Art√≠culo\n",
    "        m_art = ARTICULO_PATTERN.match(ln)\n",
    "        if m_art:\n",
    "            flush_article()\n",
    "            current_article_num = m_art.group(1)\n",
    "            current_article_lines = []\n",
    "            i += 1\n",
    "            while i < len(lines):\n",
    "                nxt = lines[i]\n",
    "                if (\n",
    "                    HEADER_PATTERNS[\"libro\"].match(nxt)\n",
    "                    or HEADER_PATTERNS[\"titulo\"].match(nxt)\n",
    "                    or HEADER_PATTERNS[\"capitulo\"].match(nxt)\n",
    "                    or ARTICULO_PATTERN.match(nxt)\n",
    "                ):\n",
    "                    break\n",
    "                current_article_lines.append(nxt)\n",
    "                i += 1\n",
    "            continue\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    flush_article()\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdb1abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_processed_data(articles):\n",
    "    \"\"\"Valida la integridad y calidad de los datos procesados.\"\"\"\n",
    "    validation_results = {\n",
    "        \"total_articles\": len(articles),\n",
    "        \"valid_articles\": 0,\n",
    "        \"invalid_articles\": [],\n",
    "        \"missing_fields\": [],\n",
    "        \"quality_score\": 0.0,\n",
    "    }\n",
    "\n",
    "    required_fields = [\"articulo_numero\", \"libro\", \"capitulo\", \"articulo\"]\n",
    "\n",
    "    for article in articles:\n",
    "        article_valid = True\n",
    "        article_issues = []\n",
    "\n",
    "        # Verificar campos requeridos\n",
    "        for field in required_fields:\n",
    "            if field not in article or not article[field]:\n",
    "                article_issues.append(f\"Campo faltante: {field}\")\n",
    "                article_valid = False\n",
    "\n",
    "        # Verificar que el n√∫mero de art√≠culo sea v√°lido\n",
    "        if \"articulo_numero\" in article:\n",
    "            art_num = article[\"articulo_numero\"]\n",
    "            if not isinstance(art_num, int) or art_num < 1 or art_num > 413:\n",
    "                article_issues.append(f\"N√∫mero de art√≠culo inv√°lido: {art_num}\")\n",
    "                article_valid = False\n",
    "\n",
    "        # Verificar que el contenido no est√© vac√≠o\n",
    "        if \"articulo\" in article and len(article[\"articulo\"].strip()) < 10:\n",
    "            article_issues.append(\"Contenido del art√≠culo demasiado corto\")\n",
    "            article_valid = False\n",
    "\n",
    "        if article_valid:\n",
    "            validation_results[\"valid_articles\"] += 1\n",
    "        else:\n",
    "            validation_results[\"invalid_articles\"].append(\n",
    "                {\n",
    "                    \"articulo_numero\": article.get(\"articulo_numero\", \"desconocido\"),\n",
    "                    \"issues\": article_issues,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Calcular score de calidad\n",
    "    validation_results[\"quality_score\"] = (\n",
    "        validation_results[\"valid_articles\"] / validation_results[\"total_articles\"]\n",
    "    )\n",
    "\n",
    "    return validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9f31e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_data_completeness(articles):\n",
    "    \"\"\"Verifica que todos los art√≠culos esperados est√©n presentes.\"\"\"\n",
    "\n",
    "    article_numbers = [art[\"articulo_numero\"] for art in articles if \"articulo_numero\" in art]\n",
    "\n",
    "    # Verificar rango completo (1-413)\n",
    "    expected_range = set(range(1, 414))\n",
    "    found_numbers = set(article_numbers)\n",
    "\n",
    "    missing_articles = expected_range - found_numbers\n",
    "    duplicate_articles = [num for num in article_numbers if article_numbers.count(num) > 1]\n",
    "\n",
    "    completeness_report = {\n",
    "        \"expected_total\": 413,\n",
    "        \"found_total\": len(found_numbers),\n",
    "        \"missing_articles\": sorted(list(missing_articles)),\n",
    "        \"duplicate_articles\": duplicate_articles,\n",
    "        \"completeness_percentage\": len(found_numbers) / 413 * 100,\n",
    "    }\n",
    "\n",
    "    if missing_articles:\n",
    "        pass\n",
    "\n",
    "    if duplicate_articles:\n",
    "        pass\n",
    "\n",
    "    return completeness_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76f78c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_content_quality(articles):\n",
    "    \"\"\"Analiza la calidad del contenido extra√≠do.\"\"\"\n",
    "    quality_metrics = {\n",
    "        \"avg_content_length\": 0,\n",
    "        \"short_articles\": 0,  # < 50 caracteres\n",
    "        \"medium_articles\": 0,  # 50-200 caracteres\n",
    "        \"long_articles\": 0,  # > 200 caracteres\n",
    "        \"articles_with_special_chars\": 0,\n",
    "        \"articles_with_numbers\": 0,\n",
    "    }\n",
    "\n",
    "    content_lengths = []\n",
    "\n",
    "    for article in articles:\n",
    "        if \"articulo\" not in article:\n",
    "            continue\n",
    "\n",
    "        content = article[\"articulo\"]\n",
    "        content_length = len(content.strip())\n",
    "        content_lengths.append(content_length)\n",
    "\n",
    "        # Clasificar por longitud\n",
    "        if content_length < 50:\n",
    "            quality_metrics[\"short_articles\"] += 1\n",
    "        elif content_length <= 200:\n",
    "            quality_metrics[\"medium_articles\"] += 1\n",
    "        else:\n",
    "            quality_metrics[\"long_articles\"] += 1\n",
    "\n",
    "        # Verificar caracter√≠sticas especiales\n",
    "        if any(char in content for char in [\"¬∞\", \"¬∫\", \"¬ß\", \"¬∂\"]):\n",
    "            quality_metrics[\"articles_with_special_chars\"] += 1\n",
    "\n",
    "        if any(char.isdigit() for char in content):\n",
    "            quality_metrics[\"articles_with_numbers\"] += 1\n",
    "\n",
    "    if content_lengths:\n",
    "        quality_metrics[\"avg_content_length\"] = sum(content_lengths) / len(content_lengths)\n",
    "\n",
    "    return quality_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a00bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quality_report(articles):\n",
    "    \"\"\"Genera un reporte completo de calidad de datos.\"\"\"\n",
    "    validation_results = validate_processed_data(articles)\n",
    "    completeness_report = verify_data_completeness(articles)\n",
    "    quality_metrics = analyze_content_quality(articles)\n",
    "\n",
    "    report = f\"\"\"\n",
    "üìä REPORTE DE CALIDAD DE DATOS PROCESADOS\n",
    "{\"=\" * 50}\n",
    "\n",
    "‚úÖ VALIDACI√ìN DE ESTRUCTURA:\n",
    "   ‚Ä¢ Art√≠culos v√°lidos: {validation_results[\"valid_articles\"]}/{validation_results[\"total_articles\"]}\n",
    "   ‚Ä¢ Score de calidad: {validation_results[\"quality_score\"]:.2%}\n",
    "   ‚Ä¢ Art√≠culos con problemas: {len(validation_results[\"invalid_articles\"])}\n",
    "\n",
    "üìã COMPLETITUD DE DATOS:\n",
    "   ‚Ä¢ Art√≠culos encontrados: {completeness_report[\"found_total\"]}/413\n",
    "   ‚Ä¢ Completitud: {completeness_report[\"completeness_percentage\"]:.1f}%\n",
    "   ‚Ä¢ Art√≠culos faltantes: {len(completeness_report[\"missing_articles\"])}\n",
    "   ‚Ä¢ Art√≠culos duplicados: {len(completeness_report[\"duplicate_articles\"])}\n",
    "\n",
    "üìù AN√ÅLISIS DE CONTENIDO:\n",
    "   ‚Ä¢ Longitud promedio: {quality_metrics[\"avg_content_length\"]:.1f} caracteres\n",
    "   ‚Ä¢ Art√≠culos cortos (< 50 chars): {quality_metrics[\"short_articles\"]}\n",
    "   ‚Ä¢ Art√≠culos medianos (50-200 chars): {quality_metrics[\"medium_articles\"]}\n",
    "   ‚Ä¢ Art√≠culos largos (> 200 chars): {quality_metrics[\"long_articles\"]}\n",
    "   ‚Ä¢ Con caracteres especiales: {quality_metrics[\"articles_with_special_chars\"]}\n",
    "   ‚Ä¢ Con n√∫meros: {quality_metrics[\"articles_with_numbers\"]}\n",
    "\n",
    "üéØ ESTADO GENERAL: {\"‚úÖ EXCELENTE\" if validation_results[\"quality_score\"] > 0.95 else \"‚ö†Ô∏è REQUIERE ATENCI√ìN\"}\n",
    "\"\"\"\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fe3d70d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parsed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Generar reporte completo de calidad\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m quality_report = generate_quality_report(\u001b[43mparsed\u001b[49m[\u001b[33m'\u001b[39m\u001b[33marticulos\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(quality_report)\n",
      "\u001b[31mNameError\u001b[39m: name 'parsed' is not defined"
     ]
    }
   ],
   "source": [
    "# Generar reporte completo de calidad\n",
    "quality_report = generate_quality_report(parsed[\"articulos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891ac855-1d97-49e3-8787-97c50de70c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_articles(lines):\n",
    "    \"\"\"Segmenta libros, t√≠tulos, cap√≠tulos y art√≠culos en detalle.\"\"\"\n",
    "    # Contexto de encabezados\n",
    "    current_libro = None\n",
    "    current_libro_num = None\n",
    "    current_titulo = None\n",
    "    current_capitulo = None\n",
    "    current_capitulo_num = None\n",
    "    current_capitulo_desc = None\n",
    "\n",
    "    # Segmentaci√≥n de art√≠culos\n",
    "    articles = []\n",
    "    current_article_num = None\n",
    "    current_article_lines = []\n",
    "\n",
    "    def flush_article():\n",
    "        if current_article_num is None:\n",
    "            return\n",
    "        body = \"\\n\".join(current_article_lines).strip()\n",
    "        articles.append(\n",
    "            {\n",
    "                \"articulo_numero\": int(current_article_num),\n",
    "                \"libro\": current_libro.lower() if current_libro else None,\n",
    "                \"libro_numero\": current_libro_num,\n",
    "                \"titulo\": current_titulo.lower() if current_titulo else None,\n",
    "                \"capitulo\": current_capitulo.lower() if current_capitulo else None,\n",
    "                \"capitulo_numero\": current_capitulo_num,\n",
    "                \"capitulo_descripcion\": current_capitulo_desc.lower()\n",
    "                if current_capitulo_desc\n",
    "                else None,\n",
    "                \"articulo\": body.lower().replace(\"\\n\", \"\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        ln = lines[i]\n",
    "\n",
    "        # Detectar LIBRO\n",
    "        m_lib = HEADER_PATTERNS[\"libro\"].match(ln)\n",
    "        if m_lib:\n",
    "            current_libro = f\"LIBRO {m_lib.group(1).title()}\"\n",
    "            current_libro_num = ROMAN_MAP.get(m_lib.group(1).upper())\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Detectar TITULO\n",
    "        m_tit = HEADER_PATTERNS[\"titulo\"].match(ln)\n",
    "        if m_tit:\n",
    "            current_titulo = f\"TITULO {m_tit.group(1).title()}\"\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Detectar CAPITULO\n",
    "        m_cap = HEADER_PATTERNS[\"capitulo\"].match(ln)\n",
    "        if m_cap:\n",
    "            roman = m_cap.group(1)\n",
    "            current_capitulo = f\"CAPITULO {roman}\"\n",
    "            current_capitulo_num = roman_to_int(roman)\n",
    "            next_desc = None\n",
    "            if i + 1 < len(lines):\n",
    "                nxt = lines[i + 1]\n",
    "                if not (\n",
    "                    HEADER_PATTERNS[\"libro\"].match(nxt)\n",
    "                    or HEADER_PATTERNS[\"titulo\"].match(nxt)\n",
    "                    or HEADER_PATTERNS[\"capitulo\"].match(nxt)\n",
    "                    or ARTICULO_PATTERN.match(nxt)\n",
    "                ):\n",
    "                    next_desc = nxt\n",
    "            current_capitulo_desc = next_desc\n",
    "            i += 2 if next_desc else 1\n",
    "            continue\n",
    "\n",
    "        # Detectar inicio de Art√≠culo\n",
    "        m_art = ARTICULO_PATTERN.match(ln)\n",
    "        if m_art:\n",
    "            flush_article()\n",
    "            current_article_num = m_art.group(1)\n",
    "            current_article_lines = []\n",
    "            i += 1\n",
    "            while i < len(lines):\n",
    "                nxt = lines[i]\n",
    "                if (\n",
    "                    HEADER_PATTERNS[\"libro\"].match(nxt)\n",
    "                    or HEADER_PATTERNS[\"titulo\"].match(nxt)\n",
    "                    or HEADER_PATTERNS[\"capitulo\"].match(nxt)\n",
    "                    or ARTICULO_PATTERN.match(nxt)\n",
    "                ):\n",
    "                    break\n",
    "                current_article_lines.append(nxt)\n",
    "                i += 1\n",
    "            continue\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    flush_article()\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3402b24c-fdc8-4314-a7ce-e39e7fb4ff43",
   "metadata": {},
   "source": [
    "# Full Law Text Parsing / Parseo Completo del Texto Legal\n",
    "\n",
    "This function combines metadata extraction and article segmentation to parse the entire legal text:\n",
    "\n",
    "- `parse_law_text(raw_text)`: Receives the raw text of a law.\n",
    "  - Cleans the text using `fix_text()` to correct encoding issues.\n",
    "  - Splits the text into non-empty lines.\n",
    "  - Calls `extract_metadata(lines)` to extract law number, promulgation date, and publication date.\n",
    "  - Calls `extract_articles(lines)` to segment books, titles, chapters, and articles.\n",
    "  - Returns a dictionary containing `meta` (metadata) and `articulos` (list of structured articles).\n",
    "\n",
    "---\n",
    "\n",
    "Esta funci√≥n combina la extracci√≥n de metadatos y la segmentaci√≥n de art√≠culos para parsear todo el texto legal:\n",
    "\n",
    "- `parse_law_text(raw_text)`: Recibe el texto bruto de una ley.\n",
    "  - Limpia el texto usando `fix_text()` para corregir problemas de codificaci√≥n.\n",
    "  - Divide el texto en l√≠neas no vac√≠as.\n",
    "  - Llama a `extract_metadata(lines)` para extraer n√∫mero de ley, fecha de promulgaci√≥n y fecha de publicaci√≥n.\n",
    "  - Llama a `extract_articles(lines)` para segmentar libros, t√≠tulos, cap√≠tulos y art√≠culos.\n",
    "  - Devuelve un diccionario que contiene `meta` (metadatos) y `articulos` (lista de art√≠culos estructurados).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dc40704-7209-45db-9d9c-1e401c73bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_law_text(raw_text):\n",
    "    \"\"\"Parsea el texto completo en metadatos y art√≠culos.\"\"\"\n",
    "    text = fix_text(raw_text)\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "\n",
    "    meta = extract_metadata(lines)\n",
    "    articles = extract_articles(lines)\n",
    "\n",
    "    return {\n",
    "        \"meta\": meta,\n",
    "        \"articulos\": articles,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ba84f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = parse_law_text(texto_limpio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83a3366-5197-4068-9621-17cbfb529282",
   "metadata": {},
   "source": [
    "# Preview Parsed Data / Previsualizaci√≥n de Datos Parseados\n",
    "\n",
    "This code prints a preview of the parsed law data in JSON format:\n",
    "\n",
    "- Uses `json.dumps` to convert a dictionary into a formatted JSON string.\n",
    "- The dictionary contains:\n",
    "  - `meta`: The metadata of the law.\n",
    "  - `preview_articulos`: Only the first three articles for quick inspection.\n",
    "- `ensure_ascii=False` preserves special characters.\n",
    "- `indent=4` makes the JSON readable.\n",
    "\n",
    "---\n",
    "\n",
    "Este c√≥digo imprime una previsualizaci√≥n de los datos parseados de la ley en formato JSON:\n",
    "\n",
    "- Usa `json.dumps` para convertir un diccionario en una cadena JSON formateada.\n",
    "- El diccionario contiene:\n",
    "  - `meta`: Los metadatos de la ley.\n",
    "  - `preview_articulos`: Solo los tres primeros art√≠culos para una inspecci√≥n r√°pida.\n",
    "- `ensure_ascii=False` preserva los caracteres especiales.\n",
    "- `indent=4` hace que el JSON sea legible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6cd018b-eada-4d7f-87ca-b94b1a9c07c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"meta\": {\n",
      "        \"numero_ley\": \"213\",\n",
      "        \"fecha_promulgacion\": \"29-06-1993\",\n",
      "        \"fecha_publicacion\": \"29-10-1993\"\n",
      "    },\n",
      "    \"preview_articulos\": [\n",
      "        {\n",
      "            \"articulo_numero\": 1,\n",
      "            \"libro\": \"libro primero\",\n",
      "            \"libro_numero\": 1,\n",
      "            \"titulo\": \"titulo primero\",\n",
      "            \"capitulo\": \"capitulo i\",\n",
      "            \"capitulo_numero\": 1,\n",
      "            \"capitulo_descripcion\": \"del objeto y aplicaci√≥n del c√≥digo\",\n",
      "            \"articulo\": \"este c√≥digo tiene por objeto establecer normas para regular las relaciones entre los trabajadores y empleadores, concernientes a la prestaci√≥n subordinada y retribuida de la actividad laboral.\"\n",
      "        },\n",
      "        {\n",
      "            \"articulo_numero\": 2,\n",
      "            \"libro\": \"libro primero\",\n",
      "            \"libro_numero\": 1,\n",
      "            \"titulo\": \"titulo primero\",\n",
      "            \"capitulo\": \"capitulo i\",\n",
      "            \"capitulo_numero\": 1,\n",
      "            \"capitulo_descripcion\": \"del objeto y aplicaci√≥n del c√≥digo\",\n",
      "            \"articulo\": \"estar√°n sujetos a las disposiciones del presente c√≥digo:los trabajadores intelectuales, manuales o t√©cnicos en relaci√≥n de dependencia y sus empleadores. los profesores de institutos de ense√±anza privada y quienes ejerzan la pr√°ctica deportiva profesional.los sindicatos de trabajadores y empleadores del sector privado.los trabajadores de las empresas del estado y de las empresas municipales.los dem√°s trabajadores del estado, sean de la administraci√≥n central o de entes descentralizados los de las municipalidades y departamentos, ser√°n regidos por ley especial.est√°n excluidos los miembros de las fuerzas armadas y de la polic√≠a.\"\n",
      "        },\n",
      "        {\n",
      "            \"articulo_numero\": 3,\n",
      "            \"libro\": \"libro primero\",\n",
      "            \"libro_numero\": 1,\n",
      "            \"titulo\": \"titulo primero\",\n",
      "            \"capitulo\": \"capitulo i\",\n",
      "            \"capitulo_numero\": 1,\n",
      "            \"capitulo_descripcion\": \"del objeto y aplicaci√≥n del c√≥digo\",\n",
      "            \"articulo\": \"los derechos reconocidos por este c√≥digo a los trabajadores no podr√°n ser objeto de renuncia, transacci√≥n o limitaci√≥n convencional. ser√° nulo todo pacto contrario.las leyes que los establecen obligan y benefician a todos los trabajadores y empleadores de la rep√∫blica, sean nacionales o extranjeros y se inspirar√°n en los principios contenidos en la declaraci√≥n universal de los derechos humanos, aprobada y proclamada por la asamblea general de las naciones unidas el 10 de diciembre de 1948, la declaraci√≥n americana de los derechos y deberes del hombre, proclamada por la novena conferencia panamericana de bogot√° el d√≠a 2 de mayo de 1948 y en los dem√°s convenios internacionales del trabajo ratificados y canjeados por el paraguay que integran el derecho positivo.\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8148200-e1db-4cb6-bdbf-d62cc148c61d",
   "metadata": {},
   "source": [
    "# Save Parsed Data to JSON / Guardar Datos Parseados en JSON\n",
    "\n",
    "This function saves the parsed law data into a JSON file:\n",
    "\n",
    "- `save_parsed_json(parsed, filename, out_dir)`:\n",
    "  - `parsed`: The dictionary containing `meta` and `articulos`.\n",
    "  - `filename`: Name of the JSON file to create (default `\"codigo_trabajo_articulos.json\"`).\n",
    "  - `out_dir`: Directory to save the file (default `\"../data/processed\"`).\n",
    "- Ensures the output directory exists with `mkdir(parents=True, exist_ok=True)`.\n",
    "- Writes the JSON data with `ensure_ascii=False` to preserve special characters and `indent=2` for readability.\n",
    "- Prints the path of the saved file and the total number of articles.\n",
    "\n",
    "---\n",
    "\n",
    "Esta funci√≥n guarda los datos parseados de la ley en un archivo JSON:\n",
    "\n",
    "- `save_parsed_json(parsed, filename, out_dir)`:\n",
    "  - `parsed`: Diccionario que contiene `meta` y `articulos`.\n",
    "  - `filename`: Nombre del archivo JSON a crear (por defecto `\"codigo_trabajo_articulos.json\"`).\n",
    "  - `out_dir`: Carpeta donde se guardar√° el archivo (por defecto `\"../data/processed\"`).\n",
    "- Asegura que la carpeta de salida exista usando `mkdir(parents=True, exist_ok=True)`.\n",
    "- Escribe los datos en JSON con `ensure_ascii=False` para preservar caracteres especiales y `indent=2` para legibilidad.\n",
    "- Imprime la ruta del archivo guardado y el n√∫mero total de art√≠culos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55758ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parsed_json(parsed, filename=\"codigo_trabajo_articulos.json\", out_dir=\"../data/processed\"):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = out_dir / filename\n",
    "\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(parsed, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58a1cf55-eb69-4f17-8610-c4db573262e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: ../data/processed/codigo_trabajo_articulos.json\n",
      "Art√≠culos totales: 410\n"
     ]
    }
   ],
   "source": [
    "save_parsed_json(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f220b86",
   "metadata": {},
   "source": [
    "# System Analysis and Improvements / An√°lisis y Mejoras del Sistema\n",
    "\n",
    "*This section documents the development process, debugging, and improvements implemented during the creation of the legal data processing system.*\n",
    "\n",
    "---\n",
    "\n",
    "*Esta secci√≥n documenta el proceso de desarrollo, debugging y mejoras implementadas durante la creaci√≥n del sistema de procesamiento de datos legales.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f66d2",
   "metadata": {},
   "source": [
    "# Soluci√≥n de Regex para Capturar Todos los Art√≠culos / Regex Solution to Capture All Articles\n",
    "\n",
    "During the system development, we identified a critical issue: **some articles were not being captured** due to variations in the official site‚Äôs HTML format.\n",
    "\n",
    "**Identified Problem**\n",
    "\n",
    "The original regex pattern was too strict:\n",
    "```python\n",
    "# Original (problematic) pattern\n",
    "ARTICULO_PATTERN = re.compile(r\"^Art[√≠i]?t?culo\\s+(\\d+)\\s*(?:[¬∞¬∫])?\\s*\\.?\\s*-\\s*$\", re.IGNORECASE)\n",
    "````\n",
    "\n",
    "**Issues:**\n",
    "\n",
    "* Required the line to **end** with a dash (`-`)\n",
    "* Did not handle variations in the HTML format\n",
    "* Articles 95, 232, and 374 were not captured\n",
    "\n",
    "**HTML Analysis**\n",
    "\n",
    "The HTML had inconsistent formats:\n",
    "\n",
    "* **Standard format**: `Art√≠culo XXX¬∞.-`\n",
    "* **Problematic format**: `<strong>Art√≠culo XXX¬∞.</strong>-`\n",
    "\n",
    "**Implemented Solution**\n",
    "\n",
    "```python\n",
    "# Corrected pattern (flexible but precise)\n",
    "ARTICULO_PATTERN = re.compile(r\"^Art[√≠i]?t?culo\\s+(\\d+)\\s*(?:[¬∞¬∫])?\\s*\\.?\\s*-?\\s*\", re.IGNORECASE)\n",
    "```\n",
    "\n",
    "**Key changes:**\n",
    "\n",
    "1. **Optional dash** (`-?`) to handle variations\n",
    "2. **Keep `^`** to avoid false matches in the middle of lines\n",
    "3. **Use `match()`** instead of `search()` for precision\n",
    "\n",
    "**Result**\n",
    "\n",
    "* **413 unique articles** correctly captured\n",
    "* **No duplicates** after the fix\n",
    "* **Problematic articles** (95, 232, 374) included\n",
    "* **Automatic data integrity validation** in place\n",
    "\n",
    "---\n",
    "\n",
    "Durante el desarrollo del sistema, identificamos un problema cr√≠tico: **algunos art√≠culos no se capturaban** debido a variaciones en el formato HTML del sitio oficial.\n",
    "\n",
    "**Problema Identificado**\n",
    "\n",
    "El patr√≥n regex original era demasiado estricto:\n",
    "```python\n",
    "# Patr√≥n original (problem√°tico)\n",
    "ARTICULO_PATTERN = re.compile(r\"^Art[√≠i]?t?culo\\s+(\\d+)\\s*(?:[¬∞¬∫])?\\s*\\.?\\s*-\\s*$\", re.IGNORECASE)\n",
    "```\n",
    "\n",
    "**Problemas:**\n",
    "- Requer√≠a que la l√≠nea **terminara** con un gui√≥n (`-`)\n",
    "- No manejaba variaciones en el formato HTML\n",
    "- Art√≠culos 95, 232 y 374 no se capturaban\n",
    "\n",
    "**An√°lisis del HTML**\n",
    "\n",
    "El HTML ten√≠a formatos inconsistentes:\n",
    "- **Formato est√°ndar**: `Art√≠culo XXX¬∞.-`\n",
    "- **Formato problem√°tico**: `<strong>Art√≠culo XXX¬∞.</strong>-`\n",
    "\n",
    "**Soluci√≥n Implementada**\n",
    "\n",
    "```python\n",
    "# Patr√≥n corregido (flexible pero preciso)\n",
    "ARTICULO_PATTERN = re.compile(r\"^Art[√≠i]?t?culo\\s+(\\d+)\\s*(?:[¬∞¬∫])?\\s*\\.?\\s*-?\\s*\", re.IGNORECASE)\n",
    "```\n",
    "\n",
    "**Cambios clave:**\n",
    "1. **Gui√≥n opcional** (`-?`) para manejar variaciones\n",
    "2. **Mantener `^`** para evitar capturas falsas en medio de l√≠neas\n",
    "3. **Usar `match()`** en lugar de `search()` para precisi√≥n\n",
    "\n",
    "**Resultado**\n",
    "\n",
    "- **413 art√≠culos √∫nicos** capturados correctamente\n",
    "- **Sin duplicados** despu√©s de la correcci√≥n\n",
    "- **Art√≠culos problem√°ticos** (95, 232, 374) incluidos\n",
    "- **Validaci√≥n autom√°tica** de integridad de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b96f85e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementaci√≥n de la soluci√≥n corregida\n",
    "def debug_article_capture():\n",
    "    \"\"\"Script de debugging para identificar art√≠culos problem√°ticos.\"\"\"\n",
    "    import json\n",
    "    from collections import Counter\n",
    "\n",
    "    # Cargar datos procesados\n",
    "    with open(\"../data/processed/codigo_trabajo_articulos.json\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Verificar duplicados y art√≠culos faltantes\n",
    "    article_numbers = [art[\"articulo_numero\"] for art in data[\"articulos\"]]\n",
    "    counter = Counter(article_numbers)\n",
    "\n",
    "    duplicates = {num: count for num, count in counter.items() if count > 1}\n",
    "    missing = [i for i in range(1, 414) if i not in article_numbers]\n",
    "\n",
    "    # Verificar art√≠culos problem√°ticos espec√≠ficos\n",
    "    problematic_articles = [95, 232, 374]\n",
    "    for art_num in problematic_articles:\n",
    "        found = art_num in article_numbers\n",
    "\n",
    "    return {\n",
    "        \"total_articles\": len(data[\"articulos\"]),\n",
    "        \"unique_articles\": len(set(article_numbers)),\n",
    "        \"duplicates\": len(duplicates),\n",
    "        \"missing\": missing,\n",
    "        \"problematic_found\": all(art in article_numbers for art in problematic_articles),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14cc757b-0f9d-45fd-8d0b-b7fa7768a660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä An√°lisis de Captura de Art√≠culos:\n",
      "   Art√≠culos duplicados: 0\n",
      "   Art√≠culos faltantes: [95, 232, 374]\n",
      "   Total procesados: 410\n",
      "   Art√≠culos √∫nicos: 410\n",
      "   Rango: 1 - 413\n",
      "\n",
      "üîç Verificaci√≥n de Art√≠culos Problem√°ticos:\n",
      "   Art√≠culo 95: ‚ùå Faltante\n",
      "   Art√≠culo 232: ‚ùå Faltante\n",
      "   Art√≠culo 374: ‚ùå Faltante\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar an√°lisis sin cambios\n",
    "results = debug_article_capture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8c59f1e-af74-40d3-b6d6-784ce6ce3789",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICULO_PATTERN = re.compile(r\"^Art[√≠i]?t?culo\\s+(\\d+)\\s*(?:[¬∞¬∫])?\\s*\\.?\\s*-?\\s*\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "820ada75-7618-4119-a521-2e351b7ffb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = parse_law_text(texto_limpio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9c839cc-c679-4b50-bd00-0b5ed08c34fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: ../data/processed/codigo_trabajo_articulos.json\n",
      "Art√≠culos totales: 413\n"
     ]
    }
   ],
   "source": [
    "save_parsed_json(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7a31b7b-05b5-4a73-ad2d-6e67d82a9011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä An√°lisis de Captura de Art√≠culos:\n",
      "   Art√≠culos duplicados: 0\n",
      "   Art√≠culos faltantes: []\n",
      "   Total procesados: 413\n",
      "   Art√≠culos √∫nicos: 413\n",
      "   Rango: 1 - 413\n",
      "\n",
      "üîç Verificaci√≥n de Art√≠culos Problem√°ticos:\n",
      "   Art√≠culo 95: ‚úÖ Capturado\n",
      "   Art√≠culo 232: ‚úÖ Capturado\n",
      "   Art√≠culo 374: ‚úÖ Capturado\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar an√°lisis con cambios\n",
    "results = debug_article_capture()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941eaa06",
   "metadata": {},
   "source": [
    "# Lessons Learned / Lecciones Aprendidas\n",
    "\n",
    "**Best Practices for Regex in Data Processing**\n",
    "\n",
    "**1. Flexibility vs. Precision**\n",
    "- **Problem**: Overly strict patterns fail with format variations  \n",
    "- **Solution**: Use flexible patterns but keep anchors (`^`, `$`) for accuracy  \n",
    "- **Example**: `-?` (optional dash) instead of `-` (mandatory dash)  \n",
    "\n",
    "**2. Systematic Debugging**\n",
    "- **Problem**: Subtle parsing errors can go unnoticed  \n",
    "- **Solution**: Create specific validation scripts to check integrity  \n",
    "- **Tools**: `Counter`, range verification, duplicate analysis  \n",
    "\n",
    "**3. Data Validation**\n",
    "- **Problem**: Assuming processing was successful without verification  \n",
    "- **Solution**: Validate both quantity and quality of extracted data  \n",
    "- **Metrics**: Unique element count, range verification, duplicate detection  \n",
    "\n",
    "**4. Rapid Iteration**\n",
    "- **Problem**: Large changes make root cause harder to identify  \n",
    "- **Solution**: Incremental approach with small changes and continuous validation  \n",
    "- **Process**: Identify ‚Üí Analyze ‚Üí Fix ‚Üí Validate ‚Üí Iterate  \n",
    "\n",
    "**Success Metrics**\n",
    "- **413 unique articles** captured  \n",
    "- **0 duplicates** after the fix  \n",
    "- **100% coverage** of problematic articles  \n",
    "- **Automatic validation** implemented  \n",
    "\n",
    "---\n",
    "\n",
    "**Mejores Pr√°cticas para Regex en Procesamiento de Datos**\n",
    "\n",
    "**1. Flexibilidad vs Precisi√≥n**\n",
    "- **Problema**: Patrones demasiado estrictos fallan con variaciones de formato\n",
    "- **Soluci√≥n**: Usar patrones flexibles pero mantener anclas (`^`, `$`) para precisi√≥n\n",
    "- **Ejemplo**: `-?` (gui√≥n opcional) en lugar de `-` (gui√≥n obligatorio)\n",
    "\n",
    "**2. Debugging Sistem√°tico**\n",
    "- **Problema**: Errores sutiles en parsing pueden pasar desapercibidos\n",
    "- **Soluci√≥n**: Crear scripts de validaci√≥n espec√≠ficos para verificar integridad\n",
    "- **Herramientas**: `Counter`, verificaci√≥n de rangos, an√°lisis de duplicados\n",
    "\n",
    "**3. Validaci√≥n de Datos**\n",
    "- **Problema**: Asumir que el procesamiento fue exitoso sin verificar\n",
    "- **Soluci√≥n**: Validar tanto cantidad como calidad de datos extra√≠dos\n",
    "- **M√©tricas**: Conteo de elementos √∫nicos, verificaci√≥n de rangos, detecci√≥n de duplicados\n",
    "\n",
    "**4. Iteraci√≥n R√°pida**\n",
    "- **Problema**: Cambios grandes dificultan identificar la causa ra√≠z\n",
    "- **Soluci√≥n**: Enfoque incremental con cambios peque√±os y validaci√≥n continua\n",
    "- **Proceso**: Identificar ‚Üí Analizar ‚Üí Corregir ‚Üí Validar ‚Üí Iterar\n",
    "\n",
    "**M√©tricas de √âxito**\n",
    "\n",
    "- **413 art√≠culos √∫nicos** capturados\n",
    "- **0 duplicados** despu√©s de la correcci√≥n\n",
    "- **100% de cobertura** de art√≠culos problem√°ticos\n",
    "- **Validaci√≥n autom√°tica** implementada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a520c7",
   "metadata": {},
   "source": [
    "# Proposed Improvements: Validation and Quality Control / Mejoras Propuestas: Validaci√≥n y Control de Calidad\n",
    "\n",
    "Once we have extracted and structured the articles, it is crucial to **validate the quality and integrity** of the processed data. This step is fundamental to ensure that our RAG system has reliable information. Although the current system works correctly, these additional validations would significantly improve the robustness of the pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "Una vez que hemos extra√≠do y estructurado los art√≠culos, es crucial **validar la calidad y integridad** de los datos procesados. Este paso es fundamental para garantizar que nuestro sistema RAG tenga informaci√≥n confiable. Aunque el sistema actual funciona correctamente, estas validaciones adicionales mejorar√≠an significativamente la robustez del pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960f65d8",
   "metadata": {},
   "source": [
    "**Automatic Structure Validation / Validaci√≥n Autom√°tica de Estructura**\n",
    "\n",
    "This function validates the integrity and quality of the processed data by checking required fields, valid numbers, and non-empty content.\n",
    "\n",
    "---\n",
    "\n",
    "Esta funci√≥n valida la integridad y calidad de los datos procesados, verificando campos requeridos, n√∫meros v√°lidos y contenido no vac√≠o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4175a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_processed_data(articles):\n",
    "    \"\"\"Valida la integridad y calidad de los datos procesados.\"\"\"\n",
    "    validation_results = {\n",
    "        \"total_articles\": len(articles),\n",
    "        \"valid_articles\": 0,\n",
    "        \"invalid_articles\": [],\n",
    "        \"missing_fields\": [],\n",
    "        \"quality_score\": 0.0,\n",
    "    }\n",
    "\n",
    "    required_fields = [\"articulo_numero\", \"libro\", \"capitulo\", \"articulo\"]\n",
    "\n",
    "    for article in articles:\n",
    "        article_valid = True\n",
    "        article_issues = []\n",
    "\n",
    "        # Verificar campos requeridos\n",
    "        for field in required_fields:\n",
    "            if field not in article or not article[field]:\n",
    "                article_issues.append(f\"Campo faltante: {field}\")\n",
    "                article_valid = False\n",
    "\n",
    "        # Verificar que el n√∫mero de art√≠culo sea v√°lido\n",
    "        if \"articulo_numero\" in article:\n",
    "            art_num = article[\"articulo_numero\"]\n",
    "            if not isinstance(art_num, int) or art_num < 1 or art_num > 413:\n",
    "                article_issues.append(f\"N√∫mero de art√≠culo inv√°lido: {art_num}\")\n",
    "                article_valid = False\n",
    "\n",
    "        # Verificar que el contenido no est√© vac√≠o\n",
    "        if \"articulo\" in article and len(article[\"articulo\"].strip()) < 10:\n",
    "            article_issues.append(\"Contenido del art√≠culo demasiado corto\")\n",
    "            article_valid = False\n",
    "\n",
    "        if article_valid:\n",
    "            validation_results[\"valid_articles\"] += 1\n",
    "        else:\n",
    "            validation_results[\"invalid_articles\"].append(\n",
    "                {\n",
    "                    \"articulo_numero\": article.get(\"articulo_numero\", \"desconocido\"),\n",
    "                    \"issues\": article_issues,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Calcular score de calidad\n",
    "    validation_results[\"quality_score\"] = (\n",
    "        validation_results[\"valid_articles\"] / validation_results[\"total_articles\"]\n",
    "    )\n",
    "\n",
    "    return validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3003bcc3-4bfb-47d1-8ddc-08a043483007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validaci√≥n completada: 413/413 art√≠culos v√°lidos\n",
      "Score de calidad: 100.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_articles': 413,\n",
       " 'valid_articles': 413,\n",
       " 'invalid_articles': [],\n",
       " 'missing_fields': [],\n",
       " 'quality_score': 1.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_processed_data(parsed.get(\"articulos\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcd4d7d",
   "metadata": {},
   "source": [
    "**Data Completeness Verification / Verificaci√≥n de Completitud de Datos**\n",
    "\n",
    "This function checks that all expected articles are present and detects duplicates.\n",
    "\n",
    "---\n",
    "\n",
    "Esta funci√≥n verifica que todos los art√≠culos esperados est√©n presentes y detecta duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29ca09fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_data_completeness(articles):\n",
    "    \"\"\"Verifica que todos los art√≠culos esperados est√©n presentes.\"\"\"\n",
    "\n",
    "    article_numbers = [art[\"articulo_numero\"] for art in articles if \"articulo_numero\" in art]\n",
    "\n",
    "    # Verificar rango completo (1-413)\n",
    "    expected_range = set(range(1, 414))\n",
    "    found_numbers = set(article_numbers)\n",
    "\n",
    "    missing_articles = expected_range - found_numbers\n",
    "    duplicate_articles = [num for num in article_numbers if article_numbers.count(num) > 1]\n",
    "\n",
    "    completeness_report = {\n",
    "        \"expected_total\": 413,\n",
    "        \"found_total\": len(found_numbers),\n",
    "        \"missing_articles\": sorted(list(missing_articles)),\n",
    "        \"duplicate_articles\": duplicate_articles,\n",
    "        \"completeness_percentage\": len(found_numbers) / 413 * 100,\n",
    "    }\n",
    "\n",
    "    if missing_articles:\n",
    "        pass\n",
    "\n",
    "    if duplicate_articles:\n",
    "        pass\n",
    "\n",
    "    return completeness_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b370f5a9-66d1-4d7b-ae60-c54230d4ba35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completitud de datos: 100.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'expected_total': 413,\n",
       " 'found_total': 413,\n",
       " 'missing_articles': [],\n",
       " 'duplicate_articles': [],\n",
       " 'completeness_percentage': 100.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify_data_completeness(parsed.get(\"articulos\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5bbd9",
   "metadata": {},
   "source": [
    "** Content Quality Analysis / An√°lisis de Calidad de Contenido**\n",
    "\n",
    "This function analyzes the quality of the extracted content, providing metrics on length, special characters, and numbers.\n",
    "\n",
    "---\n",
    "\n",
    "Esta funci√≥n analiza la calidad del contenido extra√≠do, proporcionando m√©tricas sobre longitud, caracteres especiales y n√∫meros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c569195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_content_quality(articles):\n",
    "    \"\"\"Analiza la calidad del contenido extra√≠do.\"\"\"\n",
    "    quality_metrics = {\n",
    "        \"avg_content_length\": 0,\n",
    "        \"short_articles\": 0,  # < 50 caracteres\n",
    "        \"medium_articles\": 0,  # 50-200 caracteres\n",
    "        \"long_articles\": 0,  # > 200 caracteres\n",
    "        \"articles_with_special_chars\": 0,\n",
    "        \"articles_with_numbers\": 0,\n",
    "    }\n",
    "\n",
    "    content_lengths = []\n",
    "\n",
    "    for article in articles:\n",
    "        if \"articulo\" not in article:\n",
    "            continue\n",
    "\n",
    "        content = article[\"articulo\"]\n",
    "        content_length = len(content.strip())\n",
    "        content_lengths.append(content_length)\n",
    "\n",
    "        # Clasificar por longitud\n",
    "        if content_length < 50:\n",
    "            quality_metrics[\"short_articles\"] += 1\n",
    "        elif content_length <= 200:\n",
    "            quality_metrics[\"medium_articles\"] += 1\n",
    "        else:\n",
    "            quality_metrics[\"long_articles\"] += 1\n",
    "\n",
    "        # Verificar caracter√≠sticas especiales\n",
    "        if any(char in content for char in [\"¬∞\", \"¬∫\", \"¬ß\", \"¬∂\"]):\n",
    "            quality_metrics[\"articles_with_special_chars\"] += 1\n",
    "\n",
    "        if any(char.isdigit() for char in content):\n",
    "            quality_metrics[\"articles_with_numbers\"] += 1\n",
    "\n",
    "    if content_lengths:\n",
    "        quality_metrics[\"avg_content_length\"] = sum(content_lengths) / len(content_lengths)\n",
    "\n",
    "    return quality_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac34b6c0-47d3-4ed8-9838-ea4e662fe153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An√°lisis de calidad completado\n",
      "Longitud promedio de art√≠culos: 438.2 caracteres\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_content_length': 438.17433414043586,\n",
       " 'short_articles': 0,\n",
       " 'medium_articles': 98,\n",
       " 'long_articles': 315,\n",
       " 'articles_with_special_chars': 3,\n",
       " 'articles_with_numbers': 64}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_content_quality(parsed.get(\"articulos\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65c0a07",
   "metadata": {},
   "source": [
    "**Quality Report Generation / Generaci√≥n de Reporte de Calidad**\n",
    "\n",
    "This function generates a comprehensive data quality report by combining all the previous validations.\n",
    "\n",
    "---\n",
    "\n",
    "Esta funci√≥n genera un reporte completo de calidad de datos combinando todas las validaciones anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "efebd322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quality_report(articles):\n",
    "    \"\"\"Genera un reporte completo de calidad de datos.\"\"\"\n",
    "    validation_results = validate_processed_data(articles)\n",
    "    completeness_report = verify_data_completeness(articles)\n",
    "    quality_metrics = analyze_content_quality(articles)\n",
    "\n",
    "    report = f\"\"\"\n",
    "üìä REPORTE DE CALIDAD DE DATOS PROCESADOS\n",
    "{\"=\" * 50}\n",
    "\n",
    "‚úÖ VALIDACI√ìN DE ESTRUCTURA:\n",
    "   ‚Ä¢ Art√≠culos v√°lidos: {validation_results[\"valid_articles\"]}/{validation_results[\"total_articles\"]}\n",
    "   ‚Ä¢ Score de calidad: {validation_results[\"quality_score\"]:.2%}\n",
    "   ‚Ä¢ Art√≠culos con problemas: {len(validation_results[\"invalid_articles\"])}\n",
    "\n",
    "üìã COMPLETITUD DE DATOS:\n",
    "   ‚Ä¢ Art√≠culos encontrados: {completeness_report[\"found_total\"]}/413\n",
    "   ‚Ä¢ Completitud: {completeness_report[\"completeness_percentage\"]:.1f}%\n",
    "   ‚Ä¢ Art√≠culos faltantes: {len(completeness_report[\"missing_articles\"])}\n",
    "   ‚Ä¢ Art√≠culos duplicados: {len(completeness_report[\"duplicate_articles\"])}\n",
    "\n",
    "üìù AN√ÅLISIS DE CONTENIDO:\n",
    "   ‚Ä¢ Longitud promedio: {quality_metrics[\"avg_content_length\"]:.1f} caracteres\n",
    "   ‚Ä¢ Art√≠culos cortos (< 50 chars): {quality_metrics[\"short_articles\"]}\n",
    "   ‚Ä¢ Art√≠culos medianos (50-200 chars): {quality_metrics[\"medium_articles\"]}\n",
    "   ‚Ä¢ Art√≠culos largos (> 200 chars): {quality_metrics[\"long_articles\"]}\n",
    "   ‚Ä¢ Con caracteres especiales: {quality_metrics[\"articles_with_special_chars\"]}\n",
    "   ‚Ä¢ Con n√∫meros: {quality_metrics[\"articles_with_numbers\"]}\n",
    "\n",
    "üéØ ESTADO GENERAL: {\"‚úÖ EXCELENTE\" if validation_results[\"quality_score\"] > 0.95 else \"‚ö†Ô∏è REQUIERE ATENCI√ìN\"}\n",
    "\"\"\"\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2644f45",
   "metadata": {},
   "source": [
    "**Improvements Demonstration / Demostraci√≥n de las Mejoras**\n",
    "\n",
    "Now we are going to test the validation and quality control functions with the processed data:\n",
    "\n",
    "---\n",
    "\n",
    "Ahora vamos a probar las funciones de validaci√≥n y control de calidad con los datos procesados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11738283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validaci√≥n completada: 413/413 art√≠culos v√°lidos\n",
      "Score de calidad: 100.00%\n",
      "Completitud de datos: 100.0%\n",
      "An√°lisis de calidad completado\n",
      "Longitud promedio de art√≠culos: 438.2 caracteres\n",
      "\n",
      "üìä REPORTE DE CALIDAD DE DATOS PROCESADOS\n",
      "==================================================\n",
      "\n",
      "‚úÖ VALIDACI√ìN DE ESTRUCTURA:\n",
      "   ‚Ä¢ Art√≠culos v√°lidos: 413/413\n",
      "   ‚Ä¢ Score de calidad: 100.00%\n",
      "   ‚Ä¢ Art√≠culos con problemas: 0\n",
      "\n",
      "üìã COMPLETITUD DE DATOS:\n",
      "   ‚Ä¢ Art√≠culos encontrados: 413/413\n",
      "   ‚Ä¢ Completitud: 100.0%\n",
      "   ‚Ä¢ Art√≠culos faltantes: 0\n",
      "   ‚Ä¢ Art√≠culos duplicados: 0\n",
      "\n",
      "üìù AN√ÅLISIS DE CONTENIDO:\n",
      "   ‚Ä¢ Longitud promedio: 438.2 caracteres\n",
      "   ‚Ä¢ Art√≠culos cortos (< 50 chars): 0\n",
      "   ‚Ä¢ Art√≠culos medianos (50-200 chars): 98\n",
      "   ‚Ä¢ Art√≠culos largos (> 200 chars): 315\n",
      "   ‚Ä¢ Con caracteres especiales: 3\n",
      "   ‚Ä¢ Con n√∫meros: 64\n",
      "\n",
      "üéØ ESTADO GENERAL: ‚úÖ EXCELENTE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generar reporte completo de calidad\n",
    "quality_report = generate_quality_report(parsed[\"articulos\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee86d04",
   "metadata": {},
   "source": [
    "**Benefits of Implementing These Improvements / Beneficios de Implementar Estas Mejoras**\n",
    "\n",
    "- **Early Problem Detection**: Identify errors before they reach the RAG system  \n",
    "- **Quality Metrics**: Quantify the quality of the processed data  \n",
    "- **Improved Debugging**: Detailed reports to facilitate issue resolution  \n",
    "- **System Robustness**: Automatic validations that prevent production failures  \n",
    "- **Continuous Monitoring**: Ability to detect quality degradation over time  \n",
    "\n",
    "*These functions can be easily integrated into the current system by adding them to the `extract_law_text.py` file and calling them after the article processing step.*\n",
    "\n",
    "---\n",
    "\n",
    "- **Detecci√≥n Temprana de Problemas**: Identificar errores antes de que lleguen al sistema RAG\n",
    "- **M√©tricas de Calidad**: Cuantificar la calidad de los datos procesados\n",
    "- **Debugging Mejorado**: Reportes detallados para facilitar la resoluci√≥n de problemas\n",
    "- **Robustez del Sistema**: Validaciones autom√°ticas que previenen fallos en producci√≥n\n",
    "- **Monitoreo Continuo**: Capacidad de detectar degradaci√≥n de calidad a lo largo del tiempo\n",
    "\n",
    "*Estas funciones se pueden integrar f√°cilmente al sistema actual agreg√°ndolas al archivo `extract_law_text.py` y llam√°ndolas despu√©s del procesamiento de art√≠culos.*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
