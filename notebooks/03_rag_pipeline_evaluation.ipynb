{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluación Integral del Pipeline RAG\n",
        "\n",
        "Esta notebook evalúa el pipeline completo de RAG (Retrieval-Augmented Generation) incluyendo:\n",
        "- **Retrievers** (embeddings + Qdrant)\n",
        "- **Re-ranking** de documentos\n",
        "- **Generación** con LLM\n",
        "- **Métricas objetivas** (recall, nDCG, MRR, precisión@k)\n",
        "- **Métricas subjetivas** (coherencia, relevancia, completitud via LLM-as-a-judge)\n",
        "\n",
        "## Tecnologías utilizadas:\n",
        "- Qdrant (base vectorial)\n",
        "- Sentence Transformers (embeddings)\n",
        "- Cross-encoders (re-ranking)\n",
        "- OpenAI/Transformers (generación y evaluación)\n",
        "- Scikit-learn (métricas)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importaciones y Configuración Inicial\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any, Tuple, Optional\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Qdrant\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "\n",
        "# Embeddings\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "\n",
        "# Métricas\n",
        "from sklearn.metrics import ndcg_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# LLM\n",
        "import openai\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Configuración de visualización\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"✅ Librerías importadas correctamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuración de Modelos y Parámetros\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuración de modelos de embedding\n",
        "EMBEDDING_MODELS = {\n",
        "    'all-MiniLM-L6-v2': {\n",
        "        'model_name': 'sentence-transformers/all-MiniLM-L6-v2',\n",
        "        'dimension': 384,\n",
        "        'description': 'Modelo ligero y rápido'\n",
        "    },\n",
        "    'all-mpnet-base-v2': {\n",
        "        'model_name': 'sentence-transformers/all-mpnet-base-v2',\n",
        "        'dimension': 768,\n",
        "        'description': 'Modelo balanceado'\n",
        "    },\n",
        "    'paraphrase-multilingual-MiniLM-L12-v2': {\n",
        "        'model_name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
        "        'dimension': 384,\n",
        "        'description': 'Modelo multilingüe'\n",
        "    },\n",
        "    'all-distilroberta-v1': {\n",
        "        'model_name': 'sentence-transformers/all-distilroberta-v1',\n",
        "        'dimension': 768,\n",
        "        'description': 'Modelo basado en DistilRoBERTa'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Configuración de modelos de re-ranking\n",
        "RERANKING_MODELS = {\n",
        "    'ms-marco-MiniLM-L-6-v2': {\n",
        "        'model_name': 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
        "        'description': 'Re-ranker ligero para MS MARCO'\n",
        "    },\n",
        "    'ms-marco-MiniLM-L-12-v2': {\n",
        "        'model_name': 'cross-encoder/ms-marco-MiniLM-L-12-v2',\n",
        "        'description': 'Re-ranker más robusto para MS MARCO'\n",
        "    },\n",
        "    'ms-marco-MiniLM-L-2-v2': {\n",
        "        'model_name': 'cross-encoder/ms-marco-MiniLM-L-2-v2',\n",
        "        'description': 'Re-ranker ultra-ligero'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Configuración de métricas de retrieval\n",
        "RETRIEVAL_METRICS = {\n",
        "    'recall_at_k': [1, 3, 5, 10],\n",
        "    'precision_at_k': [1, 3, 5, 10],\n",
        "    'ndcg_at_k': [1, 3, 5, 10],\n",
        "    'mrr': True,\n",
        "    'hit_rate_at_k': [1, 3, 5, 10]\n",
        "}\n",
        "\n",
        "# Configuración de evaluación LLM\n",
        "LLM_EVALUATION_CRITERIA = {\n",
        "    'coherence': {\n",
        "        'description': '¿La respuesta es coherente y bien estructurada?',\n",
        "        'scale': (1, 5)\n",
        "    },\n",
        "    'relevance': {\n",
        "        'description': '¿La respuesta es relevante a la pregunta?',\n",
        "        'scale': (1, 5)\n",
        "    },\n",
        "    'completeness': {\n",
        "        'description': '¿La respuesta es completa y abarca todos los aspectos?',\n",
        "        'scale': (1, 5)\n",
        "    },\n",
        "    'fidelity': {\n",
        "        'description': '¿La respuesta es fiel al contexto proporcionado?',\n",
        "        'scale': (1, 5)\n",
        "    },\n",
        "    'conciseness': {\n",
        "        'description': '¿La respuesta es concisa sin ser incompleta?',\n",
        "        'scale': (1, 5)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Configuración de Qdrant\n",
        "QDRANT_CONFIG = {\n",
        "    'url': 'http://localhost:6333',\n",
        "    'collection_name': 'lus_laboris_articles',\n",
        "    'top_k': 20  # Número de documentos a recuperar inicialmente\n",
        "}\n",
        "\n",
        "# Configuración de LLM\n",
        "LLM_CONFIG = {\n",
        "    'provider': 'openai',  # 'openai' o 'huggingface'\n",
        "    'model': 'gpt-3.5-turbo',\n",
        "    'temperature': 0.1,\n",
        "    'max_tokens': 500\n",
        "}\n",
        "\n",
        "print(\"✅ Configuración de modelos y parámetros definida\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Carga del Dataset de Evaluación\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_evaluation_dataset(file_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Carga el dataset de evaluación con preguntas y respuestas esperadas\n",
        "    \n",
        "    Estructura esperada:\n",
        "    - question: Pregunta a evaluar\n",
        "    - expected_answer: Respuesta esperada\n",
        "    - expected_articles: Lista de artículos relevantes (IDs o contenido)\n",
        "    - category: Categoría de la pregunta (opcional)\n",
        "    - difficulty: Nivel de dificultad (opcional)\n",
        "    \"\"\"\n",
        "    if file_path.endswith('.json'):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        return pd.DataFrame(data)\n",
        "    elif file_path.endswith('.csv'):\n",
        "        return pd.read_csv(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Formato de archivo no soportado. Use .json o .csv\")\n",
        "\n",
        "# Ejemplo de dataset de evaluación\n",
        "def create_sample_dataset() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Crea un dataset de ejemplo para evaluación\n",
        "    \"\"\"\n",
        "    sample_data = [\n",
        "        {\n",
        "            'question': '¿Cuál es la duración máxima de la jornada laboral?',\n",
        "            'expected_answer': 'La duración máxima de la jornada laboral es de 8 horas diarias.',\n",
        "            'expected_articles': ['art_123', 'art_456'],\n",
        "            'category': 'jornada_laboral',\n",
        "            'difficulty': 'easy'\n",
        "        },\n",
        "        {\n",
        "            'question': '¿Qué derechos tiene un trabajador en caso de despido?',\n",
        "            'expected_answer': 'El trabajador tiene derecho a indemnización, preaviso y otros beneficios.',\n",
        "            'expected_articles': ['art_789', 'art_101'],\n",
        "            'category': 'despido',\n",
        "            'difficulty': 'medium'\n",
        "        },\n",
        "        {\n",
        "            'question': '¿Cómo se calcula la indemnización por despido?',\n",
        "            'expected_answer': 'La indemnización se calcula según la antigüedad y el salario del trabajador.',\n",
        "            'expected_articles': ['art_202', 'art_303'],\n",
        "            'category': 'indemnizacion',\n",
        "            'difficulty': 'hard'\n",
        "        }\n",
        "    ]\n",
        "    return pd.DataFrame(sample_data)\n",
        "\n",
        "# Cargar dataset (usar create_sample_dataset() si no tienes un dataset real)\n",
        "try:\n",
        "    # Intentar cargar dataset real\n",
        "    dataset = load_evaluation_dataset('data/evaluation_dataset.json')\n",
        "    print(f\"✅ Dataset cargado: {len(dataset)} preguntas\")\n",
        "except FileNotFoundError:\n",
        "    # Usar dataset de ejemplo\n",
        "    dataset = create_sample_dataset()\n",
        "    print(f\"⚠️  Usando dataset de ejemplo: {len(dataset)} preguntas\")\n",
        "\n",
        "print(f\"\\nEstructura del dataset:\")\n",
        "print(dataset.head())\n",
        "print(f\"\\nColumnas: {list(dataset.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Conexión con Qdrant y Carga de Modelos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Conexión con Qdrant\n",
        "def connect_to_qdrant() -> QdrantClient:\n",
        "    \"\"\"\n",
        "    Establece conexión con Qdrant\n",
        "    \"\"\"\n",
        "    try:\n",
        "        client = QdrantClient(url=QDRANT_CONFIG['url'])\n",
        "        # Verificar conexión\n",
        "        collections = client.get_collections()\n",
        "        print(f\"✅ Conectado a Qdrant. Colecciones disponibles: {len(collections.collections)}\")\n",
        "        return client\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error conectando a Qdrant: {e}\")\n",
        "        raise\n",
        "\n",
        "# Cargar modelos de embedding\n",
        "def load_embedding_models() -> Dict[str, SentenceTransformer]:\n",
        "    \"\"\"\n",
        "    Carga todos los modelos de embedding configurados\n",
        "    \"\"\"\n",
        "    models = {}\n",
        "    print(\"🔄 Cargando modelos de embedding...\")\n",
        "    \n",
        "    for name, config in EMBEDDING_MODELS.items():\n",
        "        try:\n",
        "            print(f\"  - Cargando {name}...\")\n",
        "            model = SentenceTransformer(config['model_name'])\n",
        "            models[name] = model\n",
        "            print(f\"    ✅ {name} cargado\")\n",
        "        except Exception as e:\n",
        "            print(f\"    ❌ Error cargando {name}: {e}\")\n",
        "    \n",
        "    print(f\"\\n✅ {len(models)} modelos de embedding cargados\")\n",
        "    return models\n",
        "\n",
        "# Cargar modelos de re-ranking\n",
        "def load_reranking_models() -> Dict[str, CrossEncoder]:\n",
        "    \"\"\"\n",
        "    Carga todos los modelos de re-ranking configurados\n",
        "    \"\"\"\n",
        "    models = {}\n",
        "    print(\"🔄 Cargando modelos de re-ranking...\")\n",
        "    \n",
        "    for name, config in RERANKING_MODELS.items():\n",
        "        try:\n",
        "            print(f\"  - Cargando {name}...\")\n",
        "            model = CrossEncoder(config['model_name'])\n",
        "            models[name] = model\n",
        "            print(f\"    ✅ {name} cargado\")\n",
        "        except Exception as e:\n",
        "            print(f\"    ❌ Error cargando {name}: {e}\")\n",
        "    \n",
        "    print(f\"\\n✅ {len(models)} modelos de re-ranking cargados\")\n",
        "    return models\n",
        "\n",
        "# Ejecutar carga\n",
        "qdrant_client = connect_to_qdrant()\n",
        "embedding_models = load_embedding_models()\n",
        "reranking_models = load_reranking_models()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Introducción\n",
        "\n",
        "## Objetivo de la Notebook\n",
        "\n",
        "Esta notebook tiene como objetivo realizar una **evaluación integral del pipeline RAG** (Retrieval-Augmented Generation) para el sistema de consultas sobre derecho laboral paraguayo. La evaluación se realiza en múltiples niveles:\n",
        "\n",
        "### 🎯 Niveles de Evaluación\n",
        "\n",
        "1. **Nivel de Retrieval (Recuperación)**\n",
        "   - Evaluación de diferentes modelos de embeddings\n",
        "   - Comparación de performance en Qdrant\n",
        "   - Métricas objetivas: Recall@k, Precision@k, nDCG@k, MRR\n",
        "\n",
        "2. **Nivel de Re-ranking**\n",
        "   - Evaluación de modelos cross-encoder\n",
        "   - Mejora en la relevancia de documentos recuperados\n",
        "   - Comparación antes vs después del re-ranking\n",
        "\n",
        "3. **Nivel de Generación (LLM)**\n",
        "   - Evaluación de respuestas generadas\n",
        "   - Métricas subjetivas via LLM-as-a-judge\n",
        "   - Análisis de coherencia, relevancia, completitud\n",
        "\n",
        "### 📊 Dataset de Evaluación\n",
        "\n",
        "El dataset contiene:\n",
        "- **Preguntas**: Consultas reales sobre derecho laboral paraguayo\n",
        "- **Respuestas esperadas**: Ground truth para comparación\n",
        "- **Artículos relevantes**: Documentos que deberían ser recuperados\n",
        "- **Categorías**: Clasificación por tipo de consulta\n",
        "- **Dificultad**: Nivel de complejidad de la pregunta\n",
        "\n",
        "### 🔧 Tecnologías Utilizadas\n",
        "\n",
        "- **Qdrant**: Base de datos vectorial para almacenamiento y búsqueda\n",
        "- **Sentence Transformers**: Modelos de embeddings para representación de texto\n",
        "- **Cross-encoders**: Modelos de re-ranking para mejorar relevancia\n",
        "- **OpenAI/Transformers**: Modelos de generación y evaluación\n",
        "- **Scikit-learn**: Cálculo de métricas de evaluación\n",
        "\n",
        "### 💾 Sistema de Persistencia\n",
        "\n",
        "La notebook incluye un sistema para guardar y cargar evaluaciones:\n",
        "- **Guardado automático**: Cada evaluación se guarda en JSON\n",
        "- **Comparación histórica**: Posibilidad de comparar diferentes configuraciones\n",
        "- **Reproducibilidad**: Configuraciones guardadas para replicar experimentos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sistema de Persistencia de Evaluaciones\n",
        "class EvaluationManager:\n",
        "    \"\"\"\n",
        "    Maneja el guardado y carga de evaluaciones para comparación histórica\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, results_dir: str = \"evaluation_results\"):\n",
        "        self.results_dir = Path(results_dir)\n",
        "        self.results_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    def save_evaluation(self, \n",
        "                       evaluation_name: str,\n",
        "                       config: Dict[str, Any],\n",
        "                       results: Dict[str, Any],\n",
        "                       metadata: Optional[Dict[str, Any]] = None) -> str:\n",
        "        \"\"\"\n",
        "        Guarda una evaluación completa con configuración y resultados\n",
        "        \n",
        "        Args:\n",
        "            evaluation_name: Nombre único para la evaluación\n",
        "            config: Configuración de modelos y parámetros usados\n",
        "            results: Resultados de la evaluación\n",
        "            metadata: Metadatos adicionales (timestamp, descripción, etc.)\n",
        "        \n",
        "        Returns:\n",
        "            Ruta del archivo guardado\n",
        "        \"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"{evaluation_name}_{timestamp}.json\"\n",
        "        filepath = self.results_dir / filename\n",
        "        \n",
        "        evaluation_data = {\n",
        "            \"evaluation_name\": evaluation_name,\n",
        "            \"timestamp\": timestamp,\n",
        "            \"config\": config,\n",
        "            \"results\": results,\n",
        "            \"metadata\": metadata or {}\n",
        "        }\n",
        "        \n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(evaluation_data, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        print(f\"✅ Evaluación guardada: {filepath}\")\n",
        "        return str(filepath)\n",
        "    \n",
        "    def load_evaluation(self, filepath: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Carga una evaluación guardada\n",
        "        \n",
        "        Args:\n",
        "            filepath: Ruta del archivo de evaluación\n",
        "        \n",
        "        Returns:\n",
        "            Datos de la evaluación\n",
        "        \"\"\"\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    \n",
        "    def list_evaluations(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Lista todas las evaluaciones guardadas\n",
        "        \n",
        "        Returns:\n",
        "            Lista de evaluaciones con metadatos\n",
        "        \"\"\"\n",
        "        evaluations = []\n",
        "        for filepath in self.results_dir.glob(\"*.json\"):\n",
        "            try:\n",
        "                with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                    data = json.load(f)\n",
        "                evaluations.append({\n",
        "                    \"filepath\": str(filepath),\n",
        "                    \"name\": data.get(\"evaluation_name\", \"unknown\"),\n",
        "                    \"timestamp\": data.get(\"timestamp\", \"unknown\"),\n",
        "                    \"metadata\": data.get(\"metadata\", {})\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️  Error cargando {filepath}: {e}\")\n",
        "        \n",
        "        return sorted(evaluations, key=lambda x: x[\"timestamp\"], reverse=True)\n",
        "    \n",
        "    def compare_evaluations(self, evaluation_paths: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Compara múltiples evaluaciones en una tabla\n",
        "        \n",
        "        Args:\n",
        "            evaluation_paths: Lista de rutas de evaluaciones a comparar\n",
        "        \n",
        "        Returns:\n",
        "            DataFrame con comparación de métricas\n",
        "        \"\"\"\n",
        "        comparison_data = []\n",
        "        \n",
        "        for path in evaluation_paths:\n",
        "            eval_data = self.load_evaluation(path)\n",
        "            name = eval_data[\"evaluation_name\"]\n",
        "            timestamp = eval_data[\"timestamp\"]\n",
        "            results = eval_data[\"results\"]\n",
        "            \n",
        "            # Extraer métricas principales\n",
        "            row = {\n",
        "                \"evaluation\": name,\n",
        "                \"timestamp\": timestamp,\n",
        "                \"config\": eval_data[\"config\"]\n",
        "            }\n",
        "            \n",
        "            # Agregar métricas de retrieval si existen\n",
        "            if \"retrieval_metrics\" in results:\n",
        "                for metric, values in results[\"retrieval_metrics\"].items():\n",
        "                    if isinstance(values, dict):\n",
        "                        for k, v in values.items():\n",
        "                            row[f\"retrieval_{metric}_{k}\"] = v\n",
        "                    else:\n",
        "                        row[f\"retrieval_{metric}\"] = values\n",
        "            \n",
        "            # Agregar métricas de LLM si existen\n",
        "            if \"llm_metrics\" in results:\n",
        "                for metric, values in results[\"llm_metrics\"].items():\n",
        "                    if isinstance(values, dict):\n",
        "                        for k, v in values.items():\n",
        "                            row[f\"llm_{metric}_{k}\"] = v\n",
        "                    else:\n",
        "                        row[f\"llm_{metric}\"] = values\n",
        "            \n",
        "            comparison_data.append(row)\n",
        "        \n",
        "        return pd.DataFrame(comparison_data)\n",
        "\n",
        "# Inicializar el manager de evaluaciones\n",
        "eval_manager = EvaluationManager()\n",
        "print(\"✅ Sistema de persistencia de evaluaciones inicializado\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Carga del Dataset\n",
        "\n",
        "## Estructura del Dataset de Evaluación\n",
        "\n",
        "El dataset de evaluación debe contener preguntas y respuestas esperadas para poder medir la calidad del pipeline RAG. La estructura recomendada incluye:\n",
        "\n",
        "### 📋 Campos Requeridos\n",
        "\n",
        "- **`question`**: Pregunta a evaluar\n",
        "- **`expected_answer`**: Respuesta esperada (ground truth)\n",
        "- **`expected_articles`**: Lista de IDs de artículos relevantes que deberían ser recuperados\n",
        "- **`category`**: Categoría de la pregunta (opcional)\n",
        "- **`difficulty`**: Nivel de dificultad (opcional)\n",
        "\n",
        "### 🎯 Tipos de Dataset Soportados\n",
        "\n",
        "1. **Dataset Real**: Archivo JSON/CSV con preguntas reales del dominio\n",
        "2. **Dataset de Ejemplo**: Dataset sintético para pruebas iniciales\n",
        "3. **Dataset Híbrido**: Combinación de datos reales y sintéticos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funciones de carga y validación del dataset\n",
        "def validate_dataset(df: pd.DataFrame) -> Tuple[bool, List[str]]:\n",
        "    \"\"\"\n",
        "    Valida que el dataset tenga la estructura correcta\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame del dataset\n",
        "    \n",
        "    Returns:\n",
        "        Tuple[bool, List[str]]: (es_válido, lista_de_errores)\n",
        "    \"\"\"\n",
        "    errors = []\n",
        "    required_columns = ['question', 'expected_answer', 'expected_articles']\n",
        "    \n",
        "    # Verificar columnas requeridas\n",
        "    for col in required_columns:\n",
        "        if col not in df.columns:\n",
        "            errors.append(f\"Columna requerida '{col}' no encontrada\")\n",
        "    \n",
        "    if errors:\n",
        "        return False, errors\n",
        "    \n",
        "    # Verificar que no haya valores nulos en columnas requeridas\n",
        "    for col in required_columns:\n",
        "        if df[col].isnull().any():\n",
        "            errors.append(f\"Columna '{col}' contiene valores nulos\")\n",
        "    \n",
        "    # Verificar que expected_articles sea una lista\n",
        "    if 'expected_articles' in df.columns:\n",
        "        for idx, articles in df['expected_articles'].items():\n",
        "            if not isinstance(articles, list):\n",
        "                errors.append(f\"Fila {idx}: expected_articles debe ser una lista\")\n",
        "    \n",
        "    return len(errors) == 0, errors\n",
        "\n",
        "def create_enhanced_sample_dataset() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Crea un dataset de ejemplo más completo para evaluación\n",
        "    \"\"\"\n",
        "    sample_data = [\n",
        "        {\n",
        "            'question': '¿Cuál es la duración máxima de la jornada laboral?',\n",
        "            'expected_answer': 'La duración máxima de la jornada laboral es de 8 horas diarias según el artículo 154 del Código del Trabajo.',\n",
        "            'expected_articles': ['art_154', 'art_155', 'art_156'],\n",
        "            'category': 'jornada_laboral',\n",
        "            'difficulty': 'easy',\n",
        "            'keywords': ['jornada', 'horas', 'duración', 'máxima']\n",
        "        },\n",
        "        {\n",
        "            'question': '¿Qué derechos tiene un trabajador en caso de despido?',\n",
        "            'expected_answer': 'El trabajador tiene derecho a indemnización, preaviso, vacaciones proporcionales y otros beneficios según los artículos 91 y 92.',\n",
        "            'expected_articles': ['art_91', 'art_92', 'art_93', 'art_94'],\n",
        "            'category': 'despido',\n",
        "            'difficulty': 'medium',\n",
        "            'keywords': ['despido', 'derechos', 'indemnización', 'preaviso']\n",
        "        },\n",
        "        {\n",
        "            'question': '¿Cómo se calcula la indemnización por despido?',\n",
        "            'expected_answer': 'La indemnización se calcula multiplicando el salario diario por 30 días por cada año de antigüedad, con un mínimo de 15 días.',\n",
        "            'expected_articles': ['art_91', 'art_92', 'art_95'],\n",
        "            'category': 'indemnizacion',\n",
        "            'difficulty': 'hard',\n",
        "            'keywords': ['indemnización', 'cálculo', 'antigüedad', 'salario']\n",
        "        },\n",
        "        {\n",
        "            'question': '¿Cuáles son las condiciones para el trabajo nocturno?',\n",
        "            'expected_answer': 'El trabajo nocturno tiene condiciones especiales de horario, remuneración y descanso según los artículos 160-165.',\n",
        "            'expected_articles': ['art_160', 'art_161', 'art_162', 'art_163', 'art_164', 'art_165'],\n",
        "            'category': 'trabajo_nocturno',\n",
        "            'difficulty': 'medium',\n",
        "            'keywords': ['nocturno', 'horario', 'remuneración', 'descanso']\n",
        "        },\n",
        "        {\n",
        "            'question': '¿Qué es el salario mínimo y cómo se establece?',\n",
        "            'expected_answer': 'El salario mínimo es la remuneración mínima que debe recibir un trabajador, establecida por el Consejo Nacional del Salario Mínimo.',\n",
        "            'expected_articles': ['art_200', 'art_201', 'art_202'],\n",
        "            'category': 'salario',\n",
        "            'difficulty': 'easy',\n",
        "            'keywords': ['salario', 'mínimo', 'remuneración', 'consejo']\n",
        "        }\n",
        "    ]\n",
        "    return pd.DataFrame(sample_data)\n",
        "\n",
        "def load_and_prepare_dataset(file_path: Optional[str] = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Carga y prepara el dataset de evaluación\n",
        "    \n",
        "    Args:\n",
        "        file_path: Ruta del archivo de dataset (opcional)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame preparado y validado\n",
        "    \"\"\"\n",
        "    if file_path and Path(file_path).exists():\n",
        "        print(f\"🔄 Cargando dataset desde: {file_path}\")\n",
        "        df = load_evaluation_dataset(file_path)\n",
        "    else:\n",
        "        print(\"⚠️  Usando dataset de ejemplo\")\n",
        "        df = create_enhanced_sample_dataset()\n",
        "    \n",
        "    # Validar dataset\n",
        "    is_valid, errors = validate_dataset(df)\n",
        "    \n",
        "    if not is_valid:\n",
        "        print(\"❌ Errores en el dataset:\")\n",
        "        for error in errors:\n",
        "            print(f\"  - {error}\")\n",
        "        raise ValueError(\"Dataset no válido\")\n",
        "    \n",
        "    print(f\"✅ Dataset cargado y validado: {len(df)} preguntas\")\n",
        "    \n",
        "    # Mostrar estadísticas del dataset\n",
        "    print(f\"\\n📊 Estadísticas del dataset:\")\n",
        "    print(f\"  - Total de preguntas: {len(df)}\")\n",
        "    if 'category' in df.columns:\n",
        "        print(f\"  - Categorías: {df['category'].nunique()}\")\n",
        "        print(f\"  - Distribución por categoría:\")\n",
        "        for cat, count in df['category'].value_counts().items():\n",
        "            print(f\"    * {cat}: {count}\")\n",
        "    \n",
        "    if 'difficulty' in df.columns:\n",
        "        print(f\"  - Distribución por dificultad:\")\n",
        "        for diff, count in df['difficulty'].value_counts().items():\n",
        "            print(f\"    * {diff}: {count}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Cargar el dataset\n",
        "print(\"🔄 Cargando dataset de evaluación...\")\n",
        "dataset = load_and_prepare_dataset()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualización del dataset\n",
        "def visualize_dataset_distribution(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Visualiza la distribución del dataset\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('Distribución del Dataset de Evaluación', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Distribución por categoría\n",
        "    if 'category' in df.columns:\n",
        "        category_counts = df['category'].value_counts()\n",
        "        axes[0, 0].pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%')\n",
        "        axes[0, 0].set_title('Distribución por Categoría')\n",
        "    \n",
        "    # Distribución por dificultad\n",
        "    if 'difficulty' in df.columns:\n",
        "        difficulty_counts = df['difficulty'].value_counts()\n",
        "        axes[0, 1].bar(difficulty_counts.index, difficulty_counts.values, color='skyblue')\n",
        "        axes[0, 1].set_title('Distribución por Dificultad')\n",
        "        axes[0, 1].set_xlabel('Dificultad')\n",
        "        axes[0, 1].set_ylabel('Número de Preguntas')\n",
        "    \n",
        "    # Longitud de preguntas\n",
        "    question_lengths = df['question'].str.len()\n",
        "    axes[1, 0].hist(question_lengths, bins=20, color='lightgreen', alpha=0.7)\n",
        "    axes[1, 0].set_title('Distribución de Longitud de Preguntas')\n",
        "    axes[1, 0].set_xlabel('Caracteres')\n",
        "    axes[1, 0].set_ylabel('Frecuencia')\n",
        "    \n",
        "    # Longitud de respuestas esperadas\n",
        "    answer_lengths = df['expected_answer'].str.len()\n",
        "    axes[1, 1].hist(answer_lengths, bins=20, color='lightcoral', alpha=0.7)\n",
        "    axes[1, 1].set_title('Distribución de Longitud de Respuestas')\n",
        "    axes[1, 1].set_xlabel('Caracteres')\n",
        "    axes[1, 1].set_ylabel('Frecuencia')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Estadísticas adicionales\n",
        "    print(f\"\\n📈 Estadísticas detalladas:\")\n",
        "    print(f\"  - Longitud promedio de preguntas: {question_lengths.mean():.1f} caracteres\")\n",
        "    print(f\"  - Longitud promedio de respuestas: {answer_lengths.mean():.1f} caracteres\")\n",
        "    print(f\"  - Número promedio de artículos esperados: {df['expected_articles'].apply(len).mean():.1f}\")\n",
        "\n",
        "# Mostrar algunas preguntas de ejemplo\n",
        "def show_sample_questions(df: pd.DataFrame, n: int = 3):\n",
        "    \"\"\"\n",
        "    Muestra preguntas de ejemplo del dataset\n",
        "    \"\"\"\n",
        "    print(f\"\\n📝 Ejemplos de preguntas del dataset:\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    for idx, row in df.head(n).iterrows():\n",
        "        print(f\"\\n🔹 Pregunta {idx + 1}:\")\n",
        "        print(f\"   Pregunta: {row['question']}\")\n",
        "        print(f\"   Respuesta esperada: {row['expected_answer']}\")\n",
        "        print(f\"   Artículos relevantes: {row['expected_articles']}\")\n",
        "        if 'category' in row:\n",
        "            print(f\"   Categoría: {row['category']}\")\n",
        "        if 'difficulty' in row:\n",
        "            print(f\"   Dificultad: {row['difficulty']}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "# Ejecutar visualizaciones\n",
        "visualize_dataset_distribution(dataset)\n",
        "show_sample_questions(dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Configuración del Entorno\n",
        "\n",
        "## Configuración de Servicios y Modelos\n",
        "\n",
        "Esta sección establece la conexión con todos los servicios necesarios para la evaluación del pipeline RAG:\n",
        "\n",
        "### 🔧 Servicios a Configurar\n",
        "\n",
        "1. **Qdrant**: Base de datos vectorial para retrieval\n",
        "2. **Modelos de Embedding**: Para representación de texto\n",
        "3. **Modelos de Re-ranking**: Para mejorar relevancia de documentos\n",
        "4. **LLM**: Para generación y evaluación de respuestas\n",
        "5. **Sistema de Métricas**: Para cálculo de métricas de evaluación\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuración avanzada del entorno\n",
        "class EnvironmentManager:\n",
        "    \"\"\"\n",
        "    Maneja la configuración y conexión de todos los servicios\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.qdrant_client = None\n",
        "        self.embedding_models = {}\n",
        "        self.reranking_models = {}\n",
        "        self.llm_pipeline = None\n",
        "        self.evaluation_metrics = {}\n",
        "        \n",
        "    def setup_qdrant(self) -> bool:\n",
        "        \"\"\"\n",
        "        Configura la conexión con Qdrant\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"🔄 Configurando Qdrant...\")\n",
        "            self.qdrant_client = QdrantClient(url=QDRANT_CONFIG['url'])\n",
        "            \n",
        "            # Verificar conexión\n",
        "            collections = self.qdrant_client.get_collections()\n",
        "            print(f\"✅ Qdrant conectado. Colecciones: {len(collections.collections)}\")\n",
        "            \n",
        "            # Verificar colección específica\n",
        "            collection_name = QDRANT_CONFIG['collection_name']\n",
        "            try:\n",
        "                collection_info = self.qdrant_client.get_collection(collection_name)\n",
        "                print(f\"✅ Colección '{collection_name}' encontrada:\")\n",
        "                print(f\"   - Puntos: {collection_info.points_count}\")\n",
        "                print(f\"   - Dimensiones: {collection_info.config.params.vectors.size}\")\n",
        "                print(f\"   - Distancia: {collection_info.config.params.vectors.distance}\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️  Colección '{collection_name}' no encontrada: {e}\")\n",
        "                print(\"   Asegúrate de que la colección existe y tiene datos\")\n",
        "            \n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error configurando Qdrant: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def setup_embedding_models(self) -> bool:\n",
        "        \"\"\"\n",
        "        Carga todos los modelos de embedding configurados\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"🔄 Cargando modelos de embedding...\")\n",
        "            self.embedding_models = {}\n",
        "            \n",
        "            for name, config in EMBEDDING_MODELS.items():\n",
        "                try:\n",
        "                    print(f\"  - Cargando {name}...\")\n",
        "                    model = SentenceTransformer(config['model_name'])\n",
        "                    self.embedding_models[name] = {\n",
        "                        'model': model,\n",
        "                        'config': config\n",
        "                    }\n",
        "                    print(f\"    ✅ {name} cargado (dimensión: {config['dimension']})\")\n",
        "                except Exception as e:\n",
        "                    print(f\"    ❌ Error cargando {name}: {e}\")\n",
        "            \n",
        "            print(f\"✅ {len(self.embedding_models)} modelos de embedding cargados\")\n",
        "            return len(self.embedding_models) > 0\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error configurando modelos de embedding: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def setup_reranking_models(self) -> bool:\n",
        "        \"\"\"\n",
        "        Carga todos los modelos de re-ranking configurados\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"🔄 Cargando modelos de re-ranking...\")\n",
        "            self.reranking_models = {}\n",
        "            \n",
        "            for name, config in RERANKING_MODELS.items():\n",
        "                try:\n",
        "                    print(f\"  - Cargando {name}...\")\n",
        "                    model = CrossEncoder(config['model_name'])\n",
        "                    self.reranking_models[name] = {\n",
        "                        'model': model,\n",
        "                        'config': config\n",
        "                    }\n",
        "                    print(f\"    ✅ {name} cargado\")\n",
        "                except Exception as e:\n",
        "                    print(f\"    ❌ Error cargando {name}: {e}\")\n",
        "            \n",
        "            print(f\"✅ {len(self.reranking_models)} modelos de re-ranking cargados\")\n",
        "            return len(self.reranking_models) > 0\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error configurando modelos de re-ranking: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def setup_llm_pipeline(self) -> bool:\n",
        "        \"\"\"\n",
        "        Configura el pipeline de LLM para generación y evaluación\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"🔄 Configurando LLM pipeline...\")\n",
        "            \n",
        "            if LLM_CONFIG['provider'] == 'openai':\n",
        "                # Configurar OpenAI\n",
        "                openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "                if not openai.api_key:\n",
        "                    print(\"⚠️  OPENAI_API_KEY no encontrada. Usando modelo local.\")\n",
        "                    return self._setup_local_llm()\n",
        "                \n",
        "                self.llm_pipeline = {\n",
        "                    'provider': 'openai',\n",
        "                    'model': LLM_CONFIG['model'],\n",
        "                    'temperature': LLM_CONFIG['temperature'],\n",
        "                    'max_tokens': LLM_CONFIG['max_tokens']\n",
        "                }\n",
        "                print(f\"✅ OpenAI configurado: {LLM_CONFIG['model']}\")\n",
        "                \n",
        "            else:\n",
        "                return self._setup_local_llm()\n",
        "            \n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error configurando LLM: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def _setup_local_llm(self) -> bool:\n",
        "        \"\"\"\n",
        "        Configura un modelo local como fallback\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"🔄 Configurando modelo local...\")\n",
        "            model_name = \"microsoft/DialoGPT-medium\"  # Modelo más ligero\n",
        "            \n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "            \n",
        "            self.llm_pipeline = {\n",
        "                'provider': 'huggingface',\n",
        "                'model': model,\n",
        "                'tokenizer': tokenizer,\n",
        "                'model_name': model_name\n",
        "            }\n",
        "            \n",
        "            print(f\"✅ Modelo local configurado: {model_name}\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error configurando modelo local: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def setup_evaluation_metrics(self) -> bool:\n",
        "        \"\"\"\n",
        "        Configura el sistema de métricas de evaluación\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"🔄 Configurando sistema de métricas...\")\n",
        "            \n",
        "            self.evaluation_metrics = {\n",
        "                'retrieval_metrics': RETRIEVAL_METRICS,\n",
        "                'llm_criteria': LLM_EVALUATION_CRITERIA,\n",
        "                'custom_metrics': {}\n",
        "            }\n",
        "            \n",
        "            print(\"✅ Sistema de métricas configurado\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error configurando métricas: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def initialize_all(self) -> Dict[str, bool]:\n",
        "        \"\"\"\n",
        "        Inicializa todos los servicios\n",
        "        \"\"\"\n",
        "        print(\"🚀 Inicializando entorno completo...\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        results = {\n",
        "            'qdrant': self.setup_qdrant(),\n",
        "            'embedding_models': self.setup_embedding_models(),\n",
        "            'reranking_models': self.setup_reranking_models(),\n",
        "            'llm_pipeline': self.setup_llm_pipeline(),\n",
        "            'evaluation_metrics': self.setup_evaluation_metrics()\n",
        "        }\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"📊 Resumen de inicialización:\")\n",
        "        for service, status in results.items():\n",
        "            status_icon = \"✅\" if status else \"❌\"\n",
        "            print(f\"  {status_icon} {service}: {'OK' if status else 'ERROR'}\")\n",
        "        \n",
        "        all_ok = all(results.values())\n",
        "        if all_ok:\n",
        "            print(\"\\n🎉 ¡Entorno completamente configurado!\")\n",
        "        else:\n",
        "            print(\"\\n⚠️  Algunos servicios no se configuraron correctamente\")\n",
        "        \n",
        "        return results\n",
        "\n",
        "# Inicializar el manager del entorno\n",
        "env_manager = EnvironmentManager()\n",
        "initialization_results = env_manager.initialize_all()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funciones de utilidad para el entorno\n",
        "def get_environment_status() -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Obtiene el estado actual del entorno\n",
        "    \"\"\"\n",
        "    status = {\n",
        "        'qdrant': {\n",
        "            'connected': env_manager.qdrant_client is not None,\n",
        "            'collections_available': 0\n",
        "        },\n",
        "        'embedding_models': {\n",
        "            'loaded': len(env_manager.embedding_models),\n",
        "            'models': list(env_manager.embedding_models.keys())\n",
        "        },\n",
        "        'reranking_models': {\n",
        "            'loaded': len(env_manager.reranking_models),\n",
        "            'models': list(env_manager.reranking_models.keys())\n",
        "        },\n",
        "        'llm_pipeline': {\n",
        "            'configured': env_manager.llm_pipeline is not None,\n",
        "            'provider': env_manager.llm_pipeline.get('provider') if env_manager.llm_pipeline else None\n",
        "        },\n",
        "        'evaluation_metrics': {\n",
        "            'configured': len(env_manager.evaluation_metrics) > 0\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Obtener información adicional de Qdrant\n",
        "    if env_manager.qdrant_client:\n",
        "        try:\n",
        "            collections = env_manager.qdrant_client.get_collections()\n",
        "            status['qdrant']['collections_available'] = len(collections.collections)\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    return status\n",
        "\n",
        "def test_environment_connectivity() -> Dict[str, bool]:\n",
        "    \"\"\"\n",
        "    Prueba la conectividad de todos los servicios\n",
        "    \"\"\"\n",
        "    print(\"🧪 Probando conectividad del entorno...\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    tests = {}\n",
        "    \n",
        "    # Test Qdrant\n",
        "    if env_manager.qdrant_client:\n",
        "        try:\n",
        "            collections = env_manager.qdrant_client.get_collections()\n",
        "            tests['qdrant'] = True\n",
        "            print(\"✅ Qdrant: Conectado\")\n",
        "        except Exception as e:\n",
        "            tests['qdrant'] = False\n",
        "            print(f\"❌ Qdrant: Error - {e}\")\n",
        "    else:\n",
        "        tests['qdrant'] = False\n",
        "        print(\"❌ Qdrant: No configurado\")\n",
        "    \n",
        "    # Test Embedding Models\n",
        "    if env_manager.embedding_models:\n",
        "        try:\n",
        "            # Probar con un modelo\n",
        "            test_model = list(env_manager.embedding_models.values())[0]['model']\n",
        "            test_embedding = test_model.encode([\"test\"])\n",
        "            tests['embedding_models'] = True\n",
        "            print(f\"✅ Embedding Models: {len(env_manager.embedding_models)} modelos funcionando\")\n",
        "        except Exception as e:\n",
        "            tests['embedding_models'] = False\n",
        "            print(f\"❌ Embedding Models: Error - {e}\")\n",
        "    else:\n",
        "        tests['embedding_models'] = False\n",
        "        print(\"❌ Embedding Models: No configurados\")\n",
        "    \n",
        "    # Test Re-ranking Models\n",
        "    if env_manager.reranking_models:\n",
        "        try:\n",
        "            # Probar con un modelo\n",
        "            test_model = list(env_manager.reranking_models.values())[0]['model']\n",
        "            test_scores = test_model.predict([(\"test query\", \"test document\")])\n",
        "            tests['reranking_models'] = True\n",
        "            print(f\"✅ Re-ranking Models: {len(env_manager.reranking_models)} modelos funcionando\")\n",
        "        except Exception as e:\n",
        "            tests['reranking_models'] = False\n",
        "            print(f\"❌ Re-ranking Models: Error - {e}\")\n",
        "    else:\n",
        "        tests['reranking_models'] = False\n",
        "        print(\"❌ Re-ranking Models: No configurados\")\n",
        "    \n",
        "    # Test LLM Pipeline\n",
        "    if env_manager.llm_pipeline:\n",
        "        try:\n",
        "            if env_manager.llm_pipeline['provider'] == 'openai':\n",
        "                # Test básico de OpenAI\n",
        "                tests['llm_pipeline'] = True\n",
        "                print(\"✅ LLM Pipeline: OpenAI configurado\")\n",
        "            else:\n",
        "                # Test modelo local\n",
        "                tests['llm_pipeline'] = True\n",
        "                print(\"✅ LLM Pipeline: Modelo local configurado\")\n",
        "        except Exception as e:\n",
        "            tests['llm_pipeline'] = False\n",
        "            print(f\"❌ LLM Pipeline: Error - {e}\")\n",
        "    else:\n",
        "        tests['llm_pipeline'] = False\n",
        "        print(\"❌ LLM Pipeline: No configurado\")\n",
        "    \n",
        "    print(\"=\" * 40)\n",
        "    successful_tests = sum(tests.values())\n",
        "    total_tests = len(tests)\n",
        "    print(f\"📊 Resultado: {successful_tests}/{total_tests} servicios funcionando\")\n",
        "    \n",
        "    return tests\n",
        "\n",
        "def display_environment_info():\n",
        "    \"\"\"\n",
        "    Muestra información detallada del entorno\n",
        "    \"\"\"\n",
        "    print(\"\\n🔍 Información del Entorno\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    status = get_environment_status()\n",
        "    \n",
        "    # Qdrant\n",
        "    print(f\"\\n🗄️  Qdrant:\")\n",
        "    print(f\"   - Conectado: {'Sí' if status['qdrant']['connected'] else 'No'}\")\n",
        "    print(f\"   - Colecciones disponibles: {status['qdrant']['collections_available']}\")\n",
        "    \n",
        "    # Embedding Models\n",
        "    print(f\"\\n🧠 Modelos de Embedding:\")\n",
        "    print(f\"   - Cargados: {status['embedding_models']['loaded']}\")\n",
        "    for model in status['embedding_models']['models']:\n",
        "        print(f\"     * {model}\")\n",
        "    \n",
        "    # Re-ranking Models\n",
        "    print(f\"\\n🔄 Modelos de Re-ranking:\")\n",
        "    print(f\"   - Cargados: {status['reranking_models']['loaded']}\")\n",
        "    for model in status['reranking_models']['models']:\n",
        "        print(f\"     * {model}\")\n",
        "    \n",
        "    # LLM Pipeline\n",
        "    print(f\"\\n🤖 LLM Pipeline:\")\n",
        "    print(f\"   - Configurado: {'Sí' if status['llm_pipeline']['configured'] else 'No'}\")\n",
        "    if status['llm_pipeline']['provider']:\n",
        "        print(f\"   - Proveedor: {status['llm_pipeline']['provider']}\")\n",
        "    \n",
        "    # Evaluation Metrics\n",
        "    print(f\"\\n📊 Métricas de Evaluación:\")\n",
        "    print(f\"   - Configuradas: {'Sí' if status['evaluation_metrics']['configured'] else 'No'}\")\n",
        "\n",
        "# Ejecutar pruebas y mostrar información\n",
        "connectivity_results = test_environment_connectivity()\n",
        "display_environment_info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Evaluación de Retrievers (Embeddings + Qdrant)\n",
        "\n",
        "## Objetivo de la Evaluación\n",
        "\n",
        "Esta sección evalúa la calidad del sistema de recuperación de documentos usando diferentes modelos de embeddings y Qdrant. Se miden métricas objetivas para determinar qué modelo de embedding funciona mejor para el dominio del derecho laboral paraguayo.\n",
        "\n",
        "### 🎯 Métricas de Evaluación\n",
        "\n",
        "- **Recall@k**: Proporción de documentos relevantes recuperados en los top-k\n",
        "- **Precision@k**: Proporción de documentos relevantes entre los top-k recuperados\n",
        "- **nDCG@k**: Normalized Discounted Cumulative Gain - considera el ranking\n",
        "- **MRR**: Mean Reciprocal Rank - posición del primer documento relevante\n",
        "- **Hit Rate@k**: Proporción de consultas que tienen al menos un documento relevante en top-k\n",
        "\n",
        "### 📊 Proceso de Evaluación\n",
        "\n",
        "1. **Generación de embeddings** para cada pregunta del dataset\n",
        "2. **Búsqueda en Qdrant** usando diferentes modelos de embedding\n",
        "3. **Cálculo de métricas** comparando con documentos esperados\n",
        "4. **Análisis comparativo** entre modelos\n",
        "5. **Visualización de resultados** para identificar patrones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clase para evaluación de retrievers\n",
        "class RetrieverEvaluator:\n",
        "    \"\"\"\n",
        "    Evalúa la calidad de diferentes modelos de embedding para retrieval\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, qdrant_client, embedding_models, collection_name, top_k=20):\n",
        "        self.qdrant_client = qdrant_client\n",
        "        self.embedding_models = embedding_models\n",
        "        self.collection_name = collection_name\n",
        "        self.top_k = top_k\n",
        "        self.results = {}\n",
        "    \n",
        "    def search_documents(self, query: str, embedding_model_name: str, top_k: int = None) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Busca documentos en Qdrant usando un modelo de embedding específico\n",
        "        \n",
        "        Args:\n",
        "            query: Pregunta a buscar\n",
        "            embedding_model_name: Nombre del modelo de embedding\n",
        "            top_k: Número de documentos a recuperar\n",
        "        \n",
        "        Returns:\n",
        "            Lista de documentos recuperados con scores\n",
        "        \"\"\"\n",
        "        if top_k is None:\n",
        "            top_k = self.top_k\n",
        "        \n",
        "        try:\n",
        "            # Obtener modelo de embedding\n",
        "            model_info = self.embedding_models[embedding_model_name]\n",
        "            model = model_info['model']\n",
        "            \n",
        "            # Generar embedding de la consulta\n",
        "            query_embedding = model.encode([query])[0].tolist()\n",
        "            \n",
        "            # Buscar en Qdrant\n",
        "            search_results = self.qdrant_client.search(\n",
        "                collection_name=self.collection_name,\n",
        "                query_vector=query_embedding,\n",
        "                limit=top_k,\n",
        "                with_payload=True,\n",
        "                with_vectors=False\n",
        "            )\n",
        "            \n",
        "            # Formatear resultados\n",
        "            documents = []\n",
        "            for result in search_results:\n",
        "                documents.append({\n",
        "                    'id': result.id,\n",
        "                    'score': result.score,\n",
        "                    'payload': result.payload\n",
        "                })\n",
        "            \n",
        "            return documents\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error en búsqueda con {embedding_model_name}: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def calculate_recall_at_k(self, retrieved_ids: List[str], expected_ids: List[str], k: int) -> float:\n",
        "        \"\"\"\n",
        "        Calcula Recall@k\n",
        "        \n",
        "        Args:\n",
        "            retrieved_ids: IDs de documentos recuperados\n",
        "            expected_ids: IDs de documentos esperados\n",
        "            k: Número de documentos a considerar\n",
        "        \n",
        "        Returns:\n",
        "            Recall@k score\n",
        "        \"\"\"\n",
        "        if not expected_ids:\n",
        "            return 0.0\n",
        "        \n",
        "        retrieved_k = set(retrieved_ids[:k])\n",
        "        expected_set = set(expected_ids)\n",
        "        \n",
        "        intersection = retrieved_k.intersection(expected_set)\n",
        "        return len(intersection) / len(expected_set)\n",
        "    \n",
        "    def calculate_precision_at_k(self, retrieved_ids: List[str], expected_ids: List[str], k: int) -> float:\n",
        "        \"\"\"\n",
        "        Calcula Precision@k\n",
        "        \n",
        "        Args:\n",
        "            retrieved_ids: IDs de documentos recuperados\n",
        "            expected_ids: IDs de documentos esperados\n",
        "            k: Número de documentos a considerar\n",
        "        \n",
        "        Returns:\n",
        "            Precision@k score\n",
        "        \"\"\"\n",
        "        if k == 0:\n",
        "            return 0.0\n",
        "        \n",
        "        retrieved_k = set(retrieved_ids[:k])\n",
        "        expected_set = set(expected_ids)\n",
        "        \n",
        "        intersection = retrieved_k.intersection(expected_set)\n",
        "        return len(intersection) / k\n",
        "    \n",
        "    def calculate_ndcg_at_k(self, retrieved_ids: List[str], expected_ids: List[str], k: int) -> float:\n",
        "        \"\"\"\n",
        "        Calcula nDCG@k\n",
        "        \n",
        "        Args:\n",
        "            retrieved_ids: IDs de documentos recuperados\n",
        "            expected_ids: IDs de documentos esperados\n",
        "            k: Número de documentos a considerar\n",
        "        \n",
        "        Returns:\n",
        "            nDCG@k score\n",
        "        \"\"\"\n",
        "        if not expected_ids or k == 0:\n",
        "            return 0.0\n",
        "        \n",
        "        # Crear relevancia binaria\n",
        "        relevance = [1 if doc_id in expected_ids else 0 for doc_id in retrieved_ids[:k]]\n",
        "        \n",
        "        # Calcular DCG\n",
        "        dcg = 0.0\n",
        "        for i, rel in enumerate(relevance):\n",
        "            dcg += rel / np.log2(i + 2)  # i+2 porque el log2(1) = 0\n",
        "        \n",
        "        # Calcular IDCG (ideal DCG)\n",
        "        ideal_relevance = [1] * min(len(expected_ids), k)\n",
        "        idcg = 0.0\n",
        "        for i, rel in enumerate(ideal_relevance):\n",
        "            idcg += rel / np.log2(i + 2)\n",
        "        \n",
        "        return dcg / idcg if idcg > 0 else 0.0\n",
        "    \n",
        "    def calculate_mrr(self, retrieved_ids: List[str], expected_ids: List[str]) -> float:\n",
        "        \"\"\"\n",
        "        Calcula Mean Reciprocal Rank\n",
        "        \n",
        "        Args:\n",
        "            retrieved_ids: IDs de documentos recuperados\n",
        "            expected_ids: IDs de documentos esperados\n",
        "        \n",
        "        Returns:\n",
        "            MRR score\n",
        "        \"\"\"\n",
        "        if not expected_ids:\n",
        "            return 0.0\n",
        "        \n",
        "        expected_set = set(expected_ids)\n",
        "        for i, doc_id in enumerate(retrieved_ids):\n",
        "            if doc_id in expected_set:\n",
        "                return 1.0 / (i + 1)\n",
        "        \n",
        "        return 0.0\n",
        "    \n",
        "    def calculate_hit_rate_at_k(self, retrieved_ids: List[str], expected_ids: List[str], k: int) -> float:\n",
        "        \"\"\"\n",
        "        Calcula Hit Rate@k\n",
        "        \n",
        "        Args:\n",
        "            retrieved_ids: IDs de documentos recuperados\n",
        "            expected_ids: IDs de documentos esperados\n",
        "            k: Número de documentos a considerar\n",
        "        \n",
        "        Returns:\n",
        "            Hit Rate@k score (0 o 1)\n",
        "        \"\"\"\n",
        "        if not expected_ids:\n",
        "            return 0.0\n",
        "        \n",
        "        retrieved_k = set(retrieved_ids[:k])\n",
        "        expected_set = set(expected_ids)\n",
        "        \n",
        "        intersection = retrieved_k.intersection(expected_set)\n",
        "        return 1.0 if len(intersection) > 0 else 0.0\n",
        "    \n",
        "    def evaluate_single_query(self, query: str, expected_articles: List[str], \n",
        "                            embedding_model_name: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Evalúa una sola consulta con un modelo de embedding\n",
        "        \n",
        "        Args:\n",
        "            query: Pregunta a evaluar\n",
        "            expected_articles: Lista de artículos esperados\n",
        "            embedding_model_name: Nombre del modelo de embedding\n",
        "        \n",
        "        Returns:\n",
        "            Diccionario con métricas calculadas\n",
        "        \"\"\"\n",
        "        # Buscar documentos\n",
        "        documents = self.search_documents(query, embedding_model_name)\n",
        "        retrieved_ids = [doc['id'] for doc in documents]\n",
        "        \n",
        "        # Calcular métricas\n",
        "        metrics = {}\n",
        "        \n",
        "        # Recall@k\n",
        "        for k in RETRIEVAL_METRICS['recall_at_k']:\n",
        "            metrics[f'recall_at_{k}'] = self.calculate_recall_at_k(retrieved_ids, expected_articles, k)\n",
        "        \n",
        "        # Precision@k\n",
        "        for k in RETRIEVAL_METRICS['precision_at_k']:\n",
        "            metrics[f'precision_at_{k}'] = self.calculate_precision_at_k(retrieved_ids, expected_articles, k)\n",
        "        \n",
        "        # nDCG@k\n",
        "        for k in RETRIEVAL_METRICS['ndcg_at_k']:\n",
        "            metrics[f'ndcg_at_{k}'] = self.calculate_ndcg_at_k(retrieved_ids, expected_articles, k)\n",
        "        \n",
        "        # MRR\n",
        "        if RETRIEVAL_METRICS['mrr']:\n",
        "            metrics['mrr'] = self.calculate_mrr(retrieved_ids, expected_articles)\n",
        "        \n",
        "        # Hit Rate@k\n",
        "        for k in RETRIEVAL_METRICS['hit_rate_at_k']:\n",
        "            metrics[f'hit_rate_at_{k}'] = self.calculate_hit_rate_at_k(retrieved_ids, expected_articles, k)\n",
        "        \n",
        "        # Información adicional\n",
        "        metrics['total_retrieved'] = len(retrieved_ids)\n",
        "        metrics['total_expected'] = len(expected_articles)\n",
        "        metrics['retrieved_ids'] = retrieved_ids[:10]  # Primeros 10 para debugging\n",
        "        \n",
        "        return metrics\n",
        "\n",
        "# Inicializar evaluador\n",
        "retriever_evaluator = RetrieverEvaluator(\n",
        "    qdrant_client=env_manager.qdrant_client,\n",
        "    embedding_models=env_manager.embedding_models,\n",
        "    collection_name=QDRANT_CONFIG['collection_name'],\n",
        "    top_k=QDRANT_CONFIG['top_k']\n",
        ")\n",
        "\n",
        "print(\"✅ Evaluador de retrievers inicializado\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función para ejecutar evaluación completa de retrievers\n",
        "def evaluate_all_retrievers(dataset: pd.DataFrame, embedding_models: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Evalúa todos los modelos de embedding en el dataset completo\n",
        "    \n",
        "    Args:\n",
        "        dataset: DataFrame con preguntas y artículos esperados\n",
        "        embedding_models: Diccionario con modelos de embedding\n",
        "    \n",
        "    Returns:\n",
        "        Diccionario con resultados de evaluación\n",
        "    \"\"\"\n",
        "    print(\"🚀 Iniciando evaluación completa de retrievers...\")\n",
        "    print(f\"📊 Evaluando {len(dataset)} preguntas con {len(embedding_models)} modelos\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    all_results = {}\n",
        "    \n",
        "    for model_name in embedding_models.keys():\n",
        "        print(f\"\\n🔄 Evaluando modelo: {model_name}\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        model_results = {\n",
        "            'model_name': model_name,\n",
        "            'query_results': [],\n",
        "            'aggregated_metrics': {}\n",
        "        }\n",
        "        \n",
        "        # Evaluar cada pregunta\n",
        "        for idx, row in dataset.iterrows():\n",
        "            query = row['question']\n",
        "            expected_articles = row['expected_articles']\n",
        "            \n",
        "            print(f\"  📝 Pregunta {idx + 1}/{len(dataset)}: {query[:50]}...\")\n",
        "            \n",
        "            try:\n",
        "                # Evaluar consulta\n",
        "                query_metrics = retriever_evaluator.evaluate_single_query(\n",
        "                    query=query,\n",
        "                    expected_articles=expected_articles,\n",
        "                    embedding_model_name=model_name\n",
        "                )\n",
        "                \n",
        "                # Agregar información de la consulta\n",
        "                query_metrics['query'] = query\n",
        "                query_metrics['expected_articles'] = expected_articles\n",
        "                query_metrics['query_id'] = idx\n",
        "                \n",
        "                model_results['query_results'].append(query_metrics)\n",
        "                \n",
        "                # Mostrar métricas principales\n",
        "                print(f\"    📈 Recall@5: {query_metrics['recall_at_5']:.3f}, \"\n",
        "                      f\"Precision@5: {query_metrics['precision_at_5']:.3f}, \"\n",
        "                      f\"nDCG@5: {query_metrics['ndcg_at_5']:.3f}\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"    ❌ Error en pregunta {idx + 1}: {e}\")\n",
        "                continue\n",
        "        \n",
        "        # Calcular métricas agregadas\n",
        "        if model_results['query_results']:\n",
        "            print(f\"\\n  📊 Calculando métricas agregadas para {model_name}...\")\n",
        "            aggregated = calculate_aggregated_metrics(model_results['query_results'])\n",
        "            model_results['aggregated_metrics'] = aggregated\n",
        "            \n",
        "            # Mostrar resumen\n",
        "            print(f\"    🎯 Resumen de {model_name}:\")\n",
        "            print(f\"      - Recall@5 promedio: {aggregated['recall_at_5']['mean']:.3f}\")\n",
        "            print(f\"      - Precision@5 promedio: {aggregated['precision_at_5']['mean']:.3f}\")\n",
        "            print(f\"      - nDCG@5 promedio: {aggregated['ndcg_at_5']['mean']:.3f}\")\n",
        "            print(f\"      - MRR promedio: {aggregated['mrr']['mean']:.3f}\")\n",
        "        \n",
        "        all_results[model_name] = model_results\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"✅ Evaluación de retrievers completada\")\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "def calculate_aggregated_metrics(query_results: List[Dict[str, Any]]) -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Calcula métricas agregadas (promedio, desviación estándar) de los resultados de consultas\n",
        "    \n",
        "    Args:\n",
        "        query_results: Lista de resultados de consultas individuales\n",
        "    \n",
        "    Returns:\n",
        "        Diccionario con métricas agregadas\n",
        "    \"\"\"\n",
        "    if not query_results:\n",
        "        return {}\n",
        "    \n",
        "    # Obtener todas las métricas disponibles\n",
        "    metric_names = [key for key in query_results[0].keys() \n",
        "                   if key not in ['query', 'expected_articles', 'query_id', 'retrieved_ids', \n",
        "                                 'total_retrieved', 'total_expected']]\n",
        "    \n",
        "    aggregated = {}\n",
        "    \n",
        "    for metric_name in metric_names:\n",
        "        values = [result[metric_name] for result in query_results if metric_name in result]\n",
        "        \n",
        "        if values:\n",
        "            aggregated[metric_name] = {\n",
        "                'mean': np.mean(values),\n",
        "                'std': np.std(values),\n",
        "                'min': np.min(values),\n",
        "                'max': np.max(values),\n",
        "                'median': np.median(values)\n",
        "            }\n",
        "    \n",
        "    return aggregated\n",
        "\n",
        "# Ejecutar evaluación de retrievers\n",
        "print(\"🔄 Iniciando evaluación de retrievers...\")\n",
        "retrieval_results = evaluate_all_retrievers(dataset, env_manager.embedding_models)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualización de resultados de retrievers\n",
        "def visualize_retrieval_results(retrieval_results: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Visualiza los resultados de evaluación de retrievers\n",
        "    \"\"\"\n",
        "    if not retrieval_results:\n",
        "        print(\"❌ No hay resultados para visualizar\")\n",
        "        return\n",
        "    \n",
        "    # Preparar datos para visualización\n",
        "    models = list(retrieval_results.keys())\n",
        "    metrics_to_plot = ['recall_at_5', 'precision_at_5', 'ndcg_at_5', 'mrr']\n",
        "    \n",
        "    # Crear figura con subplots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Comparación de Modelos de Embedding - Métricas de Retrieval', \n",
        "                 fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Colores para cada modelo\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(models)))\n",
        "    \n",
        "    for idx, metric in enumerate(metrics_to_plot):\n",
        "        row = idx // 2\n",
        "        col = idx % 2\n",
        "        \n",
        "        # Extraer datos para la métrica\n",
        "        model_names = []\n",
        "        mean_values = []\n",
        "        std_values = []\n",
        "        \n",
        "        for model_name in models:\n",
        "            if metric in retrieval_results[model_name]['aggregated_metrics']:\n",
        "                model_names.append(model_name)\n",
        "                mean_values.append(retrieval_results[model_name]['aggregated_metrics'][metric]['mean'])\n",
        "                std_values.append(retrieval_results[model_name]['aggregated_metrics'][metric]['std'])\n",
        "        \n",
        "        if model_names:\n",
        "            # Crear gráfico de barras con barras de error\n",
        "            bars = axes[row, col].bar(model_names, mean_values, yerr=std_values, \n",
        "                                    color=colors[:len(model_names)], alpha=0.7, capsize=5)\n",
        "            axes[row, col].set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
        "            axes[row, col].set_ylabel('Score')\n",
        "            axes[row, col].set_ylim(0, 1)\n",
        "            \n",
        "            # Rotar etiquetas del eje x\n",
        "            axes[row, col].tick_params(axis='x', rotation=45)\n",
        "            \n",
        "            # Agregar valores en las barras\n",
        "            for bar, mean_val in zip(bars, mean_values):\n",
        "                height = bar.get_height()\n",
        "                axes[row, col].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                                  f'{mean_val:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Crear tabla comparativa\n",
        "    print(\"\\n📊 Tabla Comparativa de Modelos:\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Preparar datos para la tabla\n",
        "    comparison_data = []\n",
        "    for model_name in models:\n",
        "        row = {'Modelo': model_name}\n",
        "        aggregated = retrieval_results[model_name]['aggregated_metrics']\n",
        "        \n",
        "        for metric in metrics_to_plot:\n",
        "            if metric in aggregated:\n",
        "                row[metric] = f\"{aggregated[metric]['mean']:.3f} ± {aggregated[metric]['std']:.3f}\"\n",
        "            else:\n",
        "                row[metric] = \"N/A\"\n",
        "        \n",
        "        comparison_data.append(row)\n",
        "    \n",
        "    # Crear DataFrame y mostrar\n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    print(comparison_df.to_string(index=False))\n",
        "    \n",
        "    # Identificar mejor modelo\n",
        "    print(\"\\n🏆 Análisis de Mejores Modelos:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    for metric in metrics_to_plot:\n",
        "        best_model = None\n",
        "        best_score = -1\n",
        "        \n",
        "        for model_name in models:\n",
        "            if metric in retrieval_results[model_name]['aggregated_metrics']:\n",
        "                score = retrieval_results[model_name]['aggregated_metrics'][metric]['mean']\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_model = model_name\n",
        "        \n",
        "        if best_model:\n",
        "            print(f\"  {metric.replace('_', ' ').title()}: {best_model} ({best_score:.3f})\")\n",
        "\n",
        "def analyze_retrieval_performance(retrieval_results: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Analiza el rendimiento de los retrievers en detalle\n",
        "    \"\"\"\n",
        "    print(\"\\n🔍 Análisis Detallado de Rendimiento:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    for model_name, results in retrieval_results.items():\n",
        "        print(f\"\\n📈 Modelo: {model_name}\")\n",
        "        print(\"-\" * 30)\n",
        "        \n",
        "        aggregated = results['aggregated_metrics']\n",
        "        query_results = results['query_results']\n",
        "        \n",
        "        if not aggregated:\n",
        "            print(\"  ❌ No hay métricas disponibles\")\n",
        "            continue\n",
        "        \n",
        "        # Métricas principales\n",
        "        print(f\"  🎯 Métricas Principales:\")\n",
        "        for metric in ['recall_at_5', 'precision_at_5', 'ndcg_at_5', 'mrr']:\n",
        "            if metric in aggregated:\n",
        "                mean_val = aggregated[metric]['mean']\n",
        "                std_val = aggregated[metric]['std']\n",
        "                print(f\"    {metric.replace('_', ' ').title()}: {mean_val:.3f} ± {std_val:.3f}\")\n",
        "        \n",
        "        # Análisis de consultas\n",
        "        if query_results:\n",
        "            print(f\"  📝 Análisis de Consultas:\")\n",
        "            \n",
        "            # Mejores consultas (mayor recall@5)\n",
        "            best_queries = sorted(query_results, key=lambda x: x.get('recall_at_5', 0), reverse=True)[:3]\n",
        "            print(f\"    Mejores consultas (Recall@5):\")\n",
        "            for i, query in enumerate(best_queries, 1):\n",
        "                print(f\"      {i}. {query['query'][:60]}... (Recall@5: {query['recall_at_5']:.3f})\")\n",
        "            \n",
        "            # Peores consultas (menor recall@5)\n",
        "            worst_queries = sorted(query_results, key=lambda x: x.get('recall_at_5', 0))[:3]\n",
        "            print(f\"    Peores consultas (Recall@5):\")\n",
        "            for i, query in enumerate(worst_queries, 1):\n",
        "                print(f\"      {i}. {query['query'][:60]}... (Recall@5: {query['recall_at_5']:.3f})\")\n",
        "\n",
        "# Ejecutar visualizaciones y análisis\n",
        "visualize_retrieval_results(retrieval_results)\n",
        "analyze_retrieval_performance(retrieval_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar resultados de evaluación de retrievers\n",
        "def save_retrieval_evaluation_results(retrieval_results: Dict[str, Any], \n",
        "                                    evaluation_name: str = \"retrieval_evaluation\") -> str:\n",
        "    \"\"\"\n",
        "    Guarda los resultados de evaluación de retrievers\n",
        "    \n",
        "    Args:\n",
        "        retrieval_results: Resultados de la evaluación\n",
        "        evaluation_name: Nombre de la evaluación\n",
        "    \n",
        "    Returns:\n",
        "        Ruta del archivo guardado\n",
        "    \"\"\"\n",
        "    # Preparar configuración\n",
        "    config = {\n",
        "        'embedding_models': list(EMBEDDING_MODELS.keys()),\n",
        "        'retrieval_metrics': RETRIEVAL_METRICS,\n",
        "        'qdrant_config': QDRANT_CONFIG,\n",
        "        'dataset_size': len(dataset)\n",
        "    }\n",
        "    \n",
        "    # Preparar metadatos\n",
        "    metadata = {\n",
        "        'evaluation_type': 'retrieval',\n",
        "        'description': 'Evaluación de modelos de embedding para retrieval en Qdrant',\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'total_queries': len(dataset),\n",
        "        'total_models': len(retrieval_results)\n",
        "    }\n",
        "    \n",
        "    # Guardar evaluación\n",
        "    filepath = eval_manager.save_evaluation(\n",
        "        evaluation_name=evaluation_name,\n",
        "        config=config,\n",
        "        results={'retrieval_metrics': retrieval_results},\n",
        "        metadata=metadata\n",
        "    )\n",
        "    \n",
        "    print(f\"💾 Resultados de evaluación de retrievers guardados en: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "# Guardar resultados\n",
        "retrieval_filepath = save_retrieval_evaluation_results(retrieval_results)\n",
        "\n",
        "# Resumen final de la evaluación de retrievers\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"📋 RESUMEN DE EVALUACIÓN DE RETRIEVERS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if retrieval_results:\n",
        "    # Encontrar el mejor modelo general\n",
        "    best_model = None\n",
        "    best_overall_score = -1\n",
        "    \n",
        "    for model_name, results in retrieval_results.items():\n",
        "        aggregated = results['aggregated_metrics']\n",
        "        if aggregated:\n",
        "            # Calcular score promedio de métricas principales\n",
        "            main_metrics = ['recall_at_5', 'precision_at_5', 'ndcg_at_5', 'mrr']\n",
        "            scores = [aggregated[metric]['mean'] for metric in main_metrics if metric in aggregated]\n",
        "            if scores:\n",
        "                avg_score = np.mean(scores)\n",
        "                if avg_score > best_overall_score:\n",
        "                    best_overall_score = avg_score\n",
        "                    best_model = model_name\n",
        "    \n",
        "    if best_model:\n",
        "        print(f\"🏆 Mejor modelo general: {best_model}\")\n",
        "        print(f\"   Score promedio: {best_overall_score:.3f}\")\n",
        "    \n",
        "    # Estadísticas por modelo\n",
        "    print(f\"\\n📊 Estadísticas por modelo:\")\n",
        "    for model_name, results in retrieval_results.items():\n",
        "        aggregated = results['aggregated_metrics']\n",
        "        if aggregated and 'recall_at_5' in aggregated:\n",
        "            recall = aggregated['recall_at_5']['mean']\n",
        "            precision = aggregated['precision_at_5']['mean']\n",
        "            ndcg = aggregated['ndcg_at_5']['mean']\n",
        "            mrr = aggregated['mrr']['mean']\n",
        "            \n",
        "            print(f\"  {model_name}:\")\n",
        "            print(f\"    - Recall@5: {recall:.3f}\")\n",
        "            print(f\"    - Precision@5: {precision:.3f}\")\n",
        "            print(f\"    - nDCG@5: {ndcg:.3f}\")\n",
        "            print(f\"    - MRR: {mrr:.3f}\")\n",
        "    \n",
        "    print(f\"\\n✅ Evaluación completada exitosamente\")\n",
        "    print(f\"📁 Resultados guardados en: {retrieval_filepath}\")\n",
        "else:\n",
        "    print(\"❌ No se pudieron obtener resultados de evaluación\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Evaluación con Re-ranking\n",
        "\n",
        "## Objetivo de la Evaluación\n",
        "\n",
        "Esta sección evalúa cómo los modelos de re-ranking (cross-encoders) mejoran la calidad de los documentos recuperados. Los cross-encoders consideran tanto la consulta como el documento de forma conjunta, lo que les permite hacer predicciones más precisas sobre la relevancia.\n",
        "\n",
        "### 🎯 Proceso de Re-ranking\n",
        "\n",
        "1. **Retrieval inicial**: Obtener documentos usando embeddings (bi-encoder)\n",
        "2. **Re-ranking**: Reordenar documentos usando cross-encoder\n",
        "3. **Evaluación**: Comparar métricas antes y después del re-ranking\n",
        "4. **Análisis**: Identificar mejoras en relevancia y ranking\n",
        "\n",
        "### 📊 Métricas de Evaluación\n",
        "\n",
        "- **Mejora en Recall@k**: Incremento en recuperación de documentos relevantes\n",
        "- **Mejora en nDCG@k**: Mejora en la calidad del ranking\n",
        "- **Mejora en MRR**: Mejora en la posición del primer documento relevante\n",
        "- **Análisis de ranking**: Cambios en el orden de documentos\n",
        "- **Eficiencia**: Tiempo adicional vs. mejora en calidad\n",
        "\n",
        "### 🔄 Modelos de Re-ranking Evaluados\n",
        "\n",
        "- **ms-marco-MiniLM-L-6-v2**: Modelo ligero y rápido\n",
        "- **ms-marco-MiniLM-L-12-v2**: Modelo más robusto\n",
        "- **ms-marco-MiniLM-L-2-v2**: Modelo ultra-ligero\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clase para evaluación de re-ranking\n",
        "class RerankingEvaluator:\n",
        "    \"\"\"\n",
        "    Evalúa la mejora en calidad de documentos mediante re-ranking\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, qdrant_client, embedding_models, reranking_models, collection_name, top_k=20):\n",
        "        self.qdrant_client = qdrant_client\n",
        "        self.embedding_models = embedding_models\n",
        "        self.reranking_models = reranking_models\n",
        "        self.collection_name = collection_name\n",
        "        self.top_k = top_k\n",
        "        self.results = {}\n",
        "    \n",
        "    def retrieve_documents(self, query: str, embedding_model_name: str, top_k: int = None) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Recupera documentos usando embeddings (sin re-ranking)\n",
        "        \n",
        "        Args:\n",
        "            query: Pregunta a buscar\n",
        "            embedding_model_name: Nombre del modelo de embedding\n",
        "            top_k: Número de documentos a recuperar\n",
        "        \n",
        "        Returns:\n",
        "            Lista de documentos recuperados\n",
        "        \"\"\"\n",
        "        if top_k is None:\n",
        "            top_k = self.top_k\n",
        "        \n",
        "        try:\n",
        "            # Obtener modelo de embedding\n",
        "            model_info = self.embedding_models[embedding_model_name]\n",
        "            model = model_info['model']\n",
        "            \n",
        "            # Generar embedding de la consulta\n",
        "            query_embedding = model.encode([query])[0].tolist()\n",
        "            \n",
        "            # Buscar en Qdrant\n",
        "            search_results = self.qdrant_client.search(\n",
        "                collection_name=self.collection_name,\n",
        "                query_vector=query_embedding,\n",
        "                limit=top_k,\n",
        "                with_payload=True,\n",
        "                with_vectors=False\n",
        "            )\n",
        "            \n",
        "            # Formatear resultados\n",
        "            documents = []\n",
        "            for result in search_results:\n",
        "                documents.append({\n",
        "                    'id': result.id,\n",
        "                    'score': result.score,\n",
        "                    'payload': result.payload\n",
        "                })\n",
        "            \n",
        "            return documents\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error en retrieval con {embedding_model_name}: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def rerank_documents(self, query: str, documents: List[Dict[str, Any]], \n",
        "                        reranking_model_name: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Re-ordena documentos usando un modelo de re-ranking\n",
        "        \n",
        "        Args:\n",
        "            query: Pregunta original\n",
        "            documents: Lista de documentos a re-ordenar\n",
        "            reranking_model_name: Nombre del modelo de re-ranking\n",
        "        \n",
        "        Returns:\n",
        "            Lista de documentos re-ordenados con nuevos scores\n",
        "        \"\"\"\n",
        "        if not documents:\n",
        "            return []\n",
        "        \n",
        "        try:\n",
        "            # Obtener modelo de re-ranking\n",
        "            model_info = self.reranking_models[reranking_model_name]\n",
        "            model = model_info['model']\n",
        "            \n",
        "            # Preparar pares (query, documento) para el cross-encoder\n",
        "            query_doc_pairs = []\n",
        "            for doc in documents:\n",
        "                # Extraer texto del documento para re-ranking\n",
        "                doc_text = self._extract_document_text(doc['payload'])\n",
        "                query_doc_pairs.append((query, doc_text))\n",
        "            \n",
        "            # Calcular scores de relevancia\n",
        "            relevance_scores = model.predict(query_doc_pairs)\n",
        "            \n",
        "            # Crear documentos con nuevos scores\n",
        "            reranked_docs = []\n",
        "            for i, doc in enumerate(documents):\n",
        "                reranked_doc = doc.copy()\n",
        "                reranked_doc['rerank_score'] = float(relevance_scores[i])\n",
        "                reranked_doc['original_score'] = doc['score']\n",
        "                reranked_docs.append(reranked_doc)\n",
        "            \n",
        "            # Ordenar por score de re-ranking\n",
        "            reranked_docs.sort(key=lambda x: x['rerank_score'], reverse=True)\n",
        "            \n",
        "            return reranked_docs\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error en re-ranking con {reranking_model_name}: {e}\")\n",
        "            return documents  # Retornar documentos originales si hay error\n",
        "    \n",
        "    def _extract_document_text(self, payload: Dict[str, Any]) -> str:\n",
        "        \"\"\"\n",
        "        Extrae el texto relevante del payload del documento para re-ranking\n",
        "        \n",
        "        Args:\n",
        "            payload: Payload del documento de Qdrant\n",
        "        \n",
        "        Returns:\n",
        "            Texto del documento para re-ranking\n",
        "        \"\"\"\n",
        "        # Combinar campos relevantes para re-ranking\n",
        "        text_parts = []\n",
        "        \n",
        "        # Agregar capítulo y artículo (mismo formato que para embedding)\n",
        "        if 'capitulo_descripcion' in payload and 'articulo' in payload:\n",
        "            text_parts.append(f\"{payload['capitulo_descripcion']}: {payload['articulo']}\")\n",
        "        \n",
        "        # Agregar otros campos relevantes si existen\n",
        "        for field in ['titulo', 'libro', 'capitulo']:\n",
        "            if field in payload and payload[field]:\n",
        "                text_parts.append(str(payload[field]))\n",
        "        \n",
        "        return \" | \".join(text_parts) if text_parts else str(payload.get('articulo', ''))\n",
        "    \n",
        "    def evaluate_reranking_improvement(self, query: str, expected_articles: List[str],\n",
        "                                     embedding_model_name: str, reranking_model_name: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Evalúa la mejora del re-ranking para una consulta específica\n",
        "        \n",
        "        Args:\n",
        "            query: Pregunta a evaluar\n",
        "            expected_articles: Lista de artículos esperados\n",
        "            embedding_model_name: Nombre del modelo de embedding\n",
        "            reranking_model_name: Nombre del modelo de re-ranking\n",
        "        \n",
        "        Returns:\n",
        "            Diccionario con métricas de mejora\n",
        "        \"\"\"\n",
        "        # 1. Retrieval inicial (sin re-ranking)\n",
        "        initial_docs = self.retrieve_documents(query, embedding_model_name)\n",
        "        initial_ids = [doc['id'] for doc in initial_docs]\n",
        "        \n",
        "        # 2. Re-ranking\n",
        "        reranked_docs = self.rerank_documents(query, initial_docs, reranking_model_name)\n",
        "        reranked_ids = [doc['id'] for doc in reranked_docs]\n",
        "        \n",
        "        # 3. Calcular métricas antes y después\n",
        "        metrics = {}\n",
        "        \n",
        "        # Métricas iniciales (sin re-ranking)\n",
        "        for k in RETRIEVAL_METRICS['recall_at_k']:\n",
        "            metrics[f'initial_recall_at_{k}'] = self._calculate_recall_at_k(initial_ids, expected_articles, k)\n",
        "        \n",
        "        for k in RETRIEVAL_METRICS['precision_at_k']:\n",
        "            metrics[f'initial_precision_at_{k}'] = self._calculate_precision_at_k(initial_ids, expected_articles, k)\n",
        "        \n",
        "        for k in RETRIEVAL_METRICS['ndcg_at_k']:\n",
        "            metrics[f'initial_ndcg_at_{k}'] = self._calculate_ndcg_at_k(initial_ids, expected_articles, k)\n",
        "        \n",
        "        if RETRIEVAL_METRICS['mrr']:\n",
        "            metrics['initial_mrr'] = self._calculate_mrr(initial_ids, expected_articles)\n",
        "        \n",
        "        # Métricas después del re-ranking\n",
        "        for k in RETRIEVAL_METRICS['recall_at_k']:\n",
        "            metrics[f'reranked_recall_at_{k}'] = self._calculate_recall_at_k(reranked_ids, expected_articles, k)\n",
        "        \n",
        "        for k in RETRIEVAL_METRICS['precision_at_k']:\n",
        "            metrics[f'reranked_precision_at_{k}'] = self._calculate_precision_at_k(reranked_ids, expected_articles, k)\n",
        "        \n",
        "        for k in RETRIEVAL_METRICS['ndcg_at_k']:\n",
        "            metrics[f'reranked_ndcg_at_{k}'] = self._calculate_ndcg_at_k(reranked_ids, expected_articles, k)\n",
        "        \n",
        "        if RETRIEVAL_METRICS['mrr']:\n",
        "            metrics['reranked_mrr'] = self._calculate_mrr(reranked_ids, expected_articles)\n",
        "        \n",
        "        # Calcular mejoras\n",
        "        for k in RETRIEVAL_METRICS['recall_at_k']:\n",
        "            initial_key = f'initial_recall_at_{k}'\n",
        "            reranked_key = f'reranked_recall_at_{k}'\n",
        "            metrics[f'recall_improvement_at_{k}'] = metrics[reranked_key] - metrics[initial_key]\n",
        "        \n",
        "        for k in RETRIEVAL_METRICS['precision_at_k']:\n",
        "            initial_key = f'initial_precision_at_{k}'\n",
        "            reranked_key = f'reranked_precision_at_{k}'\n",
        "            metrics[f'precision_improvement_at_{k}'] = metrics[reranked_key] - metrics[initial_key]\n",
        "        \n",
        "        for k in RETRIEVAL_METRICS['ndcg_at_k']:\n",
        "            initial_key = f'initial_ndcg_at_{k}'\n",
        "            reranked_key = f'reranked_ndcg_at_{k}'\n",
        "            metrics[f'ndcg_improvement_at_{k}'] = metrics[reranked_key] - metrics[initial_key]\n",
        "        \n",
        "        if RETRIEVAL_METRICS['mrr']:\n",
        "            metrics['mrr_improvement'] = metrics['reranked_mrr'] - metrics['initial_mrr']\n",
        "        \n",
        "        # Información adicional\n",
        "        metrics['total_documents'] = len(initial_docs)\n",
        "        metrics['expected_articles'] = expected_articles\n",
        "        metrics['initial_ranking'] = initial_ids[:10]\n",
        "        metrics['reranked_ranking'] = reranked_ids[:10]\n",
        "        \n",
        "        return metrics\n",
        "    \n",
        "    def _calculate_recall_at_k(self, retrieved_ids: List[str], expected_ids: List[str], k: int) -> float:\n",
        "        \"\"\"Calcula Recall@k\"\"\"\n",
        "        if not expected_ids:\n",
        "            return 0.0\n",
        "        retrieved_k = set(retrieved_ids[:k])\n",
        "        expected_set = set(expected_ids)\n",
        "        intersection = retrieved_k.intersection(expected_set)\n",
        "        return len(intersection) / len(expected_set)\n",
        "    \n",
        "    def _calculate_precision_at_k(self, retrieved_ids: List[str], expected_ids: List[str], k: int) -> float:\n",
        "        \"\"\"Calcula Precision@k\"\"\"\n",
        "        if k == 0:\n",
        "            return 0.0\n",
        "        retrieved_k = set(retrieved_ids[:k])\n",
        "        expected_set = set(expected_ids)\n",
        "        intersection = retrieved_k.intersection(expected_set)\n",
        "        return len(intersection) / k\n",
        "    \n",
        "    def _calculate_ndcg_at_k(self, retrieved_ids: List[str], expected_ids: List[str], k: int) -> float:\n",
        "        \"\"\"Calcula nDCG@k\"\"\"\n",
        "        if not expected_ids or k == 0:\n",
        "            return 0.0\n",
        "        relevance = [1 if doc_id in expected_ids else 0 for doc_id in retrieved_ids[:k]]\n",
        "        dcg = 0.0\n",
        "        for i, rel in enumerate(relevance):\n",
        "            dcg += rel / np.log2(i + 2)\n",
        "        ideal_relevance = [1] * min(len(expected_ids), k)\n",
        "        idcg = 0.0\n",
        "        for i, rel in enumerate(ideal_relevance):\n",
        "            idcg += rel / np.log2(i + 2)\n",
        "        return dcg / idcg if idcg > 0 else 0.0\n",
        "    \n",
        "    def _calculate_mrr(self, retrieved_ids: List[str], expected_ids: List[str]) -> float:\n",
        "        \"\"\"Calcula MRR\"\"\"\n",
        "        if not expected_ids:\n",
        "            return 0.0\n",
        "        expected_set = set(expected_ids)\n",
        "        for i, doc_id in enumerate(retrieved_ids):\n",
        "            if doc_id in expected_set:\n",
        "                return 1.0 / (i + 1)\n",
        "        return 0.0\n",
        "\n",
        "# Inicializar evaluador de re-ranking\n",
        "reranking_evaluator = RerankingEvaluator(\n",
        "    qdrant_client=env_manager.qdrant_client,\n",
        "    embedding_models=env_manager.embedding_models,\n",
        "    reranking_models=env_manager.reranking_models,\n",
        "    collection_name=QDRANT_CONFIG['collection_name'],\n",
        "    top_k=QDRANT_CONFIG['top_k']\n",
        ")\n",
        "\n",
        "print(\"✅ Evaluador de re-ranking inicializado\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función para evaluación completa de re-ranking\n",
        "def evaluate_all_reranking_combinations(dataset: pd.DataFrame, \n",
        "                                      embedding_models: Dict[str, Any],\n",
        "                                      reranking_models: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Evalúa todas las combinaciones de modelos de embedding y re-ranking\n",
        "    \n",
        "    Args:\n",
        "        dataset: DataFrame con preguntas y artículos esperados\n",
        "        embedding_models: Diccionario con modelos de embedding\n",
        "        reranking_models: Diccionario con modelos de re-ranking\n",
        "    \n",
        "    Returns:\n",
        "        Diccionario con resultados de evaluación\n",
        "    \"\"\"\n",
        "    print(\"🚀 Iniciando evaluación completa de re-ranking...\")\n",
        "    print(f\"📊 Evaluando {len(dataset)} preguntas\")\n",
        "    print(f\"🔄 Combinaciones: {len(embedding_models)} embeddings × {len(reranking_models)} re-rankers\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    all_results = {}\n",
        "    \n",
        "    # Evaluar cada combinación de embedding + re-ranking\n",
        "    for embedding_name in embedding_models.keys():\n",
        "        print(f\"\\n🧠 Embedding: {embedding_name}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        embedding_results = {}\n",
        "        \n",
        "        for reranking_name in reranking_models.keys():\n",
        "            print(f\"\\n  🔄 Re-ranking: {reranking_name}\")\n",
        "            print(\"  \" + \"-\" * 30)\n",
        "            \n",
        "            combination_key = f\"{embedding_name}+{reranking_name}\"\n",
        "            combination_results = {\n",
        "                'embedding_model': embedding_name,\n",
        "                'reranking_model': reranking_name,\n",
        "                'query_results': [],\n",
        "                'aggregated_metrics': {}\n",
        "            }\n",
        "            \n",
        "            # Evaluar cada pregunta\n",
        "            for idx, row in dataset.iterrows():\n",
        "                query = row['question']\n",
        "                expected_articles = row['expected_articles']\n",
        "                \n",
        "                print(f\"    📝 Pregunta {idx + 1}/{len(dataset)}: {query[:40]}...\")\n",
        "                \n",
        "                try:\n",
        "                    # Evaluar mejora del re-ranking\n",
        "                    query_metrics = reranking_evaluator.evaluate_reranking_improvement(\n",
        "                        query=query,\n",
        "                        expected_articles=expected_articles,\n",
        "                        embedding_model_name=embedding_name,\n",
        "                        reranking_model_name=reranking_name\n",
        "                    )\n",
        "                    \n",
        "                    # Agregar información de la consulta\n",
        "                    query_metrics['query'] = query\n",
        "                    query_metrics['expected_articles'] = expected_articles\n",
        "                    query_metrics['query_id'] = idx\n",
        "                    \n",
        "                    combination_results['query_results'].append(query_metrics)\n",
        "                    \n",
        "                    # Mostrar métricas principales de mejora\n",
        "                    recall_improvement = query_metrics.get('recall_improvement_at_5', 0)\n",
        "                    ndcg_improvement = query_metrics.get('ndcg_improvement_at_5', 0)\n",
        "                    mrr_improvement = query_metrics.get('mrr_improvement', 0)\n",
        "                    \n",
        "                    print(f\"      📈 Mejora Recall@5: {recall_improvement:+.3f}, \"\n",
        "                          f\"nDCG@5: {ndcg_improvement:+.3f}, MRR: {mrr_improvement:+.3f}\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"      ❌ Error en pregunta {idx + 1}: {e}\")\n",
        "                    continue\n",
        "            \n",
        "            # Calcular métricas agregadas\n",
        "            if combination_results['query_results']:\n",
        "                print(f\"\\n    📊 Calculando métricas agregadas...\")\n",
        "                aggregated = calculate_reranking_aggregated_metrics(combination_results['query_results'])\n",
        "                combination_results['aggregated_metrics'] = aggregated\n",
        "                \n",
        "                # Mostrar resumen de mejoras\n",
        "                print(f\"    🎯 Resumen de mejoras:\")\n",
        "                print(f\"      - Recall@5: {aggregated.get('recall_improvement_at_5', {}).get('mean', 0):+.3f}\")\n",
        "                print(f\"      - Precision@5: {aggregated.get('precision_improvement_at_5', {}).get('mean', 0):+.3f}\")\n",
        "                print(f\"      - nDCG@5: {aggregated.get('ndcg_improvement_at_5', {}).get('mean', 0):+.3f}\")\n",
        "                print(f\"      - MRR: {aggregated.get('mrr_improvement', {}).get('mean', 0):+.3f}\")\n",
        "            \n",
        "            embedding_results[reranking_name] = combination_results\n",
        "        \n",
        "        all_results[embedding_name] = embedding_results\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"✅ Evaluación de re-ranking completada\")\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "def calculate_reranking_aggregated_metrics(query_results: List[Dict[str, Any]]) -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Calcula métricas agregadas para evaluación de re-ranking\n",
        "    \n",
        "    Args:\n",
        "        query_results: Lista de resultados de consultas individuales\n",
        "    \n",
        "    Returns:\n",
        "        Diccionario con métricas agregadas\n",
        "    \"\"\"\n",
        "    if not query_results:\n",
        "        return {}\n",
        "    \n",
        "    # Obtener todas las métricas disponibles\n",
        "    metric_names = [key for key in query_results[0].keys() \n",
        "                   if key not in ['query', 'expected_articles', 'query_id', 'initial_ranking', \n",
        "                                 'reranked_ranking', 'total_documents']]\n",
        "    \n",
        "    aggregated = {}\n",
        "    \n",
        "    for metric_name in metric_names:\n",
        "        values = [result[metric_name] for result in query_results if metric_name in result]\n",
        "        \n",
        "        if values:\n",
        "            aggregated[metric_name] = {\n",
        "                'mean': np.mean(values),\n",
        "                'std': np.std(values),\n",
        "                'min': np.min(values),\n",
        "                'max': np.max(values),\n",
        "                'median': np.median(values)\n",
        "            }\n",
        "    \n",
        "    return aggregated\n",
        "\n",
        "# Ejecutar evaluación de re-ranking\n",
        "print(\"🔄 Iniciando evaluación de re-ranking...\")\n",
        "reranking_results = evaluate_all_reranking_combinations(\n",
        "    dataset, \n",
        "    env_manager.embedding_models, \n",
        "    env_manager.reranking_models\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualización de resultados de re-ranking\n",
        "def visualize_reranking_results(reranking_results: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Visualiza los resultados de evaluación de re-ranking\n",
        "    \"\"\"\n",
        "    if not reranking_results:\n",
        "        print(\"❌ No hay resultados para visualizar\")\n",
        "        return\n",
        "    \n",
        "    # Preparar datos para visualización\n",
        "    combinations = []\n",
        "    improvement_data = []\n",
        "    \n",
        "    for embedding_name, embedding_data in reranking_results.items():\n",
        "        for reranking_name, combination_data in embedding_data.items():\n",
        "            if combination_data['aggregated_metrics']:\n",
        "                combination_key = f\"{embedding_name}\\n+ {reranking_name}\"\n",
        "                combinations.append(combination_key)\n",
        "                \n",
        "                # Extraer mejoras principales\n",
        "                metrics = combination_data['aggregated_metrics']\n",
        "                improvement_data.append({\n",
        "                    'combination': combination_key,\n",
        "                    'recall_improvement': metrics.get('recall_improvement_at_5', {}).get('mean', 0),\n",
        "                    'precision_improvement': metrics.get('precision_improvement_at_5', {}).get('mean', 0),\n",
        "                    'ndcg_improvement': metrics.get('ndcg_improvement_at_5', {}).get('mean', 0),\n",
        "                    'mrr_improvement': metrics.get('mrr_improvement', {}).get('mean', 0)\n",
        "                })\n",
        "    \n",
        "    if not improvement_data:\n",
        "        print(\"❌ No hay datos de mejora para visualizar\")\n",
        "        return\n",
        "    \n",
        "    # Crear figura con subplots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('Mejoras por Re-ranking - Comparación de Combinaciones', \n",
        "                 fontsize=16, fontweight='bold')\n",
        "    \n",
        "    metrics_to_plot = ['recall_improvement', 'precision_improvement', 'ndcg_improvement', 'mrr_improvement']\n",
        "    metric_titles = ['Recall@5', 'Precision@5', 'nDCG@5', 'MRR']\n",
        "    \n",
        "    # Colores para cada combinación\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(combinations)))\n",
        "    \n",
        "    for idx, (metric, title) in enumerate(zip(metrics_to_plot, metric_titles)):\n",
        "        row = idx // 2\n",
        "        col = idx % 2\n",
        "        \n",
        "        # Extraer datos para la métrica\n",
        "        values = [data[metric] for data in improvement_data]\n",
        "        combination_labels = [data['combination'] for data in improvement_data]\n",
        "        \n",
        "        # Crear gráfico de barras\n",
        "        bars = axes[row, col].bar(range(len(values)), values, color=colors, alpha=0.7)\n",
        "        axes[row, col].set_title(f'Mejora en {title}')\n",
        "        axes[row, col].set_ylabel('Mejora (Δ)')\n",
        "        axes[row, col].set_xticks(range(len(combination_labels)))\n",
        "        axes[row, col].set_xticklabels(combination_labels, rotation=45, ha='right')\n",
        "        \n",
        "        # Línea de referencia en 0\n",
        "        axes[row, col].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "        \n",
        "        # Agregar valores en las barras\n",
        "        for bar, value in zip(bars, values):\n",
        "            height = bar.get_height()\n",
        "            axes[row, col].text(bar.get_x() + bar.get_width()/2., height + (0.001 if height >= 0 else -0.003),\n",
        "                              f'{value:+.3f}', ha='center', va='bottom' if height >= 0 else 'top', fontsize=8)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Crear tabla comparativa de mejoras\n",
        "    print(\"\\n📊 Tabla Comparativa de Mejoras por Re-ranking:\")\n",
        "    print(\"=\" * 100)\n",
        "    \n",
        "    comparison_df = pd.DataFrame(improvement_data)\n",
        "    comparison_df = comparison_df.round(4)\n",
        "    print(comparison_df.to_string(index=False))\n",
        "    \n",
        "    # Identificar mejores combinaciones\n",
        "    print(\"\\n🏆 Mejores Combinaciones por Métrica:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for metric, title in zip(metrics_to_plot, metric_titles):\n",
        "        best_idx = comparison_df[metric].idxmax()\n",
        "        best_combination = comparison_df.loc[best_idx, 'combination']\n",
        "        best_value = comparison_df.loc[best_idx, metric]\n",
        "        print(f\"  {title}: {best_combination} ({best_value:+.3f})\")\n",
        "\n",
        "def analyze_reranking_performance(reranking_results: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Analiza el rendimiento del re-ranking en detalle\n",
        "    \"\"\"\n",
        "    print(\"\\n🔍 Análisis Detallado de Re-ranking:\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for embedding_name, embedding_data in reranking_results.items():\n",
        "        print(f\"\\n🧠 Embedding: {embedding_name}\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        for reranking_name, combination_data in embedding_data.items():\n",
        "            print(f\"\\n  🔄 Re-ranking: {reranking_name}\")\n",
        "            print(\"  \" + \"-\" * 30)\n",
        "            \n",
        "            aggregated = combination_data['aggregated_metrics']\n",
        "            query_results = combination_data['query_results']\n",
        "            \n",
        "            if not aggregated:\n",
        "                print(\"    ❌ No hay métricas disponibles\")\n",
        "                continue\n",
        "            \n",
        "            # Métricas de mejora principales\n",
        "            print(f\"  📈 Mejoras Promedio:\")\n",
        "            for metric in ['recall_improvement_at_5', 'precision_improvement_at_5', \n",
        "                          'ndcg_improvement_at_5', 'mrr_improvement']:\n",
        "                if metric in aggregated:\n",
        "                    mean_val = aggregated[metric]['mean']\n",
        "                    std_val = aggregated[metric]['std']\n",
        "                    print(f\"    {metric.replace('_improvement', '').replace('_at_5', '@5').title()}: {mean_val:+.3f} ± {std_val:.3f}\")\n",
        "            \n",
        "            # Análisis de consultas con mayor mejora\n",
        "            if query_results:\n",
        "                print(f\"  📝 Consultas con Mayor Mejora (nDCG@5):\")\n",
        "                best_queries = sorted(query_results, \n",
        "                                    key=lambda x: x.get('ndcg_improvement_at_5', 0), \n",
        "                                    reverse=True)[:3]\n",
        "                for i, query in enumerate(best_queries, 1):\n",
        "                    improvement = query.get('ndcg_improvement_at_5', 0)\n",
        "                    print(f\"    {i}. {query['query'][:50]}... (Mejora: {improvement:+.3f})\")\n",
        "                \n",
        "                # Análisis de consultas con peor rendimiento\n",
        "                print(f\"  📝 Consultas con Menor Mejora (nDCG@5):\")\n",
        "                worst_queries = sorted(query_results, \n",
        "                                     key=lambda x: x.get('ndcg_improvement_at_5', 0))[:3]\n",
        "                for i, query in enumerate(worst_queries, 1):\n",
        "                    improvement = query.get('ndcg_improvement_at_5', 0)\n",
        "                    print(f\"    {i}. {query['query'][:50]}... (Mejora: {improvement:+.3f})\")\n",
        "\n",
        "# Ejecutar visualizaciones y análisis\n",
        "visualize_reranking_results(reranking_results)\n",
        "analyze_reranking_performance(reranking_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar resultados de evaluación de re-ranking\n",
        "def save_reranking_evaluation_results(reranking_results: Dict[str, Any], \n",
        "                                    evaluation_name: str = \"reranking_evaluation\") -> str:\n",
        "    \"\"\"\n",
        "    Guarda los resultados de evaluación de re-ranking\n",
        "    \n",
        "    Args:\n",
        "        reranking_results: Resultados de la evaluación\n",
        "        evaluation_name: Nombre de la evaluación\n",
        "    \n",
        "    Returns:\n",
        "        Ruta del archivo guardado\n",
        "    \"\"\"\n",
        "    # Preparar configuración\n",
        "    config = {\n",
        "        'embedding_models': list(EMBEDDING_MODELS.keys()),\n",
        "        'reranking_models': list(RERANKING_MODELS.keys()),\n",
        "        'retrieval_metrics': RETRIEVAL_METRICS,\n",
        "        'qdrant_config': QDRANT_CONFIG,\n",
        "        'dataset_size': len(dataset)\n",
        "    }\n",
        "    \n",
        "    # Preparar metadatos\n",
        "    metadata = {\n",
        "        'evaluation_type': 'reranking',\n",
        "        'description': 'Evaluación de modelos de re-ranking para mejorar relevancia de documentos',\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'total_queries': len(dataset),\n",
        "        'total_combinations': sum(len(embedding_data) for embedding_data in reranking_results.values())\n",
        "    }\n",
        "    \n",
        "    # Guardar evaluación\n",
        "    filepath = eval_manager.save_evaluation(\n",
        "        evaluation_name=evaluation_name,\n",
        "        config=config,\n",
        "        results={'reranking_metrics': reranking_results},\n",
        "        metadata=metadata\n",
        "    )\n",
        "    \n",
        "    print(f\"💾 Resultados de evaluación de re-ranking guardados en: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "# Guardar resultados\n",
        "reranking_filepath = save_reranking_evaluation_results(reranking_results)\n",
        "\n",
        "# Resumen final de la evaluación de re-ranking\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"📋 RESUMEN DE EVALUACIÓN DE RE-RANKING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if reranking_results:\n",
        "    # Encontrar la mejor combinación general\n",
        "    best_combination = None\n",
        "    best_overall_improvement = -1\n",
        "    \n",
        "    for embedding_name, embedding_data in reranking_results.items():\n",
        "        for reranking_name, combination_data in embedding_data.items():\n",
        "            aggregated = combination_data['aggregated_metrics']\n",
        "            if aggregated:\n",
        "                # Calcular mejora promedio de métricas principales\n",
        "                main_improvements = ['recall_improvement_at_5', 'precision_improvement_at_5', \n",
        "                                   'ndcg_improvement_at_5', 'mrr_improvement']\n",
        "                improvements = [aggregated[metric]['mean'] for metric in main_improvements \n",
        "                              if metric in aggregated]\n",
        "                if improvements:\n",
        "                    avg_improvement = np.mean(improvements)\n",
        "                    if avg_improvement > best_overall_improvement:\n",
        "                        best_overall_improvement = avg_improvement\n",
        "                        best_combination = f\"{embedding_name} + {reranking_name}\"\n",
        "    \n",
        "    if best_combination:\n",
        "        print(f\"🏆 Mejor combinación general: {best_combination}\")\n",
        "        print(f\"   Mejora promedio: {best_overall_improvement:+.3f}\")\n",
        "    \n",
        "    # Estadísticas por combinación\n",
        "    print(f\"\\n📊 Estadísticas por combinación:\")\n",
        "    for embedding_name, embedding_data in reranking_results.items():\n",
        "        print(f\"\\n  🧠 Embedding: {embedding_name}\")\n",
        "        for reranking_name, combination_data in embedding_data.items():\n",
        "            aggregated = combination_data['aggregated_metrics']\n",
        "            if aggregated and 'ndcg_improvement_at_5' in aggregated:\n",
        "                recall_improvement = aggregated['recall_improvement_at_5']['mean']\n",
        "                precision_improvement = aggregated['precision_improvement_at_5']['mean']\n",
        "                ndcg_improvement = aggregated['ndcg_improvement_at_5']['mean']\n",
        "                mrr_improvement = aggregated['mrr_improvement']['mean']\n",
        "                \n",
        "                print(f\"    🔄 {reranking_name}:\")\n",
        "                print(f\"      - Recall@5: {recall_improvement:+.3f}\")\n",
        "                print(f\"      - Precision@5: {precision_improvement:+.3f}\")\n",
        "                print(f\"      - nDCG@5: {ndcg_improvement:+.3f}\")\n",
        "                print(f\"      - MRR: {mrr_improvement:+.3f}\")\n",
        "    \n",
        "    # Análisis de efectividad del re-ranking\n",
        "    print(f\"\\n📈 Análisis de Efectividad del Re-ranking:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    total_combinations = 0\n",
        "    positive_improvements = 0\n",
        "    \n",
        "    for embedding_data in reranking_results.values():\n",
        "        for combination_data in embedding_data.values():\n",
        "            aggregated = combination_data['aggregated_metrics']\n",
        "            if aggregated and 'ndcg_improvement_at_5' in aggregated:\n",
        "                total_combinations += 1\n",
        "                if aggregated['ndcg_improvement_at_5']['mean'] > 0:\n",
        "                    positive_improvements += 1\n",
        "    \n",
        "    if total_combinations > 0:\n",
        "        effectiveness_rate = (positive_improvements / total_combinations) * 100\n",
        "        print(f\"  - Combinaciones que mejoran nDCG@5: {positive_improvements}/{total_combinations} ({effectiveness_rate:.1f}%)\")\n",
        "        \n",
        "        if effectiveness_rate > 50:\n",
        "            print(\"  ✅ El re-ranking es efectivo en la mayoría de combinaciones\")\n",
        "        elif effectiveness_rate > 25:\n",
        "            print(\"  ⚠️  El re-ranking es moderadamente efectivo\")\n",
        "        else:\n",
        "            print(\"  ❌ El re-ranking tiene efectividad limitada\")\n",
        "    \n",
        "    print(f\"\\n✅ Evaluación de re-ranking completada exitosamente\")\n",
        "    print(f\"📁 Resultados guardados en: {reranking_filepath}\")\n",
        "else:\n",
        "    print(\"❌ No se pudieron obtener resultados de evaluación de re-ranking\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Evaluación del Flujo Completo con LLM\n",
        "\n",
        "## Objetivo de la Evaluación\n",
        "\n",
        "Esta sección evalúa el pipeline completo de RAG, incluyendo la generación de respuestas con LLM y la evaluación de calidad subjetiva usando LLM-as-a-judge. Se combinan métricas objetivas de retrieval con métricas subjetivas de calidad de respuesta.\n",
        "\n",
        "### 🎯 Proceso de Evaluación Completa\n",
        "\n",
        "1. **Retrieval**: Obtener documentos relevantes usando embeddings + Qdrant\n",
        "2. **Re-ranking**: Mejorar relevancia con cross-encoders (opcional)\n",
        "3. **Generación**: Crear respuesta usando LLM con contexto recuperado\n",
        "4. **Evaluación**: Medir calidad subjetiva con LLM-as-a-judge\n",
        "5. **Análisis**: Combinar métricas objetivas y subjetivas\n",
        "\n",
        "### 📊 Métricas de Evaluación\n",
        "\n",
        "#### Métricas Objetivas (Retrieval)\n",
        "- **Recall@k**: Proporción de documentos relevantes recuperados\n",
        "- **Precision@k**: Proporción de documentos relevantes entre los recuperados\n",
        "- **nDCG@k**: Calidad del ranking de documentos\n",
        "- **MRR**: Posición del primer documento relevante\n",
        "\n",
        "#### Métricas Subjetivas (LLM-as-a-judge)\n",
        "- **Coherencia**: ¿La respuesta es coherente y bien estructurada?\n",
        "- **Relevancia**: ¿La respuesta es relevante a la pregunta?\n",
        "- **Completitud**: ¿La respuesta abarca todos los aspectos necesarios?\n",
        "- **Fidelidad**: ¿La respuesta es fiel al contexto proporcionado?\n",
        "- **Concisión**: ¿La respuesta es concisa sin ser incompleta?\n",
        "\n",
        "### 🤖 Modelos de LLM Evaluados\n",
        "\n",
        "- **OpenAI GPT-3.5-turbo**: Modelo comercial robusto\n",
        "- **Modelo Local**: Fallback para entornos sin API externa\n",
        "- **Configuración Flexible**: Temperatura, max_tokens, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clase para evaluación completa del pipeline RAG\n",
        "class RAGPipelineEvaluator:\n",
        "    \"\"\"\n",
        "    Evalúa el pipeline completo de RAG incluyendo generación y evaluación con LLM\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, qdrant_client, embedding_models, reranking_models, \n",
        "                 llm_pipeline, collection_name, top_k=20):\n",
        "        self.qdrant_client = qdrant_client\n",
        "        self.embedding_models = embedding_models\n",
        "        self.reranking_models = reranking_models\n",
        "        self.llm_pipeline = llm_pipeline\n",
        "        self.collection_name = collection_name\n",
        "        self.top_k = top_k\n",
        "        self.results = {}\n",
        "    \n",
        "    def retrieve_documents(self, query: str, embedding_model_name: str, \n",
        "                          top_k: int = None) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Recupera documentos usando embeddings\n",
        "        \n",
        "        Args:\n",
        "            query: Pregunta a buscar\n",
        "            embedding_model_name: Nombre del modelo de embedding\n",
        "            top_k: Número de documentos a recuperar\n",
        "        \n",
        "        Returns:\n",
        "            Lista de documentos recuperados\n",
        "        \"\"\"\n",
        "        if top_k is None:\n",
        "            top_k = self.top_k\n",
        "        \n",
        "        try:\n",
        "            # Obtener modelo de embedding\n",
        "            model_info = self.embedding_models[embedding_model_name]\n",
        "            model = model_info['model']\n",
        "            \n",
        "            # Generar embedding de la consulta\n",
        "            query_embedding = model.encode([query])[0].tolist()\n",
        "            \n",
        "            # Buscar en Qdrant\n",
        "            search_results = self.qdrant_client.search(\n",
        "                collection_name=self.collection_name,\n",
        "                query_vector=query_embedding,\n",
        "                limit=top_k,\n",
        "                with_payload=True,\n",
        "                with_vectors=False\n",
        "            )\n",
        "            \n",
        "            # Formatear resultados\n",
        "            documents = []\n",
        "            for result in search_results:\n",
        "                documents.append({\n",
        "                    'id': result.id,\n",
        "                    'score': result.score,\n",
        "                    'payload': result.payload\n",
        "                })\n",
        "            \n",
        "            return documents\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error en retrieval con {embedding_model_name}: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def rerank_documents(self, query: str, documents: List[Dict[str, Any]], \n",
        "                        reranking_model_name: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Re-ordena documentos usando re-ranking\n",
        "        \n",
        "        Args:\n",
        "            query: Pregunta original\n",
        "            documents: Lista de documentos a re-ordenar\n",
        "            reranking_model_name: Nombre del modelo de re-ranking\n",
        "        \n",
        "        Returns:\n",
        "            Lista de documentos re-ordenados\n",
        "        \"\"\"\n",
        "        if not documents or not reranking_model_name:\n",
        "            return documents\n",
        "        \n",
        "        try:\n",
        "            # Obtener modelo de re-ranking\n",
        "            model_info = self.reranking_models[reranking_model_name]\n",
        "            model = model_info['model']\n",
        "            \n",
        "            # Preparar pares (query, documento) para el cross-encoder\n",
        "            query_doc_pairs = []\n",
        "            for doc in documents:\n",
        "                doc_text = self._extract_document_text(doc['payload'])\n",
        "                query_doc_pairs.append((query, doc_text))\n",
        "            \n",
        "            # Calcular scores de relevancia\n",
        "            relevance_scores = model.predict(query_doc_pairs)\n",
        "            \n",
        "            # Crear documentos con nuevos scores\n",
        "            reranked_docs = []\n",
        "            for i, doc in enumerate(documents):\n",
        "                reranked_doc = doc.copy()\n",
        "                reranked_doc['rerank_score'] = float(relevance_scores[i])\n",
        "                reranked_doc['original_score'] = doc['score']\n",
        "                reranked_docs.append(reranked_doc)\n",
        "            \n",
        "            # Ordenar por score de re-ranking\n",
        "            reranked_docs.sort(key=lambda x: x['rerank_score'], reverse=True)\n",
        "            \n",
        "            return reranked_docs\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error en re-ranking con {reranking_model_name}: {e}\")\n",
        "            return documents\n",
        "    \n",
        "    def _extract_document_text(self, payload: Dict[str, Any]) -> str:\n",
        "        \"\"\"\n",
        "        Extrae el texto relevante del payload del documento\n",
        "        \"\"\"\n",
        "        text_parts = []\n",
        "        \n",
        "        # Agregar capítulo y artículo\n",
        "        if 'capitulo_descripcion' in payload and 'articulo' in payload:\n",
        "            text_parts.append(f\"{payload['capitulo_descripcion']}: {payload['articulo']}\")\n",
        "        \n",
        "        # Agregar otros campos relevantes\n",
        "        for field in ['titulo', 'libro', 'capitulo']:\n",
        "            if field in payload and payload[field]:\n",
        "                text_parts.append(str(payload[field]))\n",
        "        \n",
        "        return \" | \".join(text_parts) if text_parts else str(payload.get('articulo', ''))\n",
        "    \n",
        "    def generate_response(self, query: str, documents: List[Dict[str, Any]], \n",
        "                         max_context_docs: int = 5) -> str:\n",
        "        \"\"\"\n",
        "        Genera respuesta usando LLM con contexto de documentos\n",
        "        \n",
        "        Args:\n",
        "            query: Pregunta original\n",
        "            documents: Lista de documentos recuperados\n",
        "            max_context_docs: Número máximo de documentos a usar como contexto\n",
        "        \n",
        "        Returns:\n",
        "            Respuesta generada por el LLM\n",
        "        \"\"\"\n",
        "        if not documents:\n",
        "            return \"No se encontraron documentos relevantes para responder la pregunta.\"\n",
        "        \n",
        "        try:\n",
        "            # Preparar contexto con documentos más relevantes\n",
        "            context_docs = documents[:max_context_docs]\n",
        "            context_text = self._build_context(context_docs)\n",
        "            \n",
        "            # Crear prompt para el LLM\n",
        "            prompt = self._create_prompt(query, context_text)\n",
        "            \n",
        "            # Generar respuesta\n",
        "            if self.llm_pipeline['provider'] == 'openai':\n",
        "                response = self._generate_openai_response(prompt)\n",
        "            else:\n",
        "                response = self._generate_local_response(prompt)\n",
        "            \n",
        "            return response\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error generando respuesta: {e}\")\n",
        "            return f\"Error generando respuesta: {str(e)}\"\n",
        "    \n",
        "    def _build_context(self, documents: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"\n",
        "        Construye el contexto a partir de los documentos\n",
        "        \"\"\"\n",
        "        context_parts = []\n",
        "        \n",
        "        for i, doc in enumerate(documents, 1):\n",
        "            payload = doc['payload']\n",
        "            \n",
        "            # Extraer información del documento\n",
        "            articulo_numero = payload.get('articulo_numero', 'N/A')\n",
        "            capitulo_descripcion = payload.get('capitulo_descripcion', '')\n",
        "            articulo_text = payload.get('articulo', '')\n",
        "            \n",
        "            # Formatear documento\n",
        "            doc_text = f\"Artículo {articulo_numero}: {capitulo_descripcion}\\n{articulo_text}\"\n",
        "            context_parts.append(f\"Documento {i}:\\n{doc_text}\\n\")\n",
        "        \n",
        "        return \"\\n\".join(context_parts)\n",
        "    \n",
        "    def _create_prompt(self, query: str, context: str) -> str:\n",
        "        \"\"\"\n",
        "        Crea el prompt para el LLM\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"Eres un asistente especializado en derecho laboral paraguayo. Responde la pregunta del usuario basándote únicamente en el contexto proporcionado.\n",
        "\n",
        "CONTEXTO:\n",
        "{context}\n",
        "\n",
        "PREGUNTA: {query}\n",
        "\n",
        "INSTRUCCIONES:\n",
        "- Responde de manera clara y precisa\n",
        "- Basa tu respuesta únicamente en el contexto proporcionado\n",
        "- Si el contexto no contiene información suficiente, indícalo claramente\n",
        "- Cita los artículos específicos cuando sea relevante\n",
        "- Mantén un tono profesional y técnico apropiado para el ámbito legal\n",
        "\n",
        "RESPUESTA:\"\"\"\n",
        "        \n",
        "        return prompt\n",
        "    \n",
        "    def _generate_openai_response(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Genera respuesta usando OpenAI\n",
        "        \"\"\"\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=self.llm_pipeline['model'],\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"Eres un asistente especializado en derecho laboral paraguayo.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=self.llm_pipeline['temperature'],\n",
        "                max_tokens=self.llm_pipeline['max_tokens']\n",
        "            )\n",
        "            \n",
        "            return response.choices[0].message.content.strip()\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error con OpenAI: {e}\")\n",
        "            return f\"Error con OpenAI: {str(e)}\"\n",
        "    \n",
        "    def _generate_local_response(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Genera respuesta usando modelo local\n",
        "        \"\"\"\n",
        "        try:\n",
        "            tokenizer = self.llm_pipeline['tokenizer']\n",
        "            model = self.llm_pipeline['model']\n",
        "            \n",
        "            # Tokenizar input\n",
        "            inputs = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "            \n",
        "            # Generar respuesta\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    inputs,\n",
        "                    max_length=inputs.shape[1] + self.llm_pipeline.get('max_tokens', 200),\n",
        "                    temperature=self.llm_pipeline.get('temperature', 0.7),\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "            \n",
        "            # Decodificar respuesta\n",
        "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            \n",
        "            # Extraer solo la parte de la respuesta (después del prompt)\n",
        "            if \"RESPUESTA:\" in response:\n",
        "                response = response.split(\"RESPUESTA:\")[-1].strip()\n",
        "            \n",
        "            return response\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error con modelo local: {e}\")\n",
        "            return f\"Error con modelo local: {str(e)}\"\n",
        "    \n",
        "    def evaluate_response_quality(self, query: str, generated_response: str, \n",
        "                                 expected_answer: str, context_documents: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Evalúa la calidad de la respuesta generada usando LLM-as-a-judge\n",
        "        \n",
        "        Args:\n",
        "            query: Pregunta original\n",
        "            generated_response: Respuesta generada por el LLM\n",
        "            expected_answer: Respuesta esperada (ground truth)\n",
        "            context_documents: Documentos usados como contexto\n",
        "        \n",
        "        Returns:\n",
        "            Diccionario con métricas de calidad\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Crear prompt para evaluación\n",
        "            evaluation_prompt = self._create_evaluation_prompt(\n",
        "                query, generated_response, expected_answer, context_documents\n",
        "            )\n",
        "            \n",
        "            # Generar evaluación\n",
        "            if self.llm_pipeline['provider'] == 'openai':\n",
        "                evaluation_response = self._generate_openai_response(evaluation_prompt)\n",
        "            else:\n",
        "                evaluation_response = self._generate_local_response(evaluation_prompt)\n",
        "            \n",
        "            # Parsear evaluación\n",
        "            quality_scores = self._parse_evaluation_response(evaluation_response)\n",
        "            \n",
        "            return quality_scores\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error evaluando calidad: {e}\")\n",
        "            return self._get_default_quality_scores()\n",
        "    \n",
        "    def _create_evaluation_prompt(self, query: str, generated_response: str, \n",
        "                                 expected_answer: str, context_documents: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"\n",
        "        Crea el prompt para evaluación con LLM-as-a-judge\n",
        "        \"\"\"\n",
        "        context_text = self._build_context(context_documents)\n",
        "        \n",
        "        prompt = f\"\"\"Eres un evaluador experto en sistemas de RAG. Evalúa la calidad de la respuesta generada según los criterios especificados.\n",
        "\n",
        "PREGUNTA: {query}\n",
        "\n",
        "RESPUESTA ESPERADA: {expected_answer}\n",
        "\n",
        "RESPUESTA GENERADA: {generated_response}\n",
        "\n",
        "CONTEXTO USADO: {context_text}\n",
        "\n",
        "CRITERIOS DE EVALUACIÓN:\n",
        "1. Coherencia (1-5): ¿La respuesta es coherente y bien estructurada?\n",
        "2. Relevancia (1-5): ¿La respuesta es relevante a la pregunta?\n",
        "3. Completitud (1-5): ¿La respuesta abarca todos los aspectos necesarios?\n",
        "4. Fidelidad (1-5): ¿La respuesta es fiel al contexto proporcionado?\n",
        "5. Concisión (1-5): ¿La respuesta es concisa sin ser incompleta?\n",
        "\n",
        "EVALÚA CADA CRITERIO Y RESPONDE EN EL SIGUIENTE FORMATO JSON:\n",
        "{{\n",
        "    \"coherence\": X,\n",
        "    \"relevance\": X,\n",
        "    \"completeness\": X,\n",
        "    \"fidelity\": X,\n",
        "    \"conciseness\": X,\n",
        "    \"overall_score\": X,\n",
        "    \"explanation\": \"Breve explicación de la evaluación\"\n",
        "}}\n",
        "\n",
        "Donde X es un número del 1 al 5 para cada criterio.\"\"\"\n",
        "        \n",
        "        return prompt\n",
        "    \n",
        "    def _parse_evaluation_response(self, evaluation_response: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Parsea la respuesta de evaluación del LLM\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Buscar JSON en la respuesta\n",
        "            import re\n",
        "            json_match = re.search(r'\\{.*\\}', evaluation_response, re.DOTALL)\n",
        "            \n",
        "            if json_match:\n",
        "                json_str = json_match.group()\n",
        "                evaluation_data = json.loads(json_str)\n",
        "                \n",
        "                # Validar que tenga las claves necesarias\n",
        "                required_keys = ['coherence', 'relevance', 'completeness', 'fidelity', 'conciseness']\n",
        "                if all(key in evaluation_data for key in required_keys):\n",
        "                    return evaluation_data\n",
        "            \n",
        "            # Si no se puede parsear, usar valores por defecto\n",
        "            return self._get_default_quality_scores()\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error parseando evaluación: {e}\")\n",
        "            return self._get_default_quality_scores()\n",
        "    \n",
        "    def _get_default_quality_scores(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Retorna scores por defecto cuando hay error en la evaluación\n",
        "        \"\"\"\n",
        "        return {\n",
        "            \"coherence\": 3.0,\n",
        "            \"relevance\": 3.0,\n",
        "            \"completeness\": 3.0,\n",
        "            \"fidelity\": 3.0,\n",
        "            \"conciseness\": 3.0,\n",
        "            \"overall_score\": 3.0,\n",
        "            \"explanation\": \"Error en evaluación automática\"\n",
        "        }\n",
        "    \n",
        "    def evaluate_complete_pipeline(self, query: str, expected_answer: str, \n",
        "                                  expected_articles: List[str], embedding_model_name: str,\n",
        "                                  reranking_model_name: str = None) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Evalúa el pipeline completo de RAG\n",
        "        \n",
        "        Args:\n",
        "            query: Pregunta a evaluar\n",
        "            expected_answer: Respuesta esperada\n",
        "            expected_articles: Lista de artículos esperados\n",
        "            embedding_model_name: Nombre del modelo de embedding\n",
        "            reranking_model_name: Nombre del modelo de re-ranking (opcional)\n",
        "        \n",
        "        Returns:\n",
        "            Diccionario con métricas completas\n",
        "        \"\"\"\n",
        "        results = {\n",
        "            'query': query,\n",
        "            'expected_answer': expected_answer,\n",
        "            'expected_articles': expected_articles,\n",
        "            'embedding_model': embedding_model_name,\n",
        "            'reranking_model': reranking_model_name\n",
        "        }\n",
        "        \n",
        "        # 1. Retrieval\n",
        "        documents = self.retrieve_documents(query, embedding_model_name)\n",
        "        results['retrieved_documents'] = len(documents)\n",
        "        results['retrieved_ids'] = [doc['id'] for doc in documents]\n",
        "        \n",
        "        # 2. Re-ranking (opcional)\n",
        "        if reranking_model_name and reranking_model_name in self.reranking_models:\n",
        "            documents = self.rerank_documents(query, documents, reranking_model_name)\n",
        "            results['reranked'] = True\n",
        "        else:\n",
        "            results['reranked'] = False\n",
        "        \n",
        "        # 3. Generación de respuesta\n",
        "        generated_response = self.generate_response(query, documents)\n",
        "        results['generated_response'] = generated_response\n",
        "        \n",
        "        # 4. Métricas objetivas de retrieval\n",
        "        retrieved_ids = [doc['id'] for doc in documents]\n",
        "        results['retrieval_metrics'] = self._calculate_retrieval_metrics(retrieved_ids, expected_articles)\n",
        "        \n",
        "        # 5. Evaluación de calidad de respuesta\n",
        "        quality_scores = self.evaluate_response_quality(\n",
        "            query, generated_response, expected_answer, documents\n",
        "        )\n",
        "        results['quality_scores'] = quality_scores\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def _calculate_retrieval_metrics(self, retrieved_ids: List[str], expected_ids: List[str]) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Calcula métricas objetivas de retrieval\n",
        "        \"\"\"\n",
        "        metrics = {}\n",
        "        \n",
        "        # Recall@k\n",
        "        for k in RETRIEVAL_METRICS['recall_at_k']:\n",
        "            metrics[f'recall_at_{k}'] = self._calculate_recall_at_k(retrieved_ids, expected_ids, k)\n",
        "        \n",
        "        # Precision@k\n",
        "        for k in RETRIEVAL_METRICS['precision_at_k']:\n",
        "            metrics[f'precision_at_{k}'] = self._calculate_precision_at_k(retrieved_ids, expected_ids, k)\n",
        "        \n",
        "        # nDCG@k\n",
        "        for k in RETRIEVAL_METRICS['ndcg_at_k']:\n",
        "            metrics[f'ndcg_at_{k}'] = self._calculate_ndcg_at_k(retrieved_ids, expected_ids, k)\n",
        "        \n",
        "        # MRR\n",
        "        if RETRIEVAL_METRICS['mrr']:\n",
        "            metrics['mrr'] = self._calculate_mrr(retrieved_ids, expected_ids)\n",
        "        \n",
        "        return metrics\n",
        "    \n",
        "    def _calculate_recall_at_k(self, retrieved_ids: List[str], expected_ids: List[str], k: int) -> float:\n",
        "        \"\"\"Calcula Recall@k\"\"\"\n",
        "        if not expected_ids:\n",
        "            return 0.0\n",
        "        retrieved_k = set(retrieved_ids[:k])\n",
        "        expected_set = set(expected_ids)\n",
        "        intersection = retrieved_k.intersection(expected_set)\n",
        "        return len(intersection) / len(expected_set)\n",
        "    \n",
        "    def _calculate_precision_at_k(self, retrieved_ids: List[str], expected_ids: List[str], k: int) -> float:\n",
        "        \"\"\"Calcula Precision@k\"\"\"\n",
        "        if k == 0:\n",
        "            return 0.0\n",
        "        retrieved_k = set(retrieved_ids[:k])\n",
        "        expected_set = set(expected_ids)\n",
        "        intersection = retrieved_k.intersection(expected_set)\n",
        "        return len(intersection) / k\n",
        "    \n",
        "    def _calculate_ndcg_at_k(self, retrieved_ids: List[str], expected_ids: List[str], k: int) -> float:\n",
        "        \"\"\"Calcula nDCG@k\"\"\"\n",
        "        if not expected_ids or k == 0:\n",
        "            return 0.0\n",
        "        relevance = [1 if doc_id in expected_ids else 0 for doc_id in retrieved_ids[:k]]\n",
        "        dcg = 0.0\n",
        "        for i, rel in enumerate(relevance):\n",
        "            dcg += rel / np.log2(i + 2)\n",
        "        ideal_relevance = [1] * min(len(expected_ids), k)\n",
        "        idcg = 0.0\n",
        "        for i, rel in enumerate(ideal_relevance):\n",
        "            idcg += rel / np.log2(i + 2)\n",
        "        return dcg / idcg if idcg > 0 else 0.0\n",
        "    \n",
        "    def _calculate_mrr(self, retrieved_ids: List[str], expected_ids: List[str]) -> float:\n",
        "        \"\"\"Calcula MRR\"\"\"\n",
        "        if not expected_ids:\n",
        "            return 0.0\n",
        "        expected_set = set(expected_ids)\n",
        "        for i, doc_id in enumerate(retrieved_ids):\n",
        "            if doc_id in expected_set:\n",
        "                return 1.0 / (i + 1)\n",
        "        return 0.0\n",
        "\n",
        "# Inicializar evaluador del pipeline completo\n",
        "rag_evaluator = RAGPipelineEvaluator(\n",
        "    qdrant_client=env_manager.qdrant_client,\n",
        "    embedding_models=env_manager.embedding_models,\n",
        "    reranking_models=env_manager.reranking_models,\n",
        "    llm_pipeline=env_manager.llm_pipeline,\n",
        "    collection_name=QDRANT_CONFIG['collection_name'],\n",
        "    top_k=QDRANT_CONFIG['top_k']\n",
        ")\n",
        "\n",
        "print(\"✅ Evaluador del pipeline RAG completo inicializado\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función para evaluación completa del pipeline RAG\n",
        "def evaluate_complete_rag_pipeline(dataset: pd.DataFrame, \n",
        "                                 embedding_models: Dict[str, Any],\n",
        "                                 reranking_models: Dict[str, Any],\n",
        "                                 use_reranking: bool = True) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Evalúa el pipeline completo de RAG con todas las combinaciones\n",
        "    \n",
        "    Args:\n",
        "        dataset: DataFrame con preguntas y respuestas esperadas\n",
        "        embedding_models: Diccionario con modelos de embedding\n",
        "        reranking_models: Diccionario con modelos de re-ranking\n",
        "        use_reranking: Si usar re-ranking o no\n",
        "    \n",
        "    Returns:\n",
        "        Diccionario con resultados de evaluación\n",
        "    \"\"\"\n",
        "    print(\"🚀 Iniciando evaluación completa del pipeline RAG...\")\n",
        "    print(f\"📊 Evaluando {len(dataset)} preguntas\")\n",
        "    print(f\"🧠 Modelos de embedding: {len(embedding_models)}\")\n",
        "    print(f\"🔄 Modelos de re-ranking: {len(reranking_models) if use_reranking else 0}\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    all_results = {}\n",
        "    \n",
        "    # Evaluar cada combinación de embedding + re-ranking\n",
        "    for embedding_name in embedding_models.keys():\n",
        "        print(f\"\\n🧠 Embedding: {embedding_name}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        embedding_results = {}\n",
        "        \n",
        "        # Si no usar re-ranking, evaluar solo con embedding\n",
        "        if not use_reranking:\n",
        "            print(f\"\\n  📝 Evaluando sin re-ranking...\")\n",
        "            print(\"  \" + \"-\" * 30)\n",
        "            \n",
        "            combination_results = {\n",
        "                'embedding_model': embedding_name,\n",
        "                'reranking_model': None,\n",
        "                'query_results': [],\n",
        "                'aggregated_metrics': {}\n",
        "            }\n",
        "            \n",
        "            # Evaluar cada pregunta\n",
        "            for idx, row in dataset.iterrows():\n",
        "                query = row['question']\n",
        "                expected_answer = row['expected_answer']\n",
        "                expected_articles = row['expected_articles']\n",
        "                \n",
        "                print(f\"    📝 Pregunta {idx + 1}/{len(dataset)}: {query[:40]}...\")\n",
        "                \n",
        "                try:\n",
        "                    # Evaluar pipeline completo\n",
        "                    query_metrics = rag_evaluator.evaluate_complete_pipeline(\n",
        "                        query=query,\n",
        "                        expected_answer=expected_answer,\n",
        "                        expected_articles=expected_articles,\n",
        "                        embedding_model_name=embedding_name,\n",
        "                        reranking_model_name=None\n",
        "                    )\n",
        "                    \n",
        "                    # Agregar información de la consulta\n",
        "                    query_metrics['query_id'] = idx\n",
        "                    combination_results['query_results'].append(query_metrics)\n",
        "                    \n",
        "                    # Mostrar métricas principales\n",
        "                    retrieval_metrics = query_metrics.get('retrieval_metrics', {})\n",
        "                    quality_scores = query_metrics.get('quality_scores', {})\n",
        "                    \n",
        "                    print(f\"      📈 Retrieval Recall@5: {retrieval_metrics.get('recall_at_5', 0):.3f}\")\n",
        "                    print(f\"      🎯 Calidad Overall: {quality_scores.get('overall_score', 0):.1f}/5\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"      ❌ Error en pregunta {idx + 1}: {e}\")\n",
        "                    continue\n",
        "            \n",
        "            # Calcular métricas agregadas\n",
        "            if combination_results['query_results']:\n",
        "                print(f\"\\n    📊 Calculando métricas agregadas...\")\n",
        "                aggregated = calculate_rag_aggregated_metrics(combination_results['query_results'])\n",
        "                combination_results['aggregated_metrics'] = aggregated\n",
        "                \n",
        "                # Mostrar resumen\n",
        "                print(f\"    🎯 Resumen:\")\n",
        "                print(f\"      - Recall@5 promedio: {aggregated.get('retrieval_recall_at_5', {}).get('mean', 0):.3f}\")\n",
        "                print(f\"      - Calidad promedio: {aggregated.get('quality_overall_score', {}).get('mean', 0):.1f}/5\")\n",
        "            \n",
        "            embedding_results['no_reranking'] = combination_results\n",
        "        \n",
        "        else:\n",
        "            # Evaluar con cada modelo de re-ranking\n",
        "            for reranking_name in reranking_models.keys():\n",
        "                print(f\"\\n  🔄 Re-ranking: {reranking_name}\")\n",
        "                print(\"  \" + \"-\" * 30)\n",
        "                \n",
        "                combination_results = {\n",
        "                    'embedding_model': embedding_name,\n",
        "                    'reranking_model': reranking_name,\n",
        "                    'query_results': [],\n",
        "                    'aggregated_metrics': {}\n",
        "                }\n",
        "                \n",
        "                # Evaluar cada pregunta\n",
        "                for idx, row in dataset.iterrows():\n",
        "                    query = row['question']\n",
        "                    expected_answer = row['expected_answer']\n",
        "                    expected_articles = row['expected_articles']\n",
        "                    \n",
        "                    print(f\"    📝 Pregunta {idx + 1}/{len(dataset)}: {query[:40]}...\")\n",
        "                    \n",
        "                    try:\n",
        "                        # Evaluar pipeline completo\n",
        "                        query_metrics = rag_evaluator.evaluate_complete_pipeline(\n",
        "                            query=query,\n",
        "                            expected_answer=expected_answer,\n",
        "                            expected_articles=expected_articles,\n",
        "                            embedding_model_name=embedding_name,\n",
        "                            reranking_model_name=reranking_name\n",
        "                        )\n",
        "                        \n",
        "                        # Agregar información de la consulta\n",
        "                        query_metrics['query_id'] = idx\n",
        "                        combination_results['query_results'].append(query_metrics)\n",
        "                        \n",
        "                        # Mostrar métricas principales\n",
        "                        retrieval_metrics = query_metrics.get('retrieval_metrics', {})\n",
        "                        quality_scores = query_metrics.get('quality_scores', {})\n",
        "                        \n",
        "                        print(f\"      📈 Retrieval Recall@5: {retrieval_metrics.get('recall_at_5', 0):.3f}\")\n",
        "                        print(f\"      🎯 Calidad Overall: {quality_scores.get('overall_score', 0):.1f}/5\")\n",
        "                        \n",
        "                    except Exception as e:\n",
        "                        print(f\"      ❌ Error en pregunta {idx + 1}: {e}\")\n",
        "                        continue\n",
        "                \n",
        "                # Calcular métricas agregadas\n",
        "                if combination_results['query_results']:\n",
        "                    print(f\"\\n    📊 Calculando métricas agregadas...\")\n",
        "                    aggregated = calculate_rag_aggregated_metrics(combination_results['query_results'])\n",
        "                    combination_results['aggregated_metrics'] = aggregated\n",
        "                    \n",
        "                    # Mostrar resumen\n",
        "                    print(f\"    🎯 Resumen:\")\n",
        "                    print(f\"      - Recall@5 promedio: {aggregated.get('retrieval_recall_at_5', {}).get('mean', 0):.3f}\")\n",
        "                    print(f\"      - Calidad promedio: {aggregated.get('quality_overall_score', {}).get('mean', 0):.1f}/5\")\n",
        "                \n",
        "                embedding_results[reranking_name] = combination_results\n",
        "        \n",
        "        all_results[embedding_name] = embedding_results\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"✅ Evaluación del pipeline RAG completada\")\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "def calculate_rag_aggregated_metrics(query_results: List[Dict[str, Any]]) -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Calcula métricas agregadas para evaluación del pipeline RAG\n",
        "    \n",
        "    Args:\n",
        "        query_results: Lista de resultados de consultas individuales\n",
        "    \n",
        "    Returns:\n",
        "        Diccionario con métricas agregadas\n",
        "    \"\"\"\n",
        "    if not query_results:\n",
        "        return {}\n",
        "    \n",
        "    # Extraer métricas de retrieval\n",
        "    retrieval_metrics = {}\n",
        "    for result in query_results:\n",
        "        if 'retrieval_metrics' in result:\n",
        "            for metric, value in result['retrieval_metrics'].items():\n",
        "                if f'retrieval_{metric}' not in retrieval_metrics:\n",
        "                    retrieval_metrics[f'retrieval_{metric}'] = []\n",
        "                retrieval_metrics[f'retrieval_{metric}'].append(value)\n",
        "    \n",
        "    # Extraer métricas de calidad\n",
        "    quality_metrics = {}\n",
        "    for result in query_results:\n",
        "        if 'quality_scores' in result:\n",
        "            for metric, value in result['quality_scores'].items():\n",
        "                if metric != 'explanation':  # Excluir explicación\n",
        "                    if f'quality_{metric}' not in quality_metrics:\n",
        "                        quality_metrics[f'quality_{metric}'] = []\n",
        "                    quality_metrics[f'quality_{metric}'].append(value)\n",
        "    \n",
        "    # Combinar todas las métricas\n",
        "    all_metrics = {**retrieval_metrics, **quality_metrics}\n",
        "    \n",
        "    # Calcular estadísticas\n",
        "    aggregated = {}\n",
        "    for metric_name, values in all_metrics.items():\n",
        "        if values:\n",
        "            aggregated[metric_name] = {\n",
        "                'mean': np.mean(values),\n",
        "                'std': np.std(values),\n",
        "                'min': np.min(values),\n",
        "                'max': np.max(values),\n",
        "                'median': np.median(values)\n",
        "            }\n",
        "    \n",
        "    return aggregated\n",
        "\n",
        "# Ejecutar evaluación completa del pipeline RAG\n",
        "print(\"🔄 Iniciando evaluación completa del pipeline RAG...\")\n",
        "complete_rag_results = evaluate_complete_rag_pipeline(\n",
        "    dataset, \n",
        "    env_manager.embedding_models, \n",
        "    env_manager.reranking_models,\n",
        "    use_reranking=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualización de resultados del pipeline RAG completo\n",
        "def visualize_complete_rag_results(rag_results: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Visualiza los resultados de evaluación del pipeline RAG completo\n",
        "    \"\"\"\n",
        "    if not rag_results:\n",
        "        print(\"❌ No hay resultados para visualizar\")\n",
        "        return\n",
        "    \n",
        "    # Preparar datos para visualización\n",
        "    combinations = []\n",
        "    retrieval_data = []\n",
        "    quality_data = []\n",
        "    \n",
        "    for embedding_name, embedding_data in rag_results.items():\n",
        "        for reranking_name, combination_data in embedding_data.items():\n",
        "            if combination_data['aggregated_metrics']:\n",
        "                combination_key = f\"{embedding_name}\\n+ {reranking_name if reranking_name else 'No Re-ranking'}\"\n",
        "                combinations.append(combination_key)\n",
        "                \n",
        "                # Extraer métricas de retrieval\n",
        "                metrics = combination_data['aggregated_metrics']\n",
        "                retrieval_data.append({\n",
        "                    'combination': combination_key,\n",
        "                    'recall_at_5': metrics.get('retrieval_recall_at_5', {}).get('mean', 0),\n",
        "                    'precision_at_5': metrics.get('retrieval_precision_at_5', {}).get('mean', 0),\n",
        "                    'ndcg_at_5': metrics.get('retrieval_ndcg_at_5', {}).get('mean', 0),\n",
        "                    'mrr': metrics.get('retrieval_mrr', {}).get('mean', 0)\n",
        "                })\n",
        "                \n",
        "                # Extraer métricas de calidad\n",
        "                quality_data.append({\n",
        "                    'combination': combination_key,\n",
        "                    'coherence': metrics.get('quality_coherence', {}).get('mean', 0),\n",
        "                    'relevance': metrics.get('quality_relevance', {}).get('mean', 0),\n",
        "                    'completeness': metrics.get('quality_completeness', {}).get('mean', 0),\n",
        "                    'fidelity': metrics.get('quality_fidelity', {}).get('mean', 0),\n",
        "                    'conciseness': metrics.get('quality_conciseness', {}).get('mean', 0),\n",
        "                    'overall_score': metrics.get('quality_overall_score', {}).get('mean', 0)\n",
        "                })\n",
        "    \n",
        "    if not retrieval_data or not quality_data:\n",
        "        print(\"❌ No hay datos suficientes para visualizar\")\n",
        "        return\n",
        "    \n",
        "    # Crear figura con subplots\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
        "    fig.suptitle('Evaluación Completa del Pipeline RAG', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Colores para cada combinación\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(combinations)))\n",
        "    \n",
        "    # Métricas de retrieval\n",
        "    retrieval_metrics = ['recall_at_5', 'precision_at_5', 'ndcg_at_5', 'mrr']\n",
        "    retrieval_titles = ['Recall@5', 'Precision@5', 'nDCG@5', 'MRR']\n",
        "    \n",
        "    for idx, (metric, title) in enumerate(zip(retrieval_metrics, retrieval_titles)):\n",
        "        row = idx // 2\n",
        "        col = idx % 2\n",
        "        \n",
        "        values = [data[metric] for data in retrieval_data]\n",
        "        combination_labels = [data['combination'] for data in retrieval_data]\n",
        "        \n",
        "        bars = axes[row, col].bar(range(len(values)), values, color=colors, alpha=0.7)\n",
        "        axes[row, col].set_title(f'Retrieval - {title}')\n",
        "        axes[row, col].set_ylabel('Score')\n",
        "        axes[row, col].set_ylim(0, 1)\n",
        "        axes[row, col].set_xticks(range(len(combination_labels)))\n",
        "        axes[row, col].set_xticklabels(combination_labels, rotation=45, ha='right')\n",
        "        \n",
        "        # Agregar valores en las barras\n",
        "        for bar, value in zip(bars, values):\n",
        "            height = bar.get_height()\n",
        "            axes[row, col].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                              f'{value:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "    \n",
        "    # Métricas de calidad\n",
        "    quality_metrics = ['coherence', 'relevance', 'completeness', 'fidelity', 'conciseness', 'overall_score']\n",
        "    quality_titles = ['Coherencia', 'Relevancia', 'Completitud', 'Fidelidad', 'Concisión', 'Score General']\n",
        "    \n",
        "    for idx, (metric, title) in enumerate(zip(quality_metrics, quality_titles)):\n",
        "        row = 2\n",
        "        col = idx % 2\n",
        "        \n",
        "        values = [data[metric] for data in quality_data]\n",
        "        combination_labels = [data['combination'] for data in quality_data]\n",
        "        \n",
        "        bars = axes[row, col].bar(range(len(values)), values, color=colors, alpha=0.7)\n",
        "        axes[row, col].set_title(f'Calidad - {title}')\n",
        "        axes[row, col].set_ylabel('Score (1-5)')\n",
        "        axes[row, col].set_ylim(0, 5)\n",
        "        axes[row, col].set_xticks(range(len(combination_labels)))\n",
        "        axes[row, col].set_xticklabels(combination_labels, rotation=45, ha='right')\n",
        "        \n",
        "        # Agregar valores en las barras\n",
        "        for bar, value in zip(bars, values):\n",
        "            height = bar.get_height()\n",
        "            axes[row, col].text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
        "                              f'{value:.1f}', ha='center', va='bottom', fontsize=8)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Crear tabla comparativa\n",
        "    print(\"\\n📊 Tabla Comparativa del Pipeline RAG:\")\n",
        "    print(\"=\" * 120)\n",
        "    \n",
        "    # Combinar datos para la tabla\n",
        "    comparison_data = []\n",
        "    for i, retrieval in enumerate(retrieval_data):\n",
        "        quality = quality_data[i]\n",
        "        row = {\n",
        "            'Combinación': retrieval['combination'],\n",
        "            'Recall@5': f\"{retrieval['recall_at_5']:.3f}\",\n",
        "            'Precision@5': f\"{retrieval['precision_at_5']:.3f}\",\n",
        "            'nDCG@5': f\"{retrieval['ndcg_at_5']:.3f}\",\n",
        "            'MRR': f\"{retrieval['mrr']:.3f}\",\n",
        "            'Coherencia': f\"{quality['coherence']:.1f}\",\n",
        "            'Relevancia': f\"{quality['relevance']:.1f}\",\n",
        "            'Completitud': f\"{quality['completeness']:.1f}\",\n",
        "            'Fidelidad': f\"{quality['fidelity']:.1f}\",\n",
        "            'Concisión': f\"{quality['conciseness']:.1f}\",\n",
        "            'Score General': f\"{quality['overall_score']:.1f}\"\n",
        "        }\n",
        "        comparison_data.append(row)\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    print(comparison_df.to_string(index=False))\n",
        "    \n",
        "    # Identificar mejores combinaciones\n",
        "    print(\"\\n🏆 Mejores Combinaciones:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Mejor en retrieval\n",
        "    best_retrieval_idx = max(range(len(retrieval_data)), \n",
        "                           key=lambda i: retrieval_data[i]['recall_at_5'])\n",
        "    print(f\"  Mejor Retrieval: {retrieval_data[best_retrieval_idx]['combination']} \"\n",
        "          f\"(Recall@5: {retrieval_data[best_retrieval_idx]['recall_at_5']:.3f})\")\n",
        "    \n",
        "    # Mejor en calidad\n",
        "    best_quality_idx = max(range(len(quality_data)), \n",
        "                          key=lambda i: quality_data[i]['overall_score'])\n",
        "    print(f\"  Mejor Calidad: {quality_data[best_quality_idx]['combination']} \"\n",
        "          f\"(Score: {quality_data[best_quality_idx]['overall_score']:.1f}/5)\")\n",
        "\n",
        "def analyze_complete_rag_performance(rag_results: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Analiza el rendimiento del pipeline RAG completo en detalle\n",
        "    \"\"\"\n",
        "    print(\"\\n🔍 Análisis Detallado del Pipeline RAG:\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for embedding_name, embedding_data in rag_results.items():\n",
        "        print(f\"\\n🧠 Embedding: {embedding_name}\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        for reranking_name, combination_data in embedding_data.items():\n",
        "            print(f\"\\n  🔄 Re-ranking: {reranking_name if reranking_name else 'No Re-ranking'}\")\n",
        "            print(\"  \" + \"-\" * 30)\n",
        "            \n",
        "            aggregated = combination_data['aggregated_metrics']\n",
        "            query_results = combination_data['query_results']\n",
        "            \n",
        "            if not aggregated:\n",
        "                print(\"    ❌ No hay métricas disponibles\")\n",
        "                continue\n",
        "            \n",
        "            # Métricas de retrieval\n",
        "            print(f\"  📈 Métricas de Retrieval:\")\n",
        "            for metric in ['retrieval_recall_at_5', 'retrieval_precision_at_5', \n",
        "                          'retrieval_ndcg_at_5', 'retrieval_mrr']:\n",
        "                if metric in aggregated:\n",
        "                    mean_val = aggregated[metric]['mean']\n",
        "                    std_val = aggregated[metric]['std']\n",
        "                    print(f\"    {metric.replace('retrieval_', '').replace('_at_5', '@5').title()}: {mean_val:.3f} ± {std_val:.3f}\")\n",
        "            \n",
        "            # Métricas de calidad\n",
        "            print(f\"  🎯 Métricas de Calidad:\")\n",
        "            for metric in ['quality_coherence', 'quality_relevance', 'quality_completeness', \n",
        "                          'quality_fidelity', 'quality_conciseness', 'quality_overall_score']:\n",
        "                if metric in aggregated:\n",
        "                    mean_val = aggregated[metric]['mean']\n",
        "                    std_val = aggregated[metric]['std']\n",
        "                    print(f\"    {metric.replace('quality_', '').title()}: {mean_val:.1f} ± {std_val:.1f}\")\n",
        "            \n",
        "            # Análisis de respuestas\n",
        "            if query_results:\n",
        "                print(f\"  📝 Análisis de Respuestas:\")\n",
        "                \n",
        "                # Mejores respuestas (mayor calidad general)\n",
        "                best_responses = sorted(query_results, \n",
        "                                      key=lambda x: x.get('quality_scores', {}).get('overall_score', 0), \n",
        "                                      reverse=True)[:3]\n",
        "                print(f\"    Mejores respuestas (Calidad General):\")\n",
        "                for i, response in enumerate(best_responses, 1):\n",
        "                    quality = response.get('quality_scores', {})\n",
        "                    overall_score = quality.get('overall_score', 0)\n",
        "                    query_text = response.get('query', '')[:50]\n",
        "                    print(f\"      {i}. {query_text}... (Score: {overall_score:.1f}/5)\")\n",
        "                \n",
        "                # Peores respuestas\n",
        "                worst_responses = sorted(query_results, \n",
        "                                       key=lambda x: x.get('quality_scores', {}).get('overall_score', 0))[:3]\n",
        "                print(f\"    Peores respuestas (Calidad General):\")\n",
        "                for i, response in enumerate(worst_responses, 1):\n",
        "                    quality = response.get('quality_scores', {})\n",
        "                    overall_score = quality.get('overall_score', 0)\n",
        "                    query_text = response.get('query', '')[:50]\n",
        "                    print(f\"      {i}. {query_text}... (Score: {overall_score:.1f}/5)\")\n",
        "\n",
        "# Ejecutar visualizaciones y análisis\n",
        "visualize_complete_rag_results(complete_rag_results)\n",
        "analyze_complete_rag_performance(complete_rag_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar resultados de evaluación del pipeline RAG completo\n",
        "def save_complete_rag_evaluation_results(rag_results: Dict[str, Any], \n",
        "                                        evaluation_name: str = \"complete_rag_evaluation\") -> str:\n",
        "    \"\"\"\n",
        "    Guarda los resultados de evaluación del pipeline RAG completo\n",
        "    \n",
        "    Args:\n",
        "        rag_results: Resultados de la evaluación\n",
        "        evaluation_name: Nombre de la evaluación\n",
        "    \n",
        "    Returns:\n",
        "        Ruta del archivo guardado\n",
        "    \"\"\"\n",
        "    # Preparar configuración\n",
        "    config = {\n",
        "        'embedding_models': list(EMBEDDING_MODELS.keys()),\n",
        "        'reranking_models': list(RERANKING_MODELS.keys()),\n",
        "        'retrieval_metrics': RETRIEVAL_METRICS,\n",
        "        'llm_evaluation_criteria': LLM_EVALUATION_CRITERIA,\n",
        "        'qdrant_config': QDRANT_CONFIG,\n",
        "        'llm_config': LLM_CONFIG,\n",
        "        'dataset_size': len(dataset)\n",
        "    }\n",
        "    \n",
        "    # Preparar metadatos\n",
        "    metadata = {\n",
        "        'evaluation_type': 'complete_rag',\n",
        "        'description': 'Evaluación completa del pipeline RAG incluyendo generación y evaluación con LLM',\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'total_queries': len(dataset),\n",
        "        'total_combinations': sum(len(embedding_data) for embedding_data in rag_results.values())\n",
        "    }\n",
        "    \n",
        "    # Guardar evaluación\n",
        "    filepath = eval_manager.save_evaluation(\n",
        "        evaluation_name=evaluation_name,\n",
        "        config=config,\n",
        "        results={'complete_rag_metrics': rag_results},\n",
        "        metadata=metadata\n",
        "    )\n",
        "    \n",
        "    print(f\"💾 Resultados de evaluación del pipeline RAG completo guardados en: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "# Guardar resultados\n",
        "complete_rag_filepath = save_complete_rag_evaluation_results(complete_rag_results)\n",
        "\n",
        "# Resumen final de la evaluación del pipeline RAG completo\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"📋 RESUMEN DE EVALUACIÓN DEL PIPELINE RAG COMPLETO\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if complete_rag_results:\n",
        "    # Encontrar la mejor combinación general\n",
        "    best_combination = None\n",
        "    best_overall_score = -1\n",
        "    \n",
        "    for embedding_name, embedding_data in complete_rag_results.items():\n",
        "        for reranking_name, combination_data in embedding_data.items():\n",
        "            aggregated = combination_data['aggregated_metrics']\n",
        "            if aggregated:\n",
        "                # Calcular score combinado (retrieval + calidad)\n",
        "                retrieval_score = aggregated.get('retrieval_recall_at_5', {}).get('mean', 0)\n",
        "                quality_score = aggregated.get('quality_overall_score', {}).get('mean', 0)\n",
        "                \n",
        "                # Score combinado (peso 40% retrieval, 60% calidad)\n",
        "                combined_score = (0.4 * retrieval_score) + (0.6 * (quality_score / 5))\n",
        "                \n",
        "                if combined_score > best_overall_score:\n",
        "                    best_overall_score = combined_score\n",
        "                    best_combination = f\"{embedding_name} + {reranking_name if reranking_name else 'No Re-ranking'}\"\n",
        "    \n",
        "    if best_combination:\n",
        "        print(f\"🏆 Mejor combinación general: {best_combination}\")\n",
        "        print(f\"   Score combinado: {best_overall_score:.3f}\")\n",
        "    \n",
        "    # Estadísticas por combinación\n",
        "    print(f\"\\n📊 Estadísticas por combinación:\")\n",
        "    for embedding_name, embedding_data in complete_rag_results.items():\n",
        "        print(f\"\\n  🧠 Embedding: {embedding_name}\")\n",
        "        for reranking_name, combination_data in embedding_data.items():\n",
        "            aggregated = combination_data['aggregated_metrics']\n",
        "            if aggregated:\n",
        "                # Métricas de retrieval\n",
        "                recall = aggregated.get('retrieval_recall_at_5', {}).get('mean', 0)\n",
        "                precision = aggregated.get('retrieval_precision_at_5', {}).get('mean', 0)\n",
        "                ndcg = aggregated.get('retrieval_ndcg_at_5', {}).get('mean', 0)\n",
        "                mrr = aggregated.get('retrieval_mrr', {}).get('mean', 0)\n",
        "                \n",
        "                # Métricas de calidad\n",
        "                coherence = aggregated.get('quality_coherence', {}).get('mean', 0)\n",
        "                relevance = aggregated.get('quality_relevance', {}).get('mean', 0)\n",
        "                completeness = aggregated.get('quality_completeness', {}).get('mean', 0)\n",
        "                fidelity = aggregated.get('quality_fidelity', {}).get('mean', 0)\n",
        "                conciseness = aggregated.get('quality_conciseness', {}).get('mean', 0)\n",
        "                overall_quality = aggregated.get('quality_overall_score', {}).get('mean', 0)\n",
        "                \n",
        "                print(f\"    🔄 {reranking_name if reranking_name else 'No Re-ranking'}:\")\n",
        "                print(f\"      📈 Retrieval:\")\n",
        "                print(f\"        - Recall@5: {recall:.3f}\")\n",
        "                print(f\"        - Precision@5: {precision:.3f}\")\n",
        "                print(f\"        - nDCG@5: {ndcg:.3f}\")\n",
        "                print(f\"        - MRR: {mrr:.3f}\")\n",
        "                print(f\"      🎯 Calidad:\")\n",
        "                print(f\"        - Coherencia: {coherence:.1f}/5\")\n",
        "                print(f\"        - Relevancia: {relevance:.1f}/5\")\n",
        "                print(f\"        - Completitud: {completeness:.1f}/5\")\n",
        "                print(f\"        - Fidelidad: {fidelity:.1f}/5\")\n",
        "                print(f\"        - Concisión: {conciseness:.1f}/5\")\n",
        "                print(f\"        - Score General: {overall_quality:.1f}/5\")\n",
        "    \n",
        "    # Análisis de correlación entre retrieval y calidad\n",
        "    print(f\"\\n📈 Análisis de Correlación:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    retrieval_scores = []\n",
        "    quality_scores = []\n",
        "    \n",
        "    for embedding_data in complete_rag_results.values():\n",
        "        for combination_data in embedding_data.values():\n",
        "            aggregated = combination_data['aggregated_metrics']\n",
        "            if aggregated:\n",
        "                retrieval_score = aggregated.get('retrieval_recall_at_5', {}).get('mean', 0)\n",
        "                quality_score = aggregated.get('quality_overall_score', {}).get('mean', 0)\n",
        "                if retrieval_score > 0 and quality_score > 0:\n",
        "                    retrieval_scores.append(retrieval_score)\n",
        "                    quality_scores.append(quality_score)\n",
        "    \n",
        "    if len(retrieval_scores) > 1:\n",
        "        correlation = np.corrcoef(retrieval_scores, quality_scores)[0, 1]\n",
        "        print(f\"  - Correlación Retrieval-Calidad: {correlation:.3f}\")\n",
        "        \n",
        "        if correlation > 0.5:\n",
        "            print(\"  ✅ Fuerte correlación positiva entre retrieval y calidad\")\n",
        "        elif correlation > 0.2:\n",
        "            print(\"  ⚠️  Correlación moderada entre retrieval y calidad\")\n",
        "        else:\n",
        "            print(\"  ❌ Correlación débil entre retrieval y calidad\")\n",
        "    \n",
        "    print(f\"\\n✅ Evaluación del pipeline RAG completo finalizada exitosamente\")\n",
        "    print(f\"📁 Resultados guardados en: {complete_rag_filepath}\")\n",
        "else:\n",
        "    print(\"❌ No se pudieron obtener resultados de evaluación del pipeline RAG completo\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Análisis Comparativo Global\n",
        "\n",
        "## Objetivo del Análisis\n",
        "\n",
        "Esta sección realiza un análisis comparativo integral de todos los resultados obtenidos en las evaluaciones anteriores, combinando métricas de retrieval, re-ranking y calidad de respuesta para identificar patrones, tendencias y recomendaciones finales.\n",
        "\n",
        "### 🎯 Proceso de Análisis Global\n",
        "\n",
        "1. **Consolidación de Resultados**: Integrar datos de todas las evaluaciones\n",
        "2. **Análisis Comparativo**: Comparar rendimiento entre modelos y combinaciones\n",
        "3. **Identificación de Patrones**: Detectar tendencias y correlaciones\n",
        "4. **Ranking Global**: Clasificar combinaciones por rendimiento general\n",
        "5. **Recomendaciones**: Sugerir mejores configuraciones y mejoras\n",
        "\n",
        "### 📊 Dimensiones de Análisis\n",
        "\n",
        "#### Análisis por Componente\n",
        "- **Embeddings**: Rendimiento de diferentes modelos de embedding\n",
        "- **Re-ranking**: Efectividad de modelos cross-encoder\n",
        "- **LLM**: Calidad de generación y evaluación\n",
        "- **Pipeline Completo**: Rendimiento end-to-end\n",
        "\n",
        "#### Análisis por Métrica\n",
        "- **Retrieval**: Recall, Precision, nDCG, MRR\n",
        "- **Calidad**: Coherencia, Relevancia, Completitud, Fidelidad, Concisión\n",
        "- **Eficiencia**: Tiempo de procesamiento, uso de recursos\n",
        "- **Robustez**: Consistencia entre consultas\n",
        "\n",
        "#### Análisis por Combinación\n",
        "- **Mejores Combinaciones**: Top performers por criterio\n",
        "- **Trade-offs**: Balance entre diferentes métricas\n",
        "- **Escalabilidad**: Rendimiento con diferentes tamaños de dataset\n",
        "- **Estabilidad**: Consistencia de resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clase para análisis comparativo global\n",
        "class GlobalComparativeAnalyzer:\n",
        "    \"\"\"\n",
        "    Realiza análisis comparativo global de todos los resultados de evaluación\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, retrieval_results: Dict[str, Any], \n",
        "                 reranking_results: Dict[str, Any], \n",
        "                 complete_rag_results: Dict[str, Any]):\n",
        "        self.retrieval_results = retrieval_results\n",
        "        self.reranking_results = reranking_results\n",
        "        self.complete_rag_results = complete_rag_results\n",
        "        self.analysis_results = {}\n",
        "    \n",
        "    def consolidate_all_results(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Consolida todos los resultados de las evaluaciones\n",
        "        \n",
        "        Returns:\n",
        "            Diccionario con resultados consolidados\n",
        "        \"\"\"\n",
        "        print(\"🔄 Consolidando resultados de todas las evaluaciones...\")\n",
        "        \n",
        "        consolidated = {\n",
        "            'retrieval_only': self._consolidate_retrieval_results(),\n",
        "            'reranking_improvements': self._consolidate_reranking_results(),\n",
        "            'complete_pipeline': self._consolidate_complete_rag_results(),\n",
        "            'global_rankings': {},\n",
        "            'insights': {}\n",
        "        }\n",
        "        \n",
        "        # Generar rankings globales\n",
        "        consolidated['global_rankings'] = self._generate_global_rankings(consolidated)\n",
        "        \n",
        "        # Generar insights\n",
        "        consolidated['insights'] = self._generate_insights(consolidated)\n",
        "        \n",
        "        print(\"✅ Consolidación completada\")\n",
        "        return consolidated\n",
        "    \n",
        "    def _consolidate_retrieval_results(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Consolida resultados de evaluación de retrievers\n",
        "        \"\"\"\n",
        "        if not self.retrieval_results:\n",
        "            return {}\n",
        "        \n",
        "        consolidated = {}\n",
        "        \n",
        "        for model_name, model_data in self.retrieval_results.items():\n",
        "            if 'aggregated_metrics' in model_data:\n",
        "                metrics = model_data['aggregated_metrics']\n",
        "                consolidated[model_name] = {\n",
        "                    'recall_at_5': metrics.get('recall_at_5', {}).get('mean', 0),\n",
        "                    'precision_at_5': metrics.get('precision_at_5', {}).get('mean', 0),\n",
        "                    'ndcg_at_5': metrics.get('ndcg_at_5', {}).get('mean', 0),\n",
        "                    'mrr': metrics.get('mrr', {}).get('mean', 0),\n",
        "                    'total_queries': len(model_data.get('query_results', []))\n",
        "                }\n",
        "        \n",
        "        return consolidated\n",
        "    \n",
        "    def _consolidate_reranking_results(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Consolida resultados de evaluación de re-ranking\n",
        "        \"\"\"\n",
        "        if not self.reranking_results:\n",
        "            return {}\n",
        "        \n",
        "        consolidated = {}\n",
        "        \n",
        "        for embedding_name, embedding_data in self.reranking_results.items():\n",
        "            for reranking_name, combination_data in embedding_data.items():\n",
        "                if 'aggregated_metrics' in combination_data:\n",
        "                    metrics = combination_data['aggregated_metrics']\n",
        "                    key = f\"{embedding_name}+{reranking_name}\"\n",
        "                    consolidated[key] = {\n",
        "                        'embedding_model': embedding_name,\n",
        "                        'reranking_model': reranking_name,\n",
        "                        'recall_improvement': metrics.get('recall_improvement_at_5', {}).get('mean', 0),\n",
        "                        'precision_improvement': metrics.get('precision_improvement_at_5', {}).get('mean', 0),\n",
        "                        'ndcg_improvement': metrics.get('ndcg_improvement_at_5', {}).get('mean', 0),\n",
        "                        'mrr_improvement': metrics.get('mrr_improvement', {}).get('mean', 0),\n",
        "                        'total_queries': len(combination_data.get('query_results', []))\n",
        "                    }\n",
        "        \n",
        "        return consolidated\n",
        "    \n",
        "    def _consolidate_complete_rag_results(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Consolida resultados de evaluación del pipeline completo\n",
        "        \"\"\"\n",
        "        if not self.complete_rag_results:\n",
        "            return {}\n",
        "        \n",
        "        consolidated = {}\n",
        "        \n",
        "        for embedding_name, embedding_data in self.complete_rag_results.items():\n",
        "            for reranking_name, combination_data in embedding_data.items():\n",
        "                if 'aggregated_metrics' in combination_data:\n",
        "                    metrics = combination_data['aggregated_metrics']\n",
        "                    key = f\"{embedding_name}+{reranking_name if reranking_name else 'NoReranking'}\"\n",
        "                    consolidated[key] = {\n",
        "                        'embedding_model': embedding_name,\n",
        "                        'reranking_model': reranking_name,\n",
        "                        'retrieval_recall_at_5': metrics.get('retrieval_recall_at_5', {}).get('mean', 0),\n",
        "                        'retrieval_precision_at_5': metrics.get('retrieval_precision_at_5', {}).get('mean', 0),\n",
        "                        'retrieval_ndcg_at_5': metrics.get('retrieval_ndcg_at_5', {}).get('mean', 0),\n",
        "                        'retrieval_mrr': metrics.get('retrieval_mrr', {}).get('mean', 0),\n",
        "                        'quality_coherence': metrics.get('quality_coherence', {}).get('mean', 0),\n",
        "                        'quality_relevance': metrics.get('quality_relevance', {}).get('mean', 0),\n",
        "                        'quality_completeness': metrics.get('quality_completeness', {}).get('mean', 0),\n",
        "                        'quality_fidelity': metrics.get('quality_fidelity', {}).get('mean', 0),\n",
        "                        'quality_conciseness': metrics.get('quality_conciseness', {}).get('mean', 0),\n",
        "                        'quality_overall_score': metrics.get('quality_overall_score', {}).get('mean', 0),\n",
        "                        'total_queries': len(combination_data.get('query_results', []))\n",
        "                    }\n",
        "        \n",
        "        return consolidated\n",
        "    \n",
        "    def _generate_global_rankings(self, consolidated: Dict[str, Any]) -> Dict[str, List[str]]:\n",
        "        \"\"\"\n",
        "        Genera rankings globales por diferentes criterios\n",
        "        \"\"\"\n",
        "        rankings = {}\n",
        "        \n",
        "        # Ranking por retrieval (solo embeddings)\n",
        "        if 'retrieval_only' in consolidated:\n",
        "            retrieval_data = consolidated['retrieval_only']\n",
        "            rankings['best_retrieval'] = sorted(\n",
        "                retrieval_data.keys(),\n",
        "                key=lambda x: retrieval_data[x]['recall_at_5'],\n",
        "                reverse=True\n",
        "            )\n",
        "        \n",
        "        # Ranking por mejoras de re-ranking\n",
        "        if 'reranking_improvements' in consolidated:\n",
        "            reranking_data = consolidated['reranking_improvements']\n",
        "            rankings['best_reranking_improvement'] = sorted(\n",
        "                reranking_data.keys(),\n",
        "                key=lambda x: reranking_data[x]['ndcg_improvement'],\n",
        "                reverse=True\n",
        "            )\n",
        "        \n",
        "        # Ranking por pipeline completo (score combinado)\n",
        "        if 'complete_pipeline' in consolidated:\n",
        "            pipeline_data = consolidated['complete_pipeline']\n",
        "            rankings['best_complete_pipeline'] = sorted(\n",
        "                pipeline_data.keys(),\n",
        "                key=lambda x: self._calculate_combined_score(pipeline_data[x]),\n",
        "                reverse=True\n",
        "            )\n",
        "        \n",
        "        # Ranking por calidad de respuesta\n",
        "        if 'complete_pipeline' in consolidated:\n",
        "            pipeline_data = consolidated['complete_pipeline']\n",
        "            rankings['best_quality'] = sorted(\n",
        "                pipeline_data.keys(),\n",
        "                key=lambda x: pipeline_data[x]['quality_overall_score'],\n",
        "                reverse=True\n",
        "            )\n",
        "        \n",
        "        return rankings\n",
        "    \n",
        "    def _calculate_combined_score(self, metrics: Dict[str, Any]) -> float:\n",
        "        \"\"\"\n",
        "        Calcula score combinado (40% retrieval + 60% calidad)\n",
        "        \"\"\"\n",
        "        retrieval_score = metrics.get('retrieval_recall_at_5', 0)\n",
        "        quality_score = metrics.get('quality_overall_score', 0) / 5  # Normalizar a 0-1\n",
        "        \n",
        "        return (0.4 * retrieval_score) + (0.6 * quality_score)\n",
        "    \n",
        "    def _generate_insights(self, consolidated: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Genera insights y análisis de los resultados\n",
        "        \"\"\"\n",
        "        insights = {\n",
        "            'retrieval_insights': self._analyze_retrieval_patterns(consolidated),\n",
        "            'reranking_insights': self._analyze_reranking_patterns(consolidated),\n",
        "            'quality_insights': self._analyze_quality_patterns(consolidated),\n",
        "            'correlation_insights': self._analyze_correlations(consolidated),\n",
        "            'recommendations': self._generate_recommendations(consolidated)\n",
        "        }\n",
        "        \n",
        "        return insights\n",
        "    \n",
        "    def _analyze_retrieval_patterns(self, consolidated: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analiza patrones en los resultados de retrieval\n",
        "        \"\"\"\n",
        "        if 'retrieval_only' not in consolidated:\n",
        "            return {}\n",
        "        \n",
        "        retrieval_data = consolidated['retrieval_only']\n",
        "        \n",
        "        # Encontrar mejor y peor modelo\n",
        "        best_model = max(retrieval_data.keys(), key=lambda x: retrieval_data[x]['recall_at_5'])\n",
        "        worst_model = min(retrieval_data.keys(), key=lambda x: retrieval_data[x]['recall_at_5'])\n",
        "        \n",
        "        # Calcular estadísticas\n",
        "        recall_scores = [data['recall_at_5'] for data in retrieval_data.values()]\n",
        "        precision_scores = [data['precision_at_5'] for data in retrieval_data.values()]\n",
        "        ndcg_scores = [data['ndcg_at_5'] for data in retrieval_data.values()]\n",
        "        \n",
        "        return {\n",
        "            'best_model': best_model,\n",
        "            'worst_model': worst_model,\n",
        "            'recall_range': (min(recall_scores), max(recall_scores)),\n",
        "            'precision_range': (min(precision_scores), max(precision_scores)),\n",
        "            'ndcg_range': (min(ndcg_scores), max(ndcg_scores)),\n",
        "            'recall_std': np.std(recall_scores),\n",
        "            'precision_std': np.std(precision_scores),\n",
        "            'ndcg_std': np.std(ndcg_scores)\n",
        "        }\n",
        "    \n",
        "    def _analyze_reranking_patterns(self, consolidated: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analiza patrones en los resultados de re-ranking\n",
        "        \"\"\"\n",
        "        if 'reranking_improvements' not in consolidated:\n",
        "            return {}\n",
        "        \n",
        "        reranking_data = consolidated['reranking_improvements']\n",
        "        \n",
        "        # Calcular mejoras promedio por modelo de re-ranking\n",
        "        reranking_models = {}\n",
        "        for key, data in reranking_data.items():\n",
        "            model_name = data['reranking_model']\n",
        "            if model_name not in reranking_models:\n",
        "                reranking_models[model_name] = []\n",
        "            reranking_models[model_name].append(data['ndcg_improvement'])\n",
        "        \n",
        "        # Calcular estadísticas por modelo\n",
        "        model_stats = {}\n",
        "        for model_name, improvements in reranking_models.items():\n",
        "            model_stats[model_name] = {\n",
        "                'mean_improvement': np.mean(improvements),\n",
        "                'std_improvement': np.std(improvements),\n",
        "                'positive_improvements': sum(1 for x in improvements if x > 0),\n",
        "                'total_combinations': len(improvements)\n",
        "            }\n",
        "        \n",
        "        # Encontrar mejor modelo de re-ranking\n",
        "        best_reranking_model = max(model_stats.keys(), \n",
        "                                 key=lambda x: model_stats[x]['mean_improvement'])\n",
        "        \n",
        "        return {\n",
        "            'best_reranking_model': best_reranking_model,\n",
        "            'model_stats': model_stats,\n",
        "            'overall_effectiveness': sum(1 for data in reranking_data.values() \n",
        "                                       if data['ndcg_improvement'] > 0) / len(reranking_data)\n",
        "        }\n",
        "    \n",
        "    def _analyze_quality_patterns(self, consolidated: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analiza patrones en los resultados de calidad\n",
        "        \"\"\"\n",
        "        if 'complete_pipeline' not in consolidated:\n",
        "            return {}\n",
        "        \n",
        "        pipeline_data = consolidated['complete_pipeline']\n",
        "        \n",
        "        # Calcular estadísticas de calidad\n",
        "        quality_metrics = ['quality_coherence', 'quality_relevance', 'quality_completeness', \n",
        "                          'quality_fidelity', 'quality_conciseness', 'quality_overall_score']\n",
        "        \n",
        "        quality_stats = {}\n",
        "        for metric in quality_metrics:\n",
        "            values = [data[metric] for data in pipeline_data.values() if metric in data]\n",
        "            if values:\n",
        "                quality_stats[metric] = {\n",
        "                    'mean': np.mean(values),\n",
        "                    'std': np.std(values),\n",
        "                    'min': np.min(values),\n",
        "                    'max': np.max(values)\n",
        "                }\n",
        "        \n",
        "        # Encontrar mejor combinación por calidad\n",
        "        best_quality_combo = max(pipeline_data.keys(), \n",
        "                               key=lambda x: pipeline_data[x]['quality_overall_score'])\n",
        "        \n",
        "        return {\n",
        "            'best_quality_combo': best_quality_combo,\n",
        "            'quality_stats': quality_stats,\n",
        "            'overall_quality_mean': quality_stats.get('quality_overall_score', {}).get('mean', 0)\n",
        "        }\n",
        "    \n",
        "    def _analyze_correlations(self, consolidated: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analiza correlaciones entre diferentes métricas\n",
        "        \"\"\"\n",
        "        if 'complete_pipeline' not in consolidated:\n",
        "            return {}\n",
        "        \n",
        "        pipeline_data = consolidated['complete_pipeline']\n",
        "        \n",
        "        # Extraer métricas para correlación\n",
        "        retrieval_scores = []\n",
        "        quality_scores = []\n",
        "        coherence_scores = []\n",
        "        relevance_scores = []\n",
        "        \n",
        "        for data in pipeline_data.values():\n",
        "            retrieval_scores.append(data['retrieval_recall_at_5'])\n",
        "            quality_scores.append(data['quality_overall_score'])\n",
        "            coherence_scores.append(data['quality_coherence'])\n",
        "            relevance_scores.append(data['quality_relevance'])\n",
        "        \n",
        "        # Calcular correlaciones\n",
        "        correlations = {}\n",
        "        if len(retrieval_scores) > 1:\n",
        "            correlations['retrieval_quality'] = np.corrcoef(retrieval_scores, quality_scores)[0, 1]\n",
        "            correlations['retrieval_coherence'] = np.corrcoef(retrieval_scores, coherence_scores)[0, 1]\n",
        "            correlations['retrieval_relevance'] = np.corrcoef(retrieval_scores, relevance_scores)[0, 1]\n",
        "            correlations['coherence_relevance'] = np.corrcoef(coherence_scores, relevance_scores)[0, 1]\n",
        "        \n",
        "        return correlations\n",
        "    \n",
        "    def _generate_recommendations(self, consolidated: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Genera recomendaciones basadas en el análisis\n",
        "        \"\"\"\n",
        "        recommendations = {\n",
        "            'best_overall_config': None,\n",
        "            'best_retrieval_config': None,\n",
        "            'best_quality_config': None,\n",
        "            'reranking_recommendation': None,\n",
        "            'improvement_suggestions': []\n",
        "        }\n",
        "        \n",
        "        # Mejor configuración general\n",
        "        if 'global_rankings' in consolidated and 'best_complete_pipeline' in consolidated['global_rankings']:\n",
        "            best_config = consolidated['global_rankings']['best_complete_pipeline'][0]\n",
        "            recommendations['best_overall_config'] = best_config\n",
        "        \n",
        "        # Mejor configuración de retrieval\n",
        "        if 'global_rankings' in consolidated and 'best_retrieval' in consolidated['global_rankings']:\n",
        "            best_retrieval = consolidated['global_rankings']['best_retrieval'][0]\n",
        "            recommendations['best_retrieval_config'] = best_retrieval\n",
        "        \n",
        "        # Mejor configuración de calidad\n",
        "        if 'global_rankings' in consolidated and 'best_quality' in consolidated['global_rankings']:\n",
        "            best_quality = consolidated['global_rankings']['best_quality'][0]\n",
        "            recommendations['best_quality_config'] = best_quality\n",
        "        \n",
        "        # Recomendación de re-ranking\n",
        "        if 'reranking_insights' in consolidated['insights']:\n",
        "            reranking_insights = consolidated['insights']['reranking_insights']\n",
        "            if reranking_insights.get('overall_effectiveness', 0) > 0.5:\n",
        "                recommendations['reranking_recommendation'] = f\"Usar re-ranking con {reranking_insights['best_reranking_model']}\"\n",
        "            else:\n",
        "                recommendations['reranking_recommendation'] = \"Re-ranking no muestra mejoras significativas\"\n",
        "        \n",
        "        # Sugerencias de mejora\n",
        "        suggestions = []\n",
        "        \n",
        "        # Analizar correlaciones\n",
        "        if 'correlation_insights' in consolidated['insights']:\n",
        "            correlations = consolidated['insights']['correlation_insights']\n",
        "            if correlations.get('retrieval_quality', 0) < 0.3:\n",
        "                suggestions.append(\"Mejorar la calidad de retrieval para mejorar respuestas\")\n",
        "            if correlations.get('coherence_relevance', 0) < 0.5:\n",
        "                suggestions.append(\"Trabajar en la coherencia y relevancia de las respuestas\")\n",
        "        \n",
        "        recommendations['improvement_suggestions'] = suggestions\n",
        "        \n",
        "        return recommendations\n",
        "\n",
        "# Inicializar analizador global\n",
        "global_analyzer = GlobalComparativeAnalyzer(\n",
        "    retrieval_results=retrieval_results,\n",
        "    reranking_results=reranking_results,\n",
        "    complete_rag_results=complete_rag_results\n",
        ")\n",
        "\n",
        "print(\"✅ Analizador comparativo global inicializado\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejecutar análisis comparativo global\n",
        "print(\"🔄 Iniciando análisis comparativo global...\")\n",
        "global_analysis = global_analyzer.consolidate_all_results()\n",
        "\n",
        "# Visualización del análisis global\n",
        "def visualize_global_analysis(global_analysis: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Visualiza el análisis comparativo global\n",
        "    \"\"\"\n",
        "    if not global_analysis:\n",
        "        print(\"❌ No hay datos para visualizar\")\n",
        "        return\n",
        "    \n",
        "    # Crear figura con múltiples subplots\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "    fig.suptitle('Análisis Comparativo Global - Pipeline RAG', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Ranking de modelos de embedding (retrieval)\n",
        "    if 'retrieval_only' in global_analysis and global_analysis['retrieval_only']:\n",
        "        retrieval_data = global_analysis['retrieval_only']\n",
        "        models = list(retrieval_data.keys())\n",
        "        recall_scores = [retrieval_data[model]['recall_at_5'] for model in models]\n",
        "        \n",
        "        axes[0, 0].bar(models, recall_scores, color='skyblue', alpha=0.7)\n",
        "        axes[0, 0].set_title('Ranking de Embeddings (Recall@5)')\n",
        "        axes[0, 0].set_ylabel('Recall@5')\n",
        "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        # Agregar valores en las barras\n",
        "        for i, v in enumerate(recall_scores):\n",
        "            axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    # 2. Mejoras de re-ranking\n",
        "    if 'reranking_improvements' in global_analysis and global_analysis['reranking_improvements']:\n",
        "        reranking_data = global_analysis['reranking_improvements']\n",
        "        combinations = list(reranking_data.keys())\n",
        "        improvements = [reranking_data[combo]['ndcg_improvement'] for combo in combinations]\n",
        "        \n",
        "        axes[0, 1].bar(range(len(combinations)), improvements, color='lightgreen', alpha=0.7)\n",
        "        axes[0, 1].set_title('Mejoras de Re-ranking (nDCG@5)')\n",
        "        axes[0, 1].set_ylabel('Mejora nDCG@5')\n",
        "        axes[0, 1].set_xticks(range(len(combinations)))\n",
        "        axes[0, 1].set_xticklabels(combinations, rotation=45, ha='right')\n",
        "        axes[0, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "        \n",
        "        # Agregar valores en las barras\n",
        "        for i, v in enumerate(improvements):\n",
        "            axes[0, 1].text(i, v + (0.001 if v >= 0 else -0.003), f'{v:+.3f}', \n",
        "                           ha='center', va='bottom' if v >= 0 else 'top', fontsize=8)\n",
        "    \n",
        "    # 3. Calidad de respuestas\n",
        "    if 'complete_pipeline' in global_analysis and global_analysis['complete_pipeline']:\n",
        "        pipeline_data = global_analysis['complete_pipeline']\n",
        "        combinations = list(pipeline_data.keys())\n",
        "        quality_scores = [pipeline_data[combo]['quality_overall_score'] for combo in combinations]\n",
        "        \n",
        "        axes[0, 2].bar(range(len(combinations)), quality_scores, color='orange', alpha=0.7)\n",
        "        axes[0, 2].set_title('Calidad de Respuestas (Score General)')\n",
        "        axes[0, 2].set_ylabel('Score (1-5)')\n",
        "        axes[0, 2].set_xticks(range(len(combinations)))\n",
        "        axes[0, 2].set_xticklabels(combinations, rotation=45, ha='right')\n",
        "        axes[0, 2].set_ylim(0, 5)\n",
        "        \n",
        "        # Agregar valores en las barras\n",
        "        for i, v in enumerate(quality_scores):\n",
        "            axes[0, 2].text(i, v + 0.05, f'{v:.1f}', ha='center', va='bottom')\n",
        "    \n",
        "    # 4. Score combinado (retrieval + calidad)\n",
        "    if 'complete_pipeline' in global_analysis and global_analysis['complete_pipeline']:\n",
        "        pipeline_data = global_analysis['complete_pipeline']\n",
        "        combinations = list(pipeline_data.keys())\n",
        "        combined_scores = [global_analyzer._calculate_combined_score(pipeline_data[combo]) for combo in combinations]\n",
        "        \n",
        "        axes[1, 0].bar(range(len(combinations)), combined_scores, color='purple', alpha=0.7)\n",
        "        axes[1, 0].set_title('Score Combinado (40% Retrieval + 60% Calidad)')\n",
        "        axes[1, 0].set_ylabel('Score Combinado')\n",
        "        axes[1, 0].set_xticks(range(len(combinations)))\n",
        "        axes[1, 0].set_xticklabels(combinations, rotation=45, ha='right')\n",
        "        \n",
        "        # Agregar valores en las barras\n",
        "        for i, v in enumerate(combined_scores):\n",
        "            axes[1, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    # 5. Correlación Retrieval vs Calidad\n",
        "    if 'complete_pipeline' in global_analysis and global_analysis['complete_pipeline']:\n",
        "        pipeline_data = global_analysis['complete_pipeline']\n",
        "        retrieval_scores = [pipeline_data[combo]['retrieval_recall_at_5'] for combo in pipeline_data.keys()]\n",
        "        quality_scores = [pipeline_data[combo]['quality_overall_score'] for combo in pipeline_data.keys()]\n",
        "        \n",
        "        axes[1, 1].scatter(retrieval_scores, quality_scores, alpha=0.7, s=100)\n",
        "        axes[1, 1].set_title('Correlación Retrieval vs Calidad')\n",
        "        axes[1, 1].set_xlabel('Recall@5')\n",
        "        axes[1, 1].set_ylabel('Calidad General')\n",
        "        \n",
        "        # Calcular y mostrar correlación\n",
        "        if len(retrieval_scores) > 1:\n",
        "            correlation = np.corrcoef(retrieval_scores, quality_scores)[0, 1]\n",
        "            axes[1, 1].text(0.05, 0.95, f'Correlación: {correlation:.3f}', \n",
        "                           transform=axes[1, 1].transAxes, fontsize=12, \n",
        "                           bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
        "    \n",
        "    # 6. Distribución de métricas de calidad\n",
        "    if 'complete_pipeline' in global_analysis and global_analysis['complete_pipeline']:\n",
        "        pipeline_data = global_analysis['complete_pipeline']\n",
        "        quality_metrics = ['quality_coherence', 'quality_relevance', 'quality_completeness', \n",
        "                          'quality_fidelity', 'quality_conciseness']\n",
        "        metric_names = ['Coherencia', 'Relevancia', 'Completitud', 'Fidelidad', 'Concisión']\n",
        "        \n",
        "        # Calcular promedios por métrica\n",
        "        metric_means = []\n",
        "        for metric in quality_metrics:\n",
        "            values = [pipeline_data[combo][metric] for combo in pipeline_data.keys() if metric in pipeline_data[combo]]\n",
        "            metric_means.append(np.mean(values) if values else 0)\n",
        "        \n",
        "        axes[1, 2].bar(metric_names, metric_means, color='lightcoral', alpha=0.7)\n",
        "        axes[1, 2].set_title('Distribución de Métricas de Calidad')\n",
        "        axes[1, 2].set_ylabel('Score Promedio (1-5)')\n",
        "        axes[1, 2].set_ylim(0, 5)\n",
        "        axes[1, 2].tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        # Agregar valores en las barras\n",
        "        for i, v in enumerate(metric_means):\n",
        "            axes[1, 2].text(i, v + 0.05, f'{v:.1f}', ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Ejecutar visualización\n",
        "visualize_global_analysis(global_analysis)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Análisis detallado y reporte final\n",
        "def generate_comprehensive_report(global_analysis: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Genera un reporte comprensivo del análisis global\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"📊 REPORTE COMPREHENSIVO - ANÁLISIS COMPARATIVO GLOBAL\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # 1. Rankings Globales\n",
        "    print(\"\\n🏆 RANKINGS GLOBALES\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    if 'global_rankings' in global_analysis:\n",
        "        rankings = global_analysis['global_rankings']\n",
        "        \n",
        "        # Mejor retrieval\n",
        "        if 'best_retrieval' in rankings and rankings['best_retrieval']:\n",
        "            print(f\"🥇 Mejor Modelo de Embedding (Retrieval): {rankings['best_retrieval'][0]}\")\n",
        "            if len(rankings['best_retrieval']) > 1:\n",
        "                print(f\"   Top 3: {', '.join(rankings['best_retrieval'][:3])}\")\n",
        "        \n",
        "        # Mejor re-ranking\n",
        "        if 'best_reranking_improvement' in rankings and rankings['best_reranking_improvement']:\n",
        "            print(f\"🥇 Mejor Mejora de Re-ranking: {rankings['best_reranking_improvement'][0]}\")\n",
        "            if len(rankings['best_reranking_improvement']) > 1:\n",
        "                print(f\"   Top 3: {', '.join(rankings['best_reranking_improvement'][:3])}\")\n",
        "        \n",
        "        # Mejor pipeline completo\n",
        "        if 'best_complete_pipeline' in rankings and rankings['best_complete_pipeline']:\n",
        "            print(f\"🥇 Mejor Pipeline Completo: {rankings['best_complete_pipeline'][0]}\")\n",
        "            if len(rankings['best_complete_pipeline']) > 1:\n",
        "                print(f\"   Top 3: {', '.join(rankings['best_complete_pipeline'][:3])}\")\n",
        "        \n",
        "        # Mejor calidad\n",
        "        if 'best_quality' in rankings and rankings['best_quality']:\n",
        "            print(f\"🥇 Mejor Calidad de Respuesta: {rankings['best_quality'][0]}\")\n",
        "            if len(rankings['best_quality']) > 1:\n",
        "                print(f\"   Top 3: {', '.join(rankings['best_quality'][:3])}\")\n",
        "    \n",
        "    # 2. Insights por Componente\n",
        "    print(\"\\n🔍 INSIGHTS POR COMPONENTE\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    if 'insights' in global_analysis:\n",
        "        insights = global_analysis['insights']\n",
        "        \n",
        "        # Insights de retrieval\n",
        "        if 'retrieval_insights' in insights and insights['retrieval_insights']:\n",
        "            retrieval_insights = insights['retrieval_insights']\n",
        "            print(f\"\\n📈 Retrieval:\")\n",
        "            print(f\"   - Mejor modelo: {retrieval_insights.get('best_model', 'N/A')}\")\n",
        "            print(f\"   - Peor modelo: {retrieval_insights.get('worst_model', 'N/A')}\")\n",
        "            print(f\"   - Rango Recall@5: {retrieval_insights.get('recall_range', (0, 0))}\")\n",
        "            print(f\"   - Desviación estándar Recall: {retrieval_insights.get('recall_std', 0):.3f}\")\n",
        "        \n",
        "        # Insights de re-ranking\n",
        "        if 'reranking_insights' in insights and insights['reranking_insights']:\n",
        "            reranking_insights = insights['reranking_insights']\n",
        "            print(f\"\\n🔄 Re-ranking:\")\n",
        "            print(f\"   - Mejor modelo: {reranking_insights.get('best_reranking_model', 'N/A')}\")\n",
        "            print(f\"   - Efectividad general: {reranking_insights.get('overall_effectiveness', 0):.1%}\")\n",
        "            \n",
        "            if 'model_stats' in reranking_insights:\n",
        "                print(f\"   - Estadísticas por modelo:\")\n",
        "                for model, stats in reranking_insights['model_stats'].items():\n",
        "                    print(f\"     * {model}: {stats['mean_improvement']:+.3f} ± {stats['std_improvement']:.3f}\")\n",
        "        \n",
        "        # Insights de calidad\n",
        "        if 'quality_insights' in insights and insights['quality_insights']:\n",
        "            quality_insights = insights['quality_insights']\n",
        "            print(f\"\\n🎯 Calidad:\")\n",
        "            print(f\"   - Mejor combinación: {quality_insights.get('best_quality_combo', 'N/A')}\")\n",
        "            print(f\"   - Calidad promedio: {quality_insights.get('overall_quality_mean', 0):.1f}/5\")\n",
        "            \n",
        "            if 'quality_stats' in quality_insights:\n",
        "                print(f\"   - Estadísticas por métrica:\")\n",
        "                for metric, stats in quality_insights['quality_stats'].items():\n",
        "                    metric_name = metric.replace('quality_', '').title()\n",
        "                    print(f\"     * {metric_name}: {stats['mean']:.1f} ± {stats['std']:.1f}\")\n",
        "    \n",
        "    # 3. Análisis de Correlaciones\n",
        "    print(\"\\n📊 ANÁLISIS DE CORRELACIONES\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    if 'insights' in global_analysis and 'correlation_insights' in global_analysis['insights']:\n",
        "        correlations = global_analysis['insights']['correlation_insights']\n",
        "        \n",
        "        print(f\"🔗 Correlaciones entre métricas:\")\n",
        "        for correlation_name, value in correlations.items():\n",
        "            correlation_display = correlation_name.replace('_', ' vs ').title()\n",
        "            strength = \"Fuerte\" if abs(value) > 0.7 else \"Moderada\" if abs(value) > 0.3 else \"Débil\"\n",
        "            direction = \"Positiva\" if value > 0 else \"Negativa\"\n",
        "            print(f\"   - {correlation_display}: {value:.3f} ({strength} {direction})\")\n",
        "    \n",
        "    # 4. Recomendaciones\n",
        "    print(\"\\n💡 RECOMENDACIONES\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    if 'insights' in global_analysis and 'recommendations' in global_analysis['insights']:\n",
        "        recommendations = global_analysis['insights']['recommendations']\n",
        "        \n",
        "        print(f\"🎯 Configuraciones Recomendadas:\")\n",
        "        if recommendations.get('best_overall_config'):\n",
        "            print(f\"   - Mejor configuración general: {recommendations['best_overall_config']}\")\n",
        "        if recommendations.get('best_retrieval_config'):\n",
        "            print(f\"   - Mejor configuración de retrieval: {recommendations['best_retrieval_config']}\")\n",
        "        if recommendations.get('best_quality_config'):\n",
        "            print(f\"   - Mejor configuración de calidad: {recommendations['best_quality_config']}\")\n",
        "        \n",
        "        print(f\"\\n🔄 Re-ranking:\")\n",
        "        if recommendations.get('reranking_recommendation'):\n",
        "            print(f\"   - {recommendations['reranking_recommendation']}\")\n",
        "        \n",
        "        print(f\"\\n🚀 Sugerencias de Mejora:\")\n",
        "        if recommendations.get('improvement_suggestions'):\n",
        "            for i, suggestion in enumerate(recommendations['improvement_suggestions'], 1):\n",
        "                print(f\"   {i}. {suggestion}\")\n",
        "        else:\n",
        "            print(\"   - No se identificaron mejoras específicas necesarias\")\n",
        "    \n",
        "    # 5. Resumen Ejecutivo\n",
        "    print(\"\\n📋 RESUMEN EJECUTIVO\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Contar total de combinaciones evaluadas\n",
        "    total_combinations = 0\n",
        "    if 'complete_pipeline' in global_analysis:\n",
        "        total_combinations = len(global_analysis['complete_pipeline'])\n",
        "    \n",
        "    print(f\"📊 Evaluación completada:\")\n",
        "    print(f\"   - Total de combinaciones evaluadas: {total_combinations}\")\n",
        "    print(f\"   - Modelos de embedding: {len(EMBEDDING_MODELS)}\")\n",
        "    print(f\"   - Modelos de re-ranking: {len(RERANKING_MODELS)}\")\n",
        "    print(f\"   - Consultas evaluadas: {len(dataset)}\")\n",
        "    \n",
        "    # Mejor configuración general\n",
        "    if 'global_rankings' in global_analysis and 'best_complete_pipeline' in global_analysis['global_rankings']:\n",
        "        best_config = global_analysis['global_rankings']['best_complete_pipeline'][0]\n",
        "        print(f\"\\n🏆 Recomendación Final:\")\n",
        "        print(f\"   - Usar configuración: {best_config}\")\n",
        "        print(f\"   - Esta configuración ofrece el mejor balance entre retrieval y calidad de respuesta\")\n",
        "    \n",
        "    print(f\"\\n✅ Análisis comparativo global completado exitosamente\")\n",
        "\n",
        "# Ejecutar reporte comprensivo\n",
        "generate_comprehensive_report(global_analysis)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar resultados del análisis global\n",
        "def save_global_analysis_results(global_analysis: Dict[str, Any], \n",
        "                                evaluation_name: str = \"global_comparative_analysis\") -> str:\n",
        "    \"\"\"\n",
        "    Guarda los resultados del análisis comparativo global\n",
        "    \n",
        "    Args:\n",
        "        global_analysis: Resultados del análisis global\n",
        "        evaluation_name: Nombre de la evaluación\n",
        "    \n",
        "    Returns:\n",
        "        Ruta del archivo guardado\n",
        "    \"\"\"\n",
        "    # Preparar configuración\n",
        "    config = {\n",
        "        'embedding_models': list(EMBEDDING_MODELS.keys()),\n",
        "        'reranking_models': list(RERANKING_MODELS.keys()),\n",
        "        'retrieval_metrics': RETRIEVAL_METRICS,\n",
        "        'llm_evaluation_criteria': LLM_EVALUATION_CRITERIA,\n",
        "        'qdrant_config': QDRANT_CONFIG,\n",
        "        'llm_config': LLM_CONFIG,\n",
        "        'dataset_size': len(dataset)\n",
        "    }\n",
        "    \n",
        "    # Preparar metadatos\n",
        "    metadata = {\n",
        "        'evaluation_type': 'global_comparative_analysis',\n",
        "        'description': 'Análisis comparativo global de todas las evaluaciones del pipeline RAG',\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'total_queries': len(dataset),\n",
        "        'total_combinations': len(global_analysis.get('complete_pipeline', {})),\n",
        "        'analysis_components': ['retrieval', 'reranking', 'complete_pipeline', 'rankings', 'insights']\n",
        "    }\n",
        "    \n",
        "    # Guardar análisis\n",
        "    filepath = eval_manager.save_evaluation(\n",
        "        evaluation_name=evaluation_name,\n",
        "        config=config,\n",
        "        results={'global_analysis': global_analysis},\n",
        "        metadata=metadata\n",
        "    )\n",
        "    \n",
        "    print(f\"💾 Resultados del análisis global guardados en: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "# Guardar resultados del análisis global\n",
        "global_analysis_filepath = save_global_analysis_results(global_analysis)\n",
        "\n",
        "# Resumen final del análisis comparativo global\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"📋 RESUMEN FINAL - ANÁLISIS COMPARATIVO GLOBAL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if global_analysis:\n",
        "    # Estadísticas generales\n",
        "    print(f\"\\n📊 Estadísticas Generales:\")\n",
        "    print(f\"   - Evaluaciones realizadas: 4 (Retrieval, Re-ranking, Pipeline Completo, Análisis Global)\")\n",
        "    print(f\"   - Modelos de embedding evaluados: {len(EMBEDDING_MODELS)}\")\n",
        "    print(f\"   - Modelos de re-ranking evaluados: {len(RERANKING_MODELS)}\")\n",
        "    print(f\"   - Consultas evaluadas: {len(dataset)}\")\n",
        "    print(f\"   - Combinaciones totales: {len(global_analysis.get('complete_pipeline', {}))}\")\n",
        "    \n",
        "    # Mejores configuraciones identificadas\n",
        "    if 'global_rankings' in global_analysis:\n",
        "        rankings = global_analysis['global_rankings']\n",
        "        \n",
        "        print(f\"\\n🏆 Mejores Configuraciones Identificadas:\")\n",
        "        \n",
        "        if 'best_retrieval' in rankings and rankings['best_retrieval']:\n",
        "            print(f\"   - Mejor Embedding: {rankings['best_retrieval'][0]}\")\n",
        "        \n",
        "        if 'best_reranking_improvement' in rankings and rankings['best_reranking_improvement']:\n",
        "            print(f\"   - Mejor Re-ranking: {rankings['best_reranking_improvement'][0]}\")\n",
        "        \n",
        "        if 'best_complete_pipeline' in rankings and rankings['best_complete_pipeline']:\n",
        "            print(f\"   - Mejor Pipeline Completo: {rankings['best_complete_pipeline'][0]}\")\n",
        "        \n",
        "        if 'best_quality' in rankings and rankings['best_quality']:\n",
        "            print(f\"   - Mejor Calidad: {rankings['best_quality'][0]}\")\n",
        "    \n",
        "    # Insights clave\n",
        "    if 'insights' in global_analysis:\n",
        "        insights = global_analysis['insights']\n",
        "        \n",
        "        print(f\"\\n🔍 Insights Clave:\")\n",
        "        \n",
        "        # Insight de retrieval\n",
        "        if 'retrieval_insights' in insights and insights['retrieval_insights']:\n",
        "            retrieval_insights = insights['retrieval_insights']\n",
        "            best_model = retrieval_insights.get('best_model', 'N/A')\n",
        "            recall_range = retrieval_insights.get('recall_range', (0, 0))\n",
        "            print(f\"   - Mejor modelo de embedding: {best_model} (Recall@5: {recall_range[1]:.3f})\")\n",
        "        \n",
        "        # Insight de re-ranking\n",
        "        if 'reranking_insights' in insights and insights['reranking_insights']:\n",
        "            reranking_insights = insights['reranking_insights']\n",
        "            effectiveness = reranking_insights.get('overall_effectiveness', 0)\n",
        "            best_reranking = reranking_insights.get('best_reranking_model', 'N/A')\n",
        "            print(f\"   - Efectividad del re-ranking: {effectiveness:.1%} (Mejor: {best_reranking})\")\n",
        "        \n",
        "        # Insight de calidad\n",
        "        if 'quality_insights' in insights and insights['quality_insights']:\n",
        "            quality_insights = insights['quality_insights']\n",
        "            overall_quality = quality_insights.get('overall_quality_mean', 0)\n",
        "            best_quality_combo = quality_insights.get('best_quality_combo', 'N/A')\n",
        "            print(f\"   - Calidad promedio: {overall_quality:.1f}/5 (Mejor: {best_quality_combo})\")\n",
        "        \n",
        "        # Insight de correlaciones\n",
        "        if 'correlation_insights' in insights and insights['correlation_insights']:\n",
        "            correlations = insights['correlation_insights']\n",
        "            retrieval_quality_corr = correlations.get('retrieval_quality', 0)\n",
        "            print(f\"   - Correlación Retrieval-Calidad: {retrieval_quality_corr:.3f}\")\n",
        "    \n",
        "    # Recomendaciones finales\n",
        "    if 'insights' in global_analysis and 'recommendations' in global_analysis['insights']:\n",
        "        recommendations = global_analysis['insights']['recommendations']\n",
        "        \n",
        "        print(f\"\\n💡 Recomendaciones Finales:\")\n",
        "        \n",
        "        if recommendations.get('best_overall_config'):\n",
        "            print(f\"   - Configuración recomendada: {recommendations['best_overall_config']}\")\n",
        "        \n",
        "        if recommendations.get('reranking_recommendation'):\n",
        "            print(f\"   - Re-ranking: {recommendations['reranking_recommendation']}\")\n",
        "        \n",
        "        if recommendations.get('improvement_suggestions'):\n",
        "            print(f\"   - Mejoras sugeridas: {len(recommendations['improvement_suggestions'])} identificadas\")\n",
        "    \n",
        "    print(f\"\\n✅ Análisis comparativo global completado exitosamente\")\n",
        "    print(f\"📁 Resultados guardados en: {global_analysis_filepath}\")\n",
        "    \n",
        "    # Próximos pasos sugeridos\n",
        "    print(f\"\\n🚀 Próximos Pasos Sugeridos:\")\n",
        "    print(f\"   1. Implementar la configuración recomendada en producción\")\n",
        "    print(f\"   2. Realizar pruebas A/B con las mejores combinaciones\")\n",
        "    print(f\"   3. Monitorear el rendimiento en tiempo real\")\n",
        "    print(f\"   4. Iterar y mejorar basándose en feedback de usuarios\")\n",
        "    print(f\"   5. Expandir el dataset de evaluación con más consultas\")\n",
        "    \n",
        "else:\n",
        "    print(\"❌ No se pudieron obtener resultados del análisis global\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. Conclusiones\n",
        "\n",
        "## Resumen Ejecutivo\n",
        "\n",
        "Este notebook presenta una evaluación comprehensiva del pipeline RAG para búsqueda semántica en derecho laboral paraguayo. A través de 8 secciones estructuradas, se evaluaron múltiples modelos de embedding, re-ranking y generación de respuestas, proporcionando insights valiosos para la optimización del sistema.\n",
        "\n",
        "### 🎯 Objetivos Cumplidos\n",
        "\n",
        "1. **Evaluación Sistemática**: Se implementó un framework robusto para evaluar componentes individuales y el pipeline completo\n",
        "2. **Análisis Comparativo**: Se compararon múltiples configuraciones para identificar las mejores combinaciones\n",
        "3. **Métricas Objetivas y Subjetivas**: Se combinaron métricas de retrieval con evaluación de calidad usando LLM-as-a-judge\n",
        "4. **Recomendaciones Accionables**: Se generaron recomendaciones específicas basadas en evidencia empírica\n",
        "\n",
        "### 📊 Alcance de la Evaluación\n",
        "\n",
        "- **Modelos de Embedding**: 3 modelos evaluados (sentence-transformers, multilingual, especializados)\n",
        "- **Modelos de Re-ranking**: 3 modelos cross-encoder evaluados\n",
        "- **Pipeline Completo**: Evaluación end-to-end con generación y evaluación de respuestas\n",
        "- **Métricas**: 15+ métricas diferentes (retrieval, calidad, correlaciones)\n",
        "- **Combinaciones**: Todas las combinaciones posibles evaluadas sistemáticamente\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hallazgos Clave\n",
        "\n",
        "### 🏆 Mejores Configuraciones Identificadas\n",
        "\n",
        "#### 1. Modelos de Embedding\n",
        "- **Mejor Rendimiento**: [Se actualizará con resultados reales]\n",
        "- **Características**: [Se actualizará con análisis de patrones]\n",
        "- **Recomendación**: [Se actualizará con insights específicos]\n",
        "\n",
        "#### 2. Modelos de Re-ranking\n",
        "- **Efectividad General**: [Se actualizará con porcentaje de mejoras]\n",
        "- **Mejor Modelo**: [Se actualizará con modelo recomendado]\n",
        "- **Impacto en Calidad**: [Se actualizará con análisis de correlaciones]\n",
        "\n",
        "#### 3. Pipeline Completo\n",
        "- **Configuración Óptima**: [Se actualizará con mejor combinación]\n",
        "- **Score Combinado**: [Se actualizará con métrica final]\n",
        "- **Balance Retrieval-Calidad**: [Se actualizará con análisis de trade-offs]\n",
        "\n",
        "### 📈 Insights de Rendimiento\n",
        "\n",
        "#### Correlaciones Identificadas\n",
        "- **Retrieval vs Calidad**: [Se actualizará con coeficiente de correlación]\n",
        "- **Coherencia vs Relevancia**: [Se actualizará con análisis de relación]\n",
        "- **Re-ranking vs Mejoras**: [Se actualizará con efectividad medida]\n",
        "\n",
        "#### Patrones de Comportamiento\n",
        "- **Consistencia**: [Se actualizará con análisis de estabilidad]\n",
        "- **Escalabilidad**: [Se actualizará con rendimiento por consulta]\n",
        "- **Robustez**: [Se actualizará con análisis de errores]\n",
        "\n",
        "### 🎯 Métricas de Éxito\n",
        "\n",
        "#### Retrieval\n",
        "- **Recall@5 Promedio**: [Se actualizará con valor]\n",
        "- **Precision@5 Promedio**: [Se actualizará con valor]\n",
        "- **nDCG@5 Promedio**: [Se actualizará con valor]\n",
        "- **MRR Promedio**: [Se actualizará con valor]\n",
        "\n",
        "#### Calidad de Respuestas\n",
        "- **Score General Promedio**: [Se actualizará con valor]/5\n",
        "- **Coherencia Promedio**: [Se actualizará con valor]/5\n",
        "- **Relevancia Promedio**: [Se actualizará con valor]/5\n",
        "- **Completitud Promedio**: [Se actualizará con valor]/5\n",
        "- **Fidelidad Promedio**: [Se actualizará con valor]/5\n",
        "- **Concisión Promedio**: [Se actualizará con valor]/5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generar conclusiones dinámicas basadas en los resultados\n",
        "def generate_dynamic_conclusions(global_analysis: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Genera conclusiones dinámicas basadas en los resultados reales\n",
        "    \"\"\"\n",
        "    conclusions = {\n",
        "        'best_configurations': {},\n",
        "        'performance_insights': {},\n",
        "        'success_metrics': {},\n",
        "        'recommendations': {},\n",
        "        'limitations': {},\n",
        "        'future_work': {}\n",
        "    }\n",
        "    \n",
        "    if not global_analysis:\n",
        "        return conclusions\n",
        "    \n",
        "    # 1. Mejores configuraciones\n",
        "    if 'global_rankings' in global_analysis:\n",
        "        rankings = global_analysis['global_rankings']\n",
        "        \n",
        "        conclusions['best_configurations'] = {\n",
        "            'best_embedding': rankings.get('best_retrieval', ['N/A'])[0],\n",
        "            'best_reranking': rankings.get('best_reranking_improvement', ['N/A'])[0],\n",
        "            'best_complete_pipeline': rankings.get('best_complete_pipeline', ['N/A'])[0],\n",
        "            'best_quality': rankings.get('best_quality', ['N/A'])[0]\n",
        "        }\n",
        "    \n",
        "    # 2. Insights de rendimiento\n",
        "    if 'insights' in global_analysis:\n",
        "        insights = global_analysis['insights']\n",
        "        \n",
        "        # Insights de retrieval\n",
        "        if 'retrieval_insights' in insights and insights['retrieval_insights']:\n",
        "            retrieval_insights = insights['retrieval_insights']\n",
        "            conclusions['performance_insights']['retrieval'] = {\n",
        "                'best_model': retrieval_insights.get('best_model', 'N/A'),\n",
        "                'recall_range': retrieval_insights.get('recall_range', (0, 0)),\n",
        "                'recall_std': retrieval_insights.get('recall_std', 0)\n",
        "            }\n",
        "        \n",
        "        # Insights de re-ranking\n",
        "        if 'reranking_insights' in insights and insights['reranking_insights']:\n",
        "            reranking_insights = insights['reranking_insights']\n",
        "            conclusions['performance_insights']['reranking'] = {\n",
        "                'best_model': reranking_insights.get('best_reranking_model', 'N/A'),\n",
        "                'effectiveness': reranking_insights.get('overall_effectiveness', 0)\n",
        "            }\n",
        "        \n",
        "        # Insights de calidad\n",
        "        if 'quality_insights' in insights and insights['quality_insights']:\n",
        "            quality_insights = insights['quality_insights']\n",
        "            conclusions['performance_insights']['quality'] = {\n",
        "                'best_combo': quality_insights.get('best_quality_combo', 'N/A'),\n",
        "                'overall_mean': quality_insights.get('overall_quality_mean', 0)\n",
        "            }\n",
        "        \n",
        "        # Correlaciones\n",
        "        if 'correlation_insights' in insights and insights['correlation_insights']:\n",
        "            correlations = insights['correlation_insights']\n",
        "            conclusions['performance_insights']['correlations'] = correlations\n",
        "    \n",
        "    # 3. Métricas de éxito\n",
        "    if 'complete_pipeline' in global_analysis:\n",
        "        pipeline_data = global_analysis['complete_pipeline']\n",
        "        \n",
        "        # Calcular promedios de métricas\n",
        "        retrieval_metrics = ['retrieval_recall_at_5', 'retrieval_precision_at_5', \n",
        "                           'retrieval_ndcg_at_5', 'retrieval_mrr']\n",
        "        quality_metrics = ['quality_coherence', 'quality_relevance', 'quality_completeness', \n",
        "                          'quality_fidelity', 'quality_conciseness', 'quality_overall_score']\n",
        "        \n",
        "        conclusions['success_metrics'] = {\n",
        "            'retrieval': {},\n",
        "            'quality': {}\n",
        "        }\n",
        "        \n",
        "        # Métricas de retrieval\n",
        "        for metric in retrieval_metrics:\n",
        "            values = [data[metric] for data in pipeline_data.values() if metric in data]\n",
        "            if values:\n",
        "                conclusions['success_metrics']['retrieval'][metric] = {\n",
        "                    'mean': np.mean(values),\n",
        "                    'std': np.std(values)\n",
        "                }\n",
        "        \n",
        "        # Métricas de calidad\n",
        "        for metric in quality_metrics:\n",
        "            values = [data[metric] for data in pipeline_data.values() if metric in data]\n",
        "            if values:\n",
        "                conclusions['success_metrics']['quality'][metric] = {\n",
        "                    'mean': np.mean(values),\n",
        "                    'std': np.std(values)\n",
        "                }\n",
        "    \n",
        "    # 4. Recomendaciones\n",
        "    if 'insights' in global_analysis and 'recommendations' in global_analysis['insights']:\n",
        "        recommendations = global_analysis['insights']['recommendations']\n",
        "        conclusions['recommendations'] = {\n",
        "            'best_overall_config': recommendations.get('best_overall_config', 'N/A'),\n",
        "            'reranking_recommendation': recommendations.get('reranking_recommendation', 'N/A'),\n",
        "            'improvement_suggestions': recommendations.get('improvement_suggestions', [])\n",
        "        }\n",
        "    \n",
        "    # 5. Limitaciones identificadas\n",
        "    conclusions['limitations'] = {\n",
        "        'dataset_size': f\"Dataset limitado a {len(dataset)} consultas\",\n",
        "        'model_coverage': f\"Solo {len(EMBEDDING_MODELS)} modelos de embedding evaluados\",\n",
        "        'domain_specificity': \"Evaluación específica para derecho laboral paraguayo\",\n",
        "        'llm_dependency': \"Dependencia de LLM para evaluación de calidad\",\n",
        "        'computational_cost': \"Evaluación computacionalmente intensiva\"\n",
        "    }\n",
        "    \n",
        "    # 6. Trabajo futuro\n",
        "    conclusions['future_work'] = {\n",
        "        'dataset_expansion': \"Expandir dataset con más consultas y casos edge\",\n",
        "        'model_diversification': \"Evaluar más modelos de embedding y re-ranking\",\n",
        "        'domain_generalization': \"Probar en otros dominios legales\",\n",
        "        'efficiency_optimization': \"Optimizar para menor costo computacional\",\n",
        "        'real_world_validation': \"Validar en entorno de producción real\"\n",
        "    }\n",
        "    \n",
        "    return conclusions\n",
        "\n",
        "# Generar conclusiones dinámicas\n",
        "dynamic_conclusions = generate_dynamic_conclusions(global_analysis)\n",
        "\n",
        "print(\"✅ Conclusiones dinámicas generadas basadas en resultados reales\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mostrar conclusiones dinámicas\n",
        "def display_dynamic_conclusions(conclusions: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Muestra las conclusiones dinámicas de forma estructurada\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"📊 CONCLUSIONES DINÁMICAS - BASADAS EN RESULTADOS REALES\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # 1. Mejores configuraciones\n",
        "    if 'best_configurations' in conclusions and conclusions['best_configurations']:\n",
        "        print(\"\\n🏆 MEJORES CONFIGURACIONES IDENTIFICADAS\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        configs = conclusions['best_configurations']\n",
        "        print(f\"🥇 Mejor Modelo de Embedding: {configs.get('best_embedding', 'N/A')}\")\n",
        "        print(f\"🥇 Mejor Modelo de Re-ranking: {configs.get('best_reranking', 'N/A')}\")\n",
        "        print(f\"🥇 Mejor Pipeline Completo: {configs.get('best_complete_pipeline', 'N/A')}\")\n",
        "        print(f\"🥇 Mejor Calidad de Respuesta: {configs.get('best_quality', 'N/A')}\")\n",
        "    \n",
        "    # 2. Insights de rendimiento\n",
        "    if 'performance_insights' in conclusions and conclusions['performance_insights']:\n",
        "        print(\"\\n📈 INSIGHTS DE RENDIMIENTO\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        insights = conclusions['performance_insights']\n",
        "        \n",
        "        # Retrieval insights\n",
        "        if 'retrieval' in insights:\n",
        "            retrieval = insights['retrieval']\n",
        "            print(f\"\\n📊 Retrieval:\")\n",
        "            print(f\"   - Mejor modelo: {retrieval.get('best_model', 'N/A')}\")\n",
        "            print(f\"   - Rango Recall@5: {retrieval.get('recall_range', (0, 0))}\")\n",
        "            print(f\"   - Desviación estándar: {retrieval.get('recall_std', 0):.3f}\")\n",
        "        \n",
        "        # Re-ranking insights\n",
        "        if 'reranking' in insights:\n",
        "            reranking = insights['reranking']\n",
        "            print(f\"\\n🔄 Re-ranking:\")\n",
        "            print(f\"   - Mejor modelo: {reranking.get('best_model', 'N/A')}\")\n",
        "            print(f\"   - Efectividad: {reranking.get('effectiveness', 0):.1%}\")\n",
        "        \n",
        "        # Quality insights\n",
        "        if 'quality' in insights:\n",
        "            quality = insights['quality']\n",
        "            print(f\"\\n🎯 Calidad:\")\n",
        "            print(f\"   - Mejor combinación: {quality.get('best_combo', 'N/A')}\")\n",
        "            print(f\"   - Calidad promedio: {quality.get('overall_mean', 0):.1f}/5\")\n",
        "        \n",
        "        # Correlaciones\n",
        "        if 'correlations' in insights:\n",
        "            correlations = insights['correlations']\n",
        "            print(f\"\\n🔗 Correlaciones:\")\n",
        "            for corr_name, value in correlations.items():\n",
        "                corr_display = corr_name.replace('_', ' vs ').title()\n",
        "                strength = \"Fuerte\" if abs(value) > 0.7 else \"Moderada\" if abs(value) > 0.3 else \"Débil\"\n",
        "                direction = \"Positiva\" if value > 0 else \"Negativa\"\n",
        "                print(f\"   - {corr_display}: {value:.3f} ({strength} {direction})\")\n",
        "    \n",
        "    # 3. Métricas de éxito\n",
        "    if 'success_metrics' in conclusions and conclusions['success_metrics']:\n",
        "        print(\"\\n🎯 MÉTRICAS DE ÉXITO\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        metrics = conclusions['success_metrics']\n",
        "        \n",
        "        # Métricas de retrieval\n",
        "        if 'retrieval' in metrics and metrics['retrieval']:\n",
        "            print(f\"\\n📊 Retrieval:\")\n",
        "            for metric, stats in metrics['retrieval'].items():\n",
        "                metric_name = metric.replace('retrieval_', '').replace('_at_5', '@5').title()\n",
        "                print(f\"   - {metric_name}: {stats['mean']:.3f} ± {stats['std']:.3f}\")\n",
        "        \n",
        "        # Métricas de calidad\n",
        "        if 'quality' in metrics and metrics['quality']:\n",
        "            print(f\"\\n🎯 Calidad:\")\n",
        "            for metric, stats in metrics['quality'].items():\n",
        "                metric_name = metric.replace('quality_', '').title()\n",
        "                print(f\"   - {metric_name}: {stats['mean']:.1f} ± {stats['std']:.1f}\")\n",
        "    \n",
        "    # 4. Recomendaciones\n",
        "    if 'recommendations' in conclusions and conclusions['recommendations']:\n",
        "        print(\"\\n💡 RECOMENDACIONES\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        recs = conclusions['recommendations']\n",
        "        print(f\"🎯 Configuración Recomendada: {recs.get('best_overall_config', 'N/A')}\")\n",
        "        print(f\"🔄 Re-ranking: {recs.get('reranking_recommendation', 'N/A')}\")\n",
        "        \n",
        "        if recs.get('improvement_suggestions'):\n",
        "            print(f\"\\n🚀 Sugerencias de Mejora:\")\n",
        "            for i, suggestion in enumerate(recs['improvement_suggestions'], 1):\n",
        "                print(f\"   {i}. {suggestion}\")\n",
        "    \n",
        "    # 5. Limitaciones\n",
        "    if 'limitations' in conclusions and conclusions['limitations']:\n",
        "        print(\"\\n⚠️ LIMITACIONES IDENTIFICADAS\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        limitations = conclusions['limitations']\n",
        "        for limitation, description in limitations.items():\n",
        "            limitation_name = limitation.replace('_', ' ').title()\n",
        "            print(f\"   - {limitation_name}: {description}\")\n",
        "    \n",
        "    # 6. Trabajo futuro\n",
        "    if 'future_work' in conclusions and conclusions['future_work']:\n",
        "        print(\"\\n🚀 TRABAJO FUTURO SUGERIDO\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        future_work = conclusions['future_work']\n",
        "        for area, description in future_work.items():\n",
        "            area_name = area.replace('_', ' ').title()\n",
        "            print(f\"   - {area_name}: {description}\")\n",
        "\n",
        "# Mostrar conclusiones dinámicas\n",
        "display_dynamic_conclusions(dynamic_conclusions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recomendaciones Finales\n",
        "\n",
        "### 🎯 Configuración Recomendada para Producción\n",
        "\n",
        "Basándose en los resultados de la evaluación, se recomienda la siguiente configuración:\n",
        "\n",
        "1. **Modelo de Embedding**: [Se actualizará con el mejor modelo identificado]\n",
        "2. **Modelo de Re-ranking**: [Se actualizará con recomendación de re-ranking]\n",
        "3. **Configuración de LLM**: [Se actualizará con configuración óptima]\n",
        "4. **Parámetros de Qdrant**: [Se actualizará con configuración recomendada]\n",
        "\n",
        "### 📊 Métricas de Monitoreo\n",
        "\n",
        "Para el monitoreo en producción, se recomienda seguir estas métricas:\n",
        "\n",
        "#### Métricas Críticas\n",
        "- **Recall@5**: Debe mantenerse por encima del [valor] identificado\n",
        "- **Calidad General**: Debe mantenerse por encima de [valor]/5\n",
        "- **Tiempo de Respuesta**: Debe ser menor a [valor] segundos\n",
        "\n",
        "#### Métricas de Calidad\n",
        "- **Coherencia**: Monitorear degradación en respuestas\n",
        "- **Relevancia**: Verificar que las respuestas sean pertinentes\n",
        "- **Fidelidad**: Asegurar que las respuestas sean fieles al contexto\n",
        "\n",
        "### 🔄 Proceso de Mejora Continua\n",
        "\n",
        "1. **Monitoreo Semanal**: Revisar métricas de rendimiento\n",
        "2. **Evaluación Mensual**: Ejecutar evaluación completa con nuevas consultas\n",
        "3. **Actualización Trimestral**: Re-evaluar modelos y configuraciones\n",
        "4. **Feedback de Usuarios**: Incorporar feedback real para mejorar métricas\n",
        "\n",
        "### 🚀 Próximos Pasos\n",
        "\n",
        "#### Inmediatos (1-2 semanas)\n",
        "- [ ] Implementar configuración recomendada en producción\n",
        "- [ ] Configurar monitoreo de métricas críticas\n",
        "- [ ] Establecer proceso de evaluación continua\n",
        "\n",
        "#### Corto Plazo (1-3 meses)\n",
        "- [ ] Expandir dataset de evaluación con más consultas\n",
        "- [ ] Implementar pruebas A/B con diferentes configuraciones\n",
        "- [ ] Optimizar para menor costo computacional\n",
        "\n",
        "#### Mediano Plazo (3-6 meses)\n",
        "- [ ] Evaluar nuevos modelos de embedding y re-ranking\n",
        "- [ ] Implementar evaluación automática de calidad\n",
        "- [ ] Desarrollar dashboard de monitoreo en tiempo real\n",
        "\n",
        "#### Largo Plazo (6+ meses)\n",
        "- [ ] Extender evaluación a otros dominios legales\n",
        "- [ ] Implementar aprendizaje continuo del sistema\n",
        "- [ ] Desarrollar métricas de satisfacción del usuario\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar conclusiones finales\n",
        "def save_final_conclusions(conclusions: Dict[str, Any], \n",
        "                          evaluation_name: str = \"final_conclusions\") -> str:\n",
        "    \"\"\"\n",
        "    Guarda las conclusiones finales del notebook\n",
        "    \"\"\"\n",
        "    # Preparar configuración\n",
        "    config = {\n",
        "        'embedding_models': list(EMBEDDING_MODELS.keys()),\n",
        "        'reranking_models': list(RERANKING_MODELS.keys()),\n",
        "        'retrieval_metrics': RETRIEVAL_METRICS,\n",
        "        'llm_evaluation_criteria': LLM_EVALUATION_CRITERIA,\n",
        "        'qdrant_config': QDRANT_CONFIG,\n",
        "        'llm_config': LLM_CONFIG,\n",
        "        'dataset_size': len(dataset)\n",
        "    }\n",
        "    \n",
        "    # Preparar metadatos\n",
        "    metadata = {\n",
        "        'evaluation_type': 'final_conclusions',\n",
        "        'description': 'Conclusiones finales del notebook de evaluación RAG',\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'total_queries': len(dataset),\n",
        "        'notebook_sections': 8,\n",
        "        'evaluation_components': ['retrieval', 'reranking', 'complete_pipeline', 'global_analysis', 'conclusions']\n",
        "    }\n",
        "    \n",
        "    # Guardar conclusiones\n",
        "    filepath = eval_manager.save_evaluation(\n",
        "        evaluation_name=evaluation_name,\n",
        "        config=config,\n",
        "        results={'final_conclusions': conclusions},\n",
        "        metadata=metadata\n",
        "    )\n",
        "    \n",
        "    print(f\"💾 Conclusiones finales guardadas en: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "# Guardar conclusiones finales\n",
        "final_conclusions_filepath = save_final_conclusions(dynamic_conclusions)\n",
        "\n",
        "# Resumen final del notebook\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"📋 RESUMEN FINAL DEL NOTEBOOK\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\n✅ NOTEBOOK DE EVALUACIÓN RAG COMPLETADO\")\n",
        "print(f\"📊 Secciones implementadas: 8/8\")\n",
        "print(f\"🔧 Componentes evaluados: Retrieval, Re-ranking, Pipeline Completo, Análisis Global\")\n",
        "print(f\"📈 Métricas implementadas: 15+ métricas diferentes\")\n",
        "print(f\"💾 Archivos generados: {len(eval_manager.list_evaluations())} evaluaciones guardadas\")\n",
        "\n",
        "print(f\"\\n📁 Archivos de resultados:\")\n",
        "print(f\"   - Retrieval: {retrieval_filepath}\")\n",
        "print(f\"   - Re-ranking: {reranking_filepath}\")\n",
        "print(f\"   - Pipeline Completo: {complete_rag_filepath}\")\n",
        "print(f\"   - Análisis Global: {global_analysis_filepath}\")\n",
        "print(f\"   - Conclusiones: {final_conclusions_filepath}\")\n",
        "\n",
        "print(f\"\\n🎯 Próximos pasos recomendados:\")\n",
        "print(f\"   1. Revisar conclusiones dinámicas generadas\")\n",
        "print(f\"   2. Implementar configuración recomendada\")\n",
        "print(f\"   3. Configurar monitoreo de métricas\")\n",
        "print(f\"   4. Establecer proceso de evaluación continua\")\n",
        "\n",
        "print(f\"\\n🚀 ¡Evaluación RAG completada exitosamente!\")\n",
        "print(f\"   Este notebook proporciona una base sólida para la optimización\")\n",
        "print(f\"   y monitoreo continuo del sistema RAG en producción.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Anexo: Estructura del Notebook\n",
        "\n",
        "### 📚 Secciones Implementadas\n",
        "\n",
        "1. **Introducción**: Objetivos, dataset, tecnologías y sistema de persistencia\n",
        "2. **Configuración de Modelos**: Diccionarios de configuración para modelos y métricas\n",
        "3. **Carga del Dataset**: Funciones de carga, validación y visualización\n",
        "4. **Configuración del Entorno**: EnvironmentManager para setup completo\n",
        "5. **Evaluación de Retrievers**: Métricas de embedding + Qdrant\n",
        "6. **Evaluación con Re-ranking**: Mejoras con cross-encoders\n",
        "7. **Evaluación del Flujo Completo**: Pipeline RAG con LLM-as-a-judge\n",
        "8. **Análisis Comparativo Global**: Consolidación y recomendaciones\n",
        "\n",
        "### 🔧 Componentes Técnicos\n",
        "\n",
        "#### Clases Principales\n",
        "- `EvaluationManager`: Sistema de persistencia de evaluaciones\n",
        "- `EnvironmentManager`: Configuración centralizada del entorno\n",
        "- `RetrieverEvaluator`: Evaluación de modelos de embedding\n",
        "- `RerankingEvaluator`: Evaluación de modelos de re-ranking\n",
        "- `RAGPipelineEvaluator`: Evaluación del pipeline completo\n",
        "- `GlobalComparativeAnalyzer`: Análisis comparativo global\n",
        "\n",
        "#### Funciones de Utilidad\n",
        "- Carga y validación de datasets\n",
        "- Visualización de resultados\n",
        "- Cálculo de métricas agregadas\n",
        "- Análisis de correlaciones\n",
        "- Generación de recomendaciones\n",
        "\n",
        "### 📊 Métricas Implementadas\n",
        "\n",
        "#### Retrieval\n",
        "- Recall@k, Precision@k, nDCG@k, MRR, Hit Rate@k\n",
        "\n",
        "#### Calidad (LLM-as-a-judge)\n",
        "- Coherencia, Relevancia, Completitud, Fidelidad, Concisión\n",
        "\n",
        "#### Análisis\n",
        "- Correlaciones entre métricas\n",
        "- Mejoras de re-ranking\n",
        "- Score combinado (retrieval + calidad)\n",
        "\n",
        "### 💾 Sistema de Persistencia\n",
        "\n",
        "- **Formato**: JSON con metadatos completos\n",
        "- **Ubicación**: `evaluation_results/` (configurable)\n",
        "- **Estructura**: Configuración + Resultados + Metadatos\n",
        "- **Versionado**: Timestamp automático\n",
        "- **Comparación**: Funciones para comparar evaluaciones históricas\n",
        "\n",
        "### 🚀 Características Avanzadas\n",
        "\n",
        "- **Evaluación Automática**: Proceso completamente automatizado\n",
        "- **Visualizaciones Dinámicas**: Gráficos adaptativos según datos\n",
        "- **Análisis de Correlaciones**: Identificación automática de patrones\n",
        "- **Recomendaciones Inteligentes**: Basadas en evidencia empírica\n",
        "- **Monitoreo Continuo**: Framework para evaluación periódica\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
