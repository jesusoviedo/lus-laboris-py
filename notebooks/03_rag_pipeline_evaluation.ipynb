{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluaci√≥n Integral del Pipeline RAG\n",
        "\n",
        "Esta notebook eval√∫a el pipeline completo de RAG (Retrieval-Augmented Generation) incluyendo:\n",
        "- **Retrievers** (embeddings + Qdrant)\n",
        "- **Re-ranking** de documentos\n",
        "- **Generaci√≥n** con LLM\n",
        "- **M√©tricas objetivas** (recall, nDCG, MRR, precisi√≥n@k)\n",
        "- **M√©tricas subjetivas** (coherencia, relevancia, completitud via LLM-as-a-judge)\n",
        "\n",
        "## Tecnolog√≠as utilizadas:\n",
        "- Qdrant (base vectorial)\n",
        "- Sentence Transformers (embeddings)\n",
        "- Cross-encoders (re-ranking)\n",
        "- OpenAI/Transformers (generaci√≥n y evaluaci√≥n)\n",
        "- Scikit-learn (m√©tricas)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importaciones y Configuraci√≥n Inicial\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any, Tuple, Optional\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Qdrant\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "\n",
        "# Embeddings\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "\n",
        "# M√©tricas\n",
        "from sklearn.metrics import ndcg_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# LLM\n",
        "import openai\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Configuraci√≥n de visualizaci√≥n\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ Librer√≠as importadas correctamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuraci√≥n de Modelos y Par√°metros\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuraci√≥n de modelos de embedding\n",
        "EMBEDDING_MODELS = {\n",
        "    'all-MiniLM-L6-v2': {\n",
        "        'model_name': 'sentence-transformers/all-MiniLM-L6-v2',\n",
        "        'dimension': 384,\n",
        "        'description': 'Modelo ligero y r√°pido'\n",
        "    },\n",
        "    'all-mpnet-base-v2': {\n",
        "        'model_name': 'sentence-transformers/all-mpnet-base-v2',\n",
        "        'dimension': 768,\n",
        "        'description': 'Modelo balanceado'\n",
        "    },\n",
        "    'paraphrase-multilingual-MiniLM-L12-v2': {\n",
        "        'model_name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
        "        'dimension': 384,\n",
        "        'description': 'Modelo multiling√ºe'\n",
        "    },\n",
        "    'all-distilroberta-v1': {\n",
        "        'model_name': 'sentence-transformers/all-distilroberta-v1',\n",
        "        'dimension': 768,\n",
        "        'description': 'Modelo basado en DistilRoBERTa'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Configuraci√≥n de modelos de re-ranking\n",
        "RERANKING_MODELS = {\n",
        "    'ms-marco-MiniLM-L-6-v2': {\n",
        "        'model_name': 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
        "        'description': 'Re-ranker ligero para MS MARCO'\n",
        "    },\n",
        "    'ms-marco-MiniLM-L-12-v2': {\n",
        "        'model_name': 'cross-encoder/ms-marco-MiniLM-L-12-v2',\n",
        "        'description': 'Re-ranker m√°s robusto para MS MARCO'\n",
        "    },\n",
        "    'ms-marco-MiniLM-L-2-v2': {\n",
        "        'model_name': 'cross-encoder/ms-marco-MiniLM-L-2-v2',\n",
        "        'description': 'Re-ranker ultra-ligero'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Configuraci√≥n de m√©tricas de retrieval\n",
        "RETRIEVAL_METRICS = {\n",
        "    'recall_at_k': [1, 3, 5, 10],\n",
        "    'precision_at_k': [1, 3, 5, 10],\n",
        "    'ndcg_at_k': [1, 3, 5, 10],\n",
        "    'mrr': True,\n",
        "    'hit_rate_at_k': [1, 3, 5, 10]\n",
        "}\n",
        "\n",
        "# Configuraci√≥n de evaluaci√≥n LLM\n",
        "LLM_EVALUATION_CRITERIA = {\n",
        "    'coherence': {\n",
        "        'description': '¬øLa respuesta es coherente y bien estructurada?',\n",
        "        'scale': (1, 5)\n",
        "    },\n",
        "    'relevance': {\n",
        "        'description': '¬øLa respuesta es relevante a la pregunta?',\n",
        "        'scale': (1, 5)\n",
        "    },\n",
        "    'completeness': {\n",
        "        'description': '¬øLa respuesta es completa y abarca todos los aspectos?',\n",
        "        'scale': (1, 5)\n",
        "    },\n",
        "    'fidelity': {\n",
        "        'description': '¬øLa respuesta es fiel al contexto proporcionado?',\n",
        "        'scale': (1, 5)\n",
        "    },\n",
        "    'conciseness': {\n",
        "        'description': '¬øLa respuesta es concisa sin ser incompleta?',\n",
        "        'scale': (1, 5)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Configuraci√≥n de Qdrant\n",
        "QDRANT_CONFIG = {\n",
        "    'url': 'http://localhost:6333',\n",
        "    'collection_name': 'lus_laboris_articles',\n",
        "    'top_k': 20  # N√∫mero de documentos a recuperar inicialmente\n",
        "}\n",
        "\n",
        "# Configuraci√≥n de LLM\n",
        "LLM_CONFIG = {\n",
        "    'provider': 'openai',  # 'openai' o 'huggingface'\n",
        "    'model': 'gpt-3.5-turbo',\n",
        "    'temperature': 0.1,\n",
        "    'max_tokens': 500\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Configuraci√≥n de modelos y par√°metros definida\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Carga del Dataset de Evaluaci√≥n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_evaluation_dataset(file_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Carga el dataset de evaluaci√≥n con preguntas y respuestas esperadas\n",
        "    \n",
        "    Estructura esperada:\n",
        "    - question: Pregunta a evaluar\n",
        "    - expected_answer: Respuesta esperada\n",
        "    - expected_articles: Lista de art√≠culos relevantes (IDs o contenido)\n",
        "    - category: Categor√≠a de la pregunta (opcional)\n",
        "    - difficulty: Nivel de dificultad (opcional)\n",
        "    \"\"\"\n",
        "    if file_path.endswith('.json'):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        return pd.DataFrame(data)\n",
        "    elif file_path.endswith('.csv'):\n",
        "        return pd.read_csv(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Formato de archivo no soportado. Use .json o .csv\")\n",
        "\n",
        "# Ejemplo de dataset de evaluaci√≥n\n",
        "def create_sample_dataset() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Crea un dataset de ejemplo para evaluaci√≥n\n",
        "    \"\"\"\n",
        "    sample_data = [\n",
        "        {\n",
        "            'question': '¬øCu√°l es la duraci√≥n m√°xima de la jornada laboral?',\n",
        "            'expected_answer': 'La duraci√≥n m√°xima de la jornada laboral es de 8 horas diarias.',\n",
        "            'expected_articles': ['art_123', 'art_456'],\n",
        "            'category': 'jornada_laboral',\n",
        "            'difficulty': 'easy'\n",
        "        },\n",
        "        {\n",
        "            'question': '¬øQu√© derechos tiene un trabajador en caso de despido?',\n",
        "            'expected_answer': 'El trabajador tiene derecho a indemnizaci√≥n, preaviso y otros beneficios.',\n",
        "            'expected_articles': ['art_789', 'art_101'],\n",
        "            'category': 'despido',\n",
        "            'difficulty': 'medium'\n",
        "        },\n",
        "        {\n",
        "            'question': '¬øC√≥mo se calcula la indemnizaci√≥n por despido?',\n",
        "            'expected_answer': 'La indemnizaci√≥n se calcula seg√∫n la antig√ºedad y el salario del trabajador.',\n",
        "            'expected_articles': ['art_202', 'art_303'],\n",
        "            'category': 'indemnizacion',\n",
        "            'difficulty': 'hard'\n",
        "        }\n",
        "    ]\n",
        "    return pd.DataFrame(sample_data)\n",
        "\n",
        "# Cargar dataset (usar create_sample_dataset() si no tienes un dataset real)\n",
        "try:\n",
        "    # Intentar cargar dataset real\n",
        "    dataset = load_evaluation_dataset('data/evaluation_dataset.json')\n",
        "    print(f\"‚úÖ Dataset cargado: {len(dataset)} preguntas\")\n",
        "except FileNotFoundError:\n",
        "    # Usar dataset de ejemplo\n",
        "    dataset = create_sample_dataset()\n",
        "    print(f\"‚ö†Ô∏è  Usando dataset de ejemplo: {len(dataset)} preguntas\")\n",
        "\n",
        "print(f\"\\nEstructura del dataset:\")\n",
        "print(dataset.head())\n",
        "print(f\"\\nColumnas: {list(dataset.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Conexi√≥n con Qdrant y Carga de Modelos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Conexi√≥n con Qdrant\n",
        "def connect_to_qdrant() -> QdrantClient:\n",
        "    \"\"\"\n",
        "    Establece conexi√≥n con Qdrant\n",
        "    \"\"\"\n",
        "    try:\n",
        "        client = QdrantClient(url=QDRANT_CONFIG['url'])\n",
        "        # Verificar conexi√≥n\n",
        "        collections = client.get_collections()\n",
        "        print(f\"‚úÖ Conectado a Qdrant. Colecciones disponibles: {len(collections.collections)}\")\n",
        "        return client\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error conectando a Qdrant: {e}\")\n",
        "        raise\n",
        "\n",
        "# Cargar modelos de embedding\n",
        "def load_embedding_models() -> Dict[str, SentenceTransformer]:\n",
        "    \"\"\"\n",
        "    Carga todos los modelos de embedding configurados\n",
        "    \"\"\"\n",
        "    models = {}\n",
        "    print(\"üîÑ Cargando modelos de embedding...\")\n",
        "    \n",
        "    for name, config in EMBEDDING_MODELS.items():\n",
        "        try:\n",
        "            print(f\"  - Cargando {name}...\")\n",
        "            model = SentenceTransformer(config['model_name'])\n",
        "            models[name] = model\n",
        "            print(f\"    ‚úÖ {name} cargado\")\n",
        "        except Exception as e:\n",
        "            print(f\"    ‚ùå Error cargando {name}: {e}\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ {len(models)} modelos de embedding cargados\")\n",
        "    return models\n",
        "\n",
        "# Cargar modelos de re-ranking\n",
        "def load_reranking_models() -> Dict[str, CrossEncoder]:\n",
        "    \"\"\"\n",
        "    Carga todos los modelos de re-ranking configurados\n",
        "    \"\"\"\n",
        "    models = {}\n",
        "    print(\"üîÑ Cargando modelos de re-ranking...\")\n",
        "    \n",
        "    for name, config in RERANKING_MODELS.items():\n",
        "        try:\n",
        "            print(f\"  - Cargando {name}...\")\n",
        "            model = CrossEncoder(config['model_name'])\n",
        "            models[name] = model\n",
        "            print(f\"    ‚úÖ {name} cargado\")\n",
        "        except Exception as e:\n",
        "            print(f\"    ‚ùå Error cargando {name}: {e}\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ {len(models)} modelos de re-ranking cargados\")\n",
        "    return models\n",
        "\n",
        "# Ejecutar carga\n",
        "qdrant_client = connect_to_qdrant()\n",
        "embedding_models = load_embedding_models()\n",
        "reranking_models = load_reranking_models()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Introducci√≥n\n",
        "\n",
        "## Objetivo de la Notebook\n",
        "\n",
        "Esta notebook tiene como objetivo realizar una **evaluaci√≥n integral del pipeline RAG** (Retrieval-Augmented Generation) para el sistema de consultas sobre derecho laboral paraguayo. La evaluaci√≥n se realiza en m√∫ltiples niveles:\n",
        "\n",
        "### üéØ Niveles de Evaluaci√≥n\n",
        "\n",
        "1. **Nivel de Retrieval (Recuperaci√≥n)**\n",
        "   - Evaluaci√≥n de diferentes modelos de embeddings\n",
        "   - Comparaci√≥n de performance en Qdrant\n",
        "   - M√©tricas objetivas: Recall@k, Precision@k, nDCG@k, MRR\n",
        "\n",
        "2. **Nivel de Re-ranking**\n",
        "   - Evaluaci√≥n de modelos cross-encoder\n",
        "   - Mejora en la relevancia de documentos recuperados\n",
        "   - Comparaci√≥n antes vs despu√©s del re-ranking\n",
        "\n",
        "3. **Nivel de Generaci√≥n (LLM)**\n",
        "   - Evaluaci√≥n de respuestas generadas\n",
        "   - M√©tricas subjetivas via LLM-as-a-judge\n",
        "   - An√°lisis de coherencia, relevancia, completitud\n",
        "\n",
        "### üìä Dataset de Evaluaci√≥n\n",
        "\n",
        "El dataset contiene:\n",
        "- **Preguntas**: Consultas reales sobre derecho laboral paraguayo\n",
        "- **Respuestas esperadas**: Ground truth para comparaci√≥n\n",
        "- **Art√≠culos relevantes**: Documentos que deber√≠an ser recuperados\n",
        "- **Categor√≠as**: Clasificaci√≥n por tipo de consulta\n",
        "- **Dificultad**: Nivel de complejidad de la pregunta\n",
        "\n",
        "### üîß Tecnolog√≠as Utilizadas\n",
        "\n",
        "- **Qdrant**: Base de datos vectorial para almacenamiento y b√∫squeda\n",
        "- **Sentence Transformers**: Modelos de embeddings para representaci√≥n de texto\n",
        "- **Cross-encoders**: Modelos de re-ranking para mejorar relevancia\n",
        "- **OpenAI/Transformers**: Modelos de generaci√≥n y evaluaci√≥n\n",
        "- **Scikit-learn**: C√°lculo de m√©tricas de evaluaci√≥n\n",
        "\n",
        "### üíæ Sistema de Persistencia\n",
        "\n",
        "La notebook incluye un sistema para guardar y cargar evaluaciones:\n",
        "- **Guardado autom√°tico**: Cada evaluaci√≥n se guarda en JSON\n",
        "- **Comparaci√≥n hist√≥rica**: Posibilidad de comparar diferentes configuraciones\n",
        "- **Reproducibilidad**: Configuraciones guardadas para replicar experimentos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sistema de Persistencia de Evaluaciones\n",
        "class EvaluationManager:\n",
        "    \"\"\"\n",
        "    Maneja el guardado y carga de evaluaciones para comparaci√≥n hist√≥rica\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, results_dir: str = \"evaluation_results\"):\n",
        "        self.results_dir = Path(results_dir)\n",
        "        self.results_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    def save_evaluation(self, \n",
        "                       evaluation_name: str,\n",
        "                       config: Dict[str, Any],\n",
        "                       results: Dict[str, Any],\n",
        "                       metadata: Optional[Dict[str, Any]] = None) -> str:\n",
        "        \"\"\"\n",
        "        Guarda una evaluaci√≥n completa con configuraci√≥n y resultados\n",
        "        \n",
        "        Args:\n",
        "            evaluation_name: Nombre √∫nico para la evaluaci√≥n\n",
        "            config: Configuraci√≥n de modelos y par√°metros usados\n",
        "            results: Resultados de la evaluaci√≥n\n",
        "            metadata: Metadatos adicionales (timestamp, descripci√≥n, etc.)\n",
        "        \n",
        "        Returns:\n",
        "            Ruta del archivo guardado\n",
        "        \"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"{evaluation_name}_{timestamp}.json\"\n",
        "        filepath = self.results_dir / filename\n",
        "        \n",
        "        evaluation_data = {\n",
        "            \"evaluation_name\": evaluation_name,\n",
        "            \"timestamp\": timestamp,\n",
        "            \"config\": config,\n",
        "            \"results\": results,\n",
        "            \"metadata\": metadata or {}\n",
        "        }\n",
        "        \n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(evaluation_data, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        print(f\"‚úÖ Evaluaci√≥n guardada: {filepath}\")\n",
        "        return str(filepath)\n",
        "    \n",
        "    def load_evaluation(self, filepath: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Carga una evaluaci√≥n guardada\n",
        "        \n",
        "        Args:\n",
        "            filepath: Ruta del archivo de evaluaci√≥n\n",
        "        \n",
        "        Returns:\n",
        "            Datos de la evaluaci√≥n\n",
        "        \"\"\"\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    \n",
        "    def list_evaluations(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Lista todas las evaluaciones guardadas\n",
        "        \n",
        "        Returns:\n",
        "            Lista de evaluaciones con metadatos\n",
        "        \"\"\"\n",
        "        evaluations = []\n",
        "        for filepath in self.results_dir.glob(\"*.json\"):\n",
        "            try:\n",
        "                with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                    data = json.load(f)\n",
        "                evaluations.append({\n",
        "                    \"filepath\": str(filepath),\n",
        "                    \"name\": data.get(\"evaluation_name\", \"unknown\"),\n",
        "                    \"timestamp\": data.get(\"timestamp\", \"unknown\"),\n",
        "                    \"metadata\": data.get(\"metadata\", {})\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Error cargando {filepath}: {e}\")\n",
        "        \n",
        "        return sorted(evaluations, key=lambda x: x[\"timestamp\"], reverse=True)\n",
        "    \n",
        "    def compare_evaluations(self, evaluation_paths: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Compara m√∫ltiples evaluaciones en una tabla\n",
        "        \n",
        "        Args:\n",
        "            evaluation_paths: Lista de rutas de evaluaciones a comparar\n",
        "        \n",
        "        Returns:\n",
        "            DataFrame con comparaci√≥n de m√©tricas\n",
        "        \"\"\"\n",
        "        comparison_data = []\n",
        "        \n",
        "        for path in evaluation_paths:\n",
        "            eval_data = self.load_evaluation(path)\n",
        "            name = eval_data[\"evaluation_name\"]\n",
        "            timestamp = eval_data[\"timestamp\"]\n",
        "            results = eval_data[\"results\"]\n",
        "            \n",
        "            # Extraer m√©tricas principales\n",
        "            row = {\n",
        "                \"evaluation\": name,\n",
        "                \"timestamp\": timestamp,\n",
        "                \"config\": eval_data[\"config\"]\n",
        "            }\n",
        "            \n",
        "            # Agregar m√©tricas de retrieval si existen\n",
        "            if \"retrieval_metrics\" in results:\n",
        "                for metric, values in results[\"retrieval_metrics\"].items():\n",
        "                    if isinstance(values, dict):\n",
        "                        for k, v in values.items():\n",
        "                            row[f\"retrieval_{metric}_{k}\"] = v\n",
        "                    else:\n",
        "                        row[f\"retrieval_{metric}\"] = values\n",
        "            \n",
        "            # Agregar m√©tricas de LLM si existen\n",
        "            if \"llm_metrics\" in results:\n",
        "                for metric, values in results[\"llm_metrics\"].items():\n",
        "                    if isinstance(values, dict):\n",
        "                        for k, v in values.items():\n",
        "                            row[f\"llm_{metric}_{k}\"] = v\n",
        "                    else:\n",
        "                        row[f\"llm_{metric}\"] = values\n",
        "            \n",
        "            comparison_data.append(row)\n",
        "        \n",
        "        return pd.DataFrame(comparison_data)\n",
        "\n",
        "# Inicializar el manager de evaluaciones\n",
        "eval_manager = EvaluationManager()\n",
        "print(\"‚úÖ Sistema de persistencia de evaluaciones inicializado\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Carga del Dataset\n",
        "\n",
        "## Estructura del Dataset de Evaluaci√≥n\n",
        "\n",
        "El dataset de evaluaci√≥n debe contener preguntas y respuestas esperadas para poder medir la calidad del pipeline RAG. La estructura recomendada incluye:\n",
        "\n",
        "### üìã Campos Requeridos\n",
        "\n",
        "- **`question`**: Pregunta a evaluar\n",
        "- **`expected_answer`**: Respuesta esperada (ground truth)\n",
        "- **`expected_articles`**: Lista de IDs de art√≠culos relevantes que deber√≠an ser recuperados\n",
        "- **`category`**: Categor√≠a de la pregunta (opcional)\n",
        "- **`difficulty`**: Nivel de dificultad (opcional)\n",
        "\n",
        "### üéØ Tipos de Dataset Soportados\n",
        "\n",
        "1. **Dataset Real**: Archivo JSON/CSV con preguntas reales del dominio\n",
        "2. **Dataset de Ejemplo**: Dataset sint√©tico para pruebas iniciales\n",
        "3. **Dataset H√≠brido**: Combinaci√≥n de datos reales y sint√©ticos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funciones de carga y validaci√≥n del dataset\n",
        "def validate_dataset(df: pd.DataFrame) -> Tuple[bool, List[str]]:\n",
        "    \"\"\"\n",
        "    Valida que el dataset tenga la estructura correcta\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame del dataset\n",
        "    \n",
        "    Returns:\n",
        "        Tuple[bool, List[str]]: (es_v√°lido, lista_de_errores)\n",
        "    \"\"\"\n",
        "    errors = []\n",
        "    required_columns = ['question', 'expected_answer', 'expected_articles']\n",
        "    \n",
        "    # Verificar columnas requeridas\n",
        "    for col in required_columns:\n",
        "        if col not in df.columns:\n",
        "            errors.append(f\"Columna requerida '{col}' no encontrada\")\n",
        "    \n",
        "    if errors:\n",
        "        return False, errors\n",
        "    \n",
        "    # Verificar que no haya valores nulos en columnas requeridas\n",
        "    for col in required_columns:\n",
        "        if df[col].isnull().any():\n",
        "            errors.append(f\"Columna '{col}' contiene valores nulos\")\n",
        "    \n",
        "    # Verificar que expected_articles sea una lista\n",
        "    if 'expected_articles' in df.columns:\n",
        "        for idx, articles in df['expected_articles'].items():\n",
        "            if not isinstance(articles, list):\n",
        "                errors.append(f\"Fila {idx}: expected_articles debe ser una lista\")\n",
        "    \n",
        "    return len(errors) == 0, errors\n",
        "\n",
        "def create_enhanced_sample_dataset() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Crea un dataset de ejemplo m√°s completo para evaluaci√≥n\n",
        "    \"\"\"\n",
        "    sample_data = [\n",
        "        {\n",
        "            'question': '¬øCu√°l es la duraci√≥n m√°xima de la jornada laboral?',\n",
        "            'expected_answer': 'La duraci√≥n m√°xima de la jornada laboral es de 8 horas diarias seg√∫n el art√≠culo 154 del C√≥digo del Trabajo.',\n",
        "            'expected_articles': ['art_154', 'art_155', 'art_156'],\n",
        "            'category': 'jornada_laboral',\n",
        "            'difficulty': 'easy',\n",
        "            'keywords': ['jornada', 'horas', 'duraci√≥n', 'm√°xima']\n",
        "        },\n",
        "        {\n",
        "            'question': '¬øQu√© derechos tiene un trabajador en caso de despido?',\n",
        "            'expected_answer': 'El trabajador tiene derecho a indemnizaci√≥n, preaviso, vacaciones proporcionales y otros beneficios seg√∫n los art√≠culos 91 y 92.',\n",
        "            'expected_articles': ['art_91', 'art_92', 'art_93', 'art_94'],\n",
        "            'category': 'despido',\n",
        "            'difficulty': 'medium',\n",
        "            'keywords': ['despido', 'derechos', 'indemnizaci√≥n', 'preaviso']\n",
        "        },\n",
        "        {\n",
        "            'question': '¬øC√≥mo se calcula la indemnizaci√≥n por despido?',\n",
        "            'expected_answer': 'La indemnizaci√≥n se calcula multiplicando el salario diario por 30 d√≠as por cada a√±o de antig√ºedad, con un m√≠nimo de 15 d√≠as.',\n",
        "            'expected_articles': ['art_91', 'art_92', 'art_95'],\n",
        "            'category': 'indemnizacion',\n",
        "            'difficulty': 'hard',\n",
        "            'keywords': ['indemnizaci√≥n', 'c√°lculo', 'antig√ºedad', 'salario']\n",
        "        },\n",
        "        {\n",
        "            'question': '¬øCu√°les son las condiciones para el trabajo nocturno?',\n",
        "            'expected_answer': 'El trabajo nocturno tiene condiciones especiales de horario, remuneraci√≥n y descanso seg√∫n los art√≠culos 160-165.',\n",
        "            'expected_articles': ['art_160', 'art_161', 'art_162', 'art_163', 'art_164', 'art_165'],\n",
        "            'category': 'trabajo_nocturno',\n",
        "            'difficulty': 'medium',\n",
        "            'keywords': ['nocturno', 'horario', 'remuneraci√≥n', 'descanso']\n",
        "        },\n",
        "        {\n",
        "            'question': '¬øQu√© es el salario m√≠nimo y c√≥mo se establece?',\n",
        "            'expected_answer': 'El salario m√≠nimo es la remuneraci√≥n m√≠nima que debe recibir un trabajador, establecida por el Consejo Nacional del Salario M√≠nimo.',\n",
        "            'expected_articles': ['art_200', 'art_201', 'art_202'],\n",
        "            'category': 'salario',\n",
        "            'difficulty': 'easy',\n",
        "            'keywords': ['salario', 'm√≠nimo', 'remuneraci√≥n', 'consejo']\n",
        "        }\n",
        "    ]\n",
        "    return pd.DataFrame(sample_data)\n",
        "\n",
        "def load_and_prepare_dataset(file_path: Optional[str] = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Carga y prepara el dataset de evaluaci√≥n\n",
        "    \n",
        "    Args:\n",
        "        file_path: Ruta del archivo de dataset (opcional)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame preparado y validado\n",
        "    \"\"\"\n",
        "    if file_path and Path(file_path).exists():\n",
        "        print(f\"üîÑ Cargando dataset desde: {file_path}\")\n",
        "        df = load_evaluation_dataset(file_path)\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  Usando dataset de ejemplo\")\n",
        "        df = create_enhanced_sample_dataset()\n",
        "    \n",
        "    # Validar dataset\n",
        "    is_valid, errors = validate_dataset(df)\n",
        "    \n",
        "    if not is_valid:\n",
        "        print(\"‚ùå Errores en el dataset:\")\n",
        "        for error in errors:\n",
        "            print(f\"  - {error}\")\n",
        "        raise ValueError(\"Dataset no v√°lido\")\n",
        "    \n",
        "    print(f\"‚úÖ Dataset cargado y validado: {len(df)} preguntas\")\n",
        "    \n",
        "    # Mostrar estad√≠sticas del dataset\n",
        "    print(f\"\\nüìä Estad√≠sticas del dataset:\")\n",
        "    print(f\"  - Total de preguntas: {len(df)}\")\n",
        "    if 'category' in df.columns:\n",
        "        print(f\"  - Categor√≠as: {df['category'].nunique()}\")\n",
        "        print(f\"  - Distribuci√≥n por categor√≠a:\")\n",
        "        for cat, count in df['category'].value_counts().items():\n",
        "            print(f\"    * {cat}: {count}\")\n",
        "    \n",
        "    if 'difficulty' in df.columns:\n",
        "        print(f\"  - Distribuci√≥n por dificultad:\")\n",
        "        for diff, count in df['difficulty'].value_counts().items():\n",
        "            print(f\"    * {diff}: {count}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Cargar el dataset\n",
        "print(\"üîÑ Cargando dataset de evaluaci√≥n...\")\n",
        "dataset = load_and_prepare_dataset()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizaci√≥n del dataset\n",
        "def visualize_dataset_distribution(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Visualiza la distribuci√≥n del dataset\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('Distribuci√≥n del Dataset de Evaluaci√≥n', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Distribuci√≥n por categor√≠a\n",
        "    if 'category' in df.columns:\n",
        "        category_counts = df['category'].value_counts()\n",
        "        axes[0, 0].pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%')\n",
        "        axes[0, 0].set_title('Distribuci√≥n por Categor√≠a')\n",
        "    \n",
        "    # Distribuci√≥n por dificultad\n",
        "    if 'difficulty' in df.columns:\n",
        "        difficulty_counts = df['difficulty'].value_counts()\n",
        "        axes[0, 1].bar(difficulty_counts.index, difficulty_counts.values, color='skyblue')\n",
        "        axes[0, 1].set_title('Distribuci√≥n por Dificultad')\n",
        "        axes[0, 1].set_xlabel('Dificultad')\n",
        "        axes[0, 1].set_ylabel('N√∫mero de Preguntas')\n",
        "    \n",
        "    # Longitud de preguntas\n",
        "    question_lengths = df['question'].str.len()\n",
        "    axes[1, 0].hist(question_lengths, bins=20, color='lightgreen', alpha=0.7)\n",
        "    axes[1, 0].set_title('Distribuci√≥n de Longitud de Preguntas')\n",
        "    axes[1, 0].set_xlabel('Caracteres')\n",
        "    axes[1, 0].set_ylabel('Frecuencia')\n",
        "    \n",
        "    # Longitud de respuestas esperadas\n",
        "    answer_lengths = df['expected_answer'].str.len()\n",
        "    axes[1, 1].hist(answer_lengths, bins=20, color='lightcoral', alpha=0.7)\n",
        "    axes[1, 1].set_title('Distribuci√≥n de Longitud de Respuestas')\n",
        "    axes[1, 1].set_xlabel('Caracteres')\n",
        "    axes[1, 1].set_ylabel('Frecuencia')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Estad√≠sticas adicionales\n",
        "    print(f\"\\nüìà Estad√≠sticas detalladas:\")\n",
        "    print(f\"  - Longitud promedio de preguntas: {question_lengths.mean():.1f} caracteres\")\n",
        "    print(f\"  - Longitud promedio de respuestas: {answer_lengths.mean():.1f} caracteres\")\n",
        "    print(f\"  - N√∫mero promedio de art√≠culos esperados: {df['expected_articles'].apply(len).mean():.1f}\")\n",
        "\n",
        "# Mostrar algunas preguntas de ejemplo\n",
        "def show_sample_questions(df: pd.DataFrame, n: int = 3):\n",
        "    \"\"\"\n",
        "    Muestra preguntas de ejemplo del dataset\n",
        "    \"\"\"\n",
        "    print(f\"\\nüìù Ejemplos de preguntas del dataset:\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    for idx, row in df.head(n).iterrows():\n",
        "        print(f\"\\nüîπ Pregunta {idx + 1}:\")\n",
        "        print(f\"   Pregunta: {row['question']}\")\n",
        "        print(f\"   Respuesta esperada: {row['expected_answer']}\")\n",
        "        print(f\"   Art√≠culos relevantes: {row['expected_articles']}\")\n",
        "        if 'category' in row:\n",
        "            print(f\"   Categor√≠a: {row['category']}\")\n",
        "        if 'difficulty' in row:\n",
        "            print(f\"   Dificultad: {row['difficulty']}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "# Ejecutar visualizaciones\n",
        "visualize_dataset_distribution(dataset)\n",
        "show_sample_questions(dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Configuraci√≥n del Entorno\n",
        "\n",
        "## Configuraci√≥n de Servicios y Modelos\n",
        "\n",
        "Esta secci√≥n establece la conexi√≥n con todos los servicios necesarios para la evaluaci√≥n del pipeline RAG:\n",
        "\n",
        "### üîß Servicios a Configurar\n",
        "\n",
        "1. **Qdrant**: Base de datos vectorial para retrieval\n",
        "2. **Modelos de Embedding**: Para representaci√≥n de texto\n",
        "3. **Modelos de Re-ranking**: Para mejorar relevancia de documentos\n",
        "4. **LLM**: Para generaci√≥n y evaluaci√≥n de respuestas\n",
        "5. **Sistema de M√©tricas**: Para c√°lculo de m√©tricas de evaluaci√≥n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuraci√≥n avanzada del entorno\n",
        "class EnvironmentManager:\n",
        "    \"\"\"\n",
        "    Maneja la configuraci√≥n y conexi√≥n de todos los servicios\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.qdrant_client = None\n",
        "        self.embedding_models = {}\n",
        "        self.reranking_models = {}\n",
        "        self.llm_pipeline = None\n",
        "        self.evaluation_metrics = {}\n",
        "        \n",
        "    def setup_qdrant(self) -> bool:\n",
        "        \"\"\"\n",
        "        Configura la conexi√≥n con Qdrant\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"üîÑ Configurando Qdrant...\")\n",
        "            self.qdrant_client = QdrantClient(url=QDRANT_CONFIG['url'])\n",
        "            \n",
        "            # Verificar conexi√≥n\n",
        "            collections = self.qdrant_client.get_collections()\n",
        "            print(f\"‚úÖ Qdrant conectado. Colecciones: {len(collections.collections)}\")\n",
        "            \n",
        "            # Verificar colecci√≥n espec√≠fica\n",
        "            collection_name = QDRANT_CONFIG['collection_name']\n",
        "            try:\n",
        "                collection_info = self.qdrant_client.get_collection(collection_name)\n",
        "                print(f\"‚úÖ Colecci√≥n '{collection_name}' encontrada:\")\n",
        "                print(f\"   - Puntos: {collection_info.points_count}\")\n",
        "                print(f\"   - Dimensiones: {collection_info.config.params.vectors.size}\")\n",
        "                print(f\"   - Distancia: {collection_info.config.params.vectors.distance}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Colecci√≥n '{collection_name}' no encontrada: {e}\")\n",
        "                print(\"   Aseg√∫rate de que la colecci√≥n existe y tiene datos\")\n",
        "            \n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error configurando Qdrant: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def setup_embedding_models(self) -> bool:\n",
        "        \"\"\"\n",
        "        Carga todos los modelos de embedding configurados\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"üîÑ Cargando modelos de embedding...\")\n",
        "            self.embedding_models = {}\n",
        "            \n",
        "            for name, config in EMBEDDING_MODELS.items():\n",
        "                try:\n",
        "                    print(f\"  - Cargando {name}...\")\n",
        "                    model = SentenceTransformer(config['model_name'])\n",
        "                    self.embedding_models[name] = {\n",
        "                        'model': model,\n",
        "                        'config': config\n",
        "                    }\n",
        "                    print(f\"    ‚úÖ {name} cargado (dimensi√≥n: {config['dimension']})\")\n",
        "                except Exception as e:\n",
        "                    print(f\"    ‚ùå Error cargando {name}: {e}\")\n",
        "            \n",
        "            print(f\"‚úÖ {len(self.embedding_models)} modelos de embedding cargados\")\n",
        "            return len(self.embedding_models) > 0\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error configurando modelos de embedding: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def setup_reranking_models(self) -> bool:\n",
        "        \"\"\"\n",
        "        Carga todos los modelos de re-ranking configurados\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"üîÑ Cargando modelos de re-ranking...\")\n",
        "            self.reranking_models = {}\n",
        "            \n",
        "            for name, config in RERANKING_MODELS.items():\n",
        "                try:\n",
        "                    print(f\"  - Cargando {name}...\")\n",
        "                    model = CrossEncoder(config['model_name'])\n",
        "                    self.reranking_models[name] = {\n",
        "                        'model': model,\n",
        "                        'config': config\n",
        "                    }\n",
        "                    print(f\"    ‚úÖ {name} cargado\")\n",
        "                except Exception as e:\n",
        "                    print(f\"    ‚ùå Error cargando {name}: {e}\")\n",
        "            \n",
        "            print(f\"‚úÖ {len(self.reranking_models)} modelos de re-ranking cargados\")\n",
        "            return len(self.reranking_models) > 0\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error configurando modelos de re-ranking: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def setup_llm_pipeline(self) -> bool:\n",
        "        \"\"\"\n",
        "        Configura el pipeline de LLM para generaci√≥n y evaluaci√≥n\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"üîÑ Configurando LLM pipeline...\")\n",
        "            \n",
        "            if LLM_CONFIG['provider'] == 'openai':\n",
        "                # Configurar OpenAI\n",
        "                openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "                if not openai.api_key:\n",
        "                    print(\"‚ö†Ô∏è  OPENAI_API_KEY no encontrada. Usando modelo local.\")\n",
        "                    return self._setup_local_llm()\n",
        "                \n",
        "                self.llm_pipeline = {\n",
        "                    'provider': 'openai',\n",
        "                    'model': LLM_CONFIG['model'],\n",
        "                    'temperature': LLM_CONFIG['temperature'],\n",
        "                    'max_tokens': LLM_CONFIG['max_tokens']\n",
        "                }\n",
        "                print(f\"‚úÖ OpenAI configurado: {LLM_CONFIG['model']}\")\n",
        "                \n",
        "            else:\n",
        "                return self._setup_local_llm()\n",
        "            \n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error configurando LLM: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def _setup_local_llm(self) -> bool:\n",
        "        \"\"\"\n",
        "        Configura un modelo local como fallback\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"üîÑ Configurando modelo local...\")\n",
        "            model_name = \"microsoft/DialoGPT-medium\"  # Modelo m√°s ligero\n",
        "            \n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "            \n",
        "            self.llm_pipeline = {\n",
        "                'provider': 'huggingface',\n",
        "                'model': model,\n",
        "                'tokenizer': tokenizer,\n",
        "                'model_name': model_name\n",
        "            }\n",
        "            \n",
        "            print(f\"‚úÖ Modelo local configurado: {model_name}\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error configurando modelo local: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def setup_evaluation_metrics(self) -> bool:\n",
        "        \"\"\"\n",
        "        Configura el sistema de m√©tricas de evaluaci√≥n\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"üîÑ Configurando sistema de m√©tricas...\")\n",
        "            \n",
        "            self.evaluation_metrics = {\n",
        "                'retrieval_metrics': RETRIEVAL_METRICS,\n",
        "                'llm_criteria': LLM_EVALUATION_CRITERIA,\n",
        "                'custom_metrics': {}\n",
        "            }\n",
        "            \n",
        "            print(\"‚úÖ Sistema de m√©tricas configurado\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error configurando m√©tricas: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def initialize_all(self) -> Dict[str, bool]:\n",
        "        \"\"\"\n",
        "        Inicializa todos los servicios\n",
        "        \"\"\"\n",
        "        print(\"üöÄ Inicializando entorno completo...\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        results = {\n",
        "            'qdrant': self.setup_qdrant(),\n",
        "            'embedding_models': self.setup_embedding_models(),\n",
        "            'reranking_models': self.setup_reranking_models(),\n",
        "            'llm_pipeline': self.setup_llm_pipeline(),\n",
        "            'evaluation_metrics': self.setup_evaluation_metrics()\n",
        "        }\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"üìä Resumen de inicializaci√≥n:\")\n",
        "        for service, status in results.items():\n",
        "            status_icon = \"‚úÖ\" if status else \"‚ùå\"\n",
        "            print(f\"  {status_icon} {service}: {'OK' if status else 'ERROR'}\")\n",
        "        \n",
        "        all_ok = all(results.values())\n",
        "        if all_ok:\n",
        "            print(\"\\nüéâ ¬°Entorno completamente configurado!\")\n",
        "        else:\n",
        "            print(\"\\n‚ö†Ô∏è  Algunos servicios no se configuraron correctamente\")\n",
        "        \n",
        "        return results\n",
        "\n",
        "# Inicializar el manager del entorno\n",
        "env_manager = EnvironmentManager()\n",
        "initialization_results = env_manager.initialize_all()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funciones de utilidad para el entorno\n",
        "def get_environment_status() -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Obtiene el estado actual del entorno\n",
        "    \"\"\"\n",
        "    status = {\n",
        "        'qdrant': {\n",
        "            'connected': env_manager.qdrant_client is not None,\n",
        "            'collections_available': 0\n",
        "        },\n",
        "        'embedding_models': {\n",
        "            'loaded': len(env_manager.embedding_models),\n",
        "            'models': list(env_manager.embedding_models.keys())\n",
        "        },\n",
        "        'reranking_models': {\n",
        "            'loaded': len(env_manager.reranking_models),\n",
        "            'models': list(env_manager.reranking_models.keys())\n",
        "        },\n",
        "        'llm_pipeline': {\n",
        "            'configured': env_manager.llm_pipeline is not None,\n",
        "            'provider': env_manager.llm_pipeline.get('provider') if env_manager.llm_pipeline else None\n",
        "        },\n",
        "        'evaluation_metrics': {\n",
        "            'configured': len(env_manager.evaluation_metrics) > 0\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Obtener informaci√≥n adicional de Qdrant\n",
        "    if env_manager.qdrant_client:\n",
        "        try:\n",
        "            collections = env_manager.qdrant_client.get_collections()\n",
        "            status['qdrant']['collections_available'] = len(collections.collections)\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    return status\n",
        "\n",
        "def test_environment_connectivity() -> Dict[str, bool]:\n",
        "    \"\"\"\n",
        "    Prueba la conectividad de todos los servicios\n",
        "    \"\"\"\n",
        "    print(\"üß™ Probando conectividad del entorno...\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    tests = {}\n",
        "    \n",
        "    # Test Qdrant\n",
        "    if env_manager.qdrant_client:\n",
        "        try:\n",
        "            collections = env_manager.qdrant_client.get_collections()\n",
        "            tests['qdrant'] = True\n",
        "            print(\"‚úÖ Qdrant: Conectado\")\n",
        "        except Exception as e:\n",
        "            tests['qdrant'] = False\n",
        "            print(f\"‚ùå Qdrant: Error - {e}\")\n",
        "    else:\n",
        "        tests['qdrant'] = False\n",
        "        print(\"‚ùå Qdrant: No configurado\")\n",
        "    \n",
        "    # Test Embedding Models\n",
        "    if env_manager.embedding_models:\n",
        "        try:\n",
        "            # Probar con un modelo\n",
        "            test_model = list(env_manager.embedding_models.values())[0]['model']\n",
        "            test_embedding = test_model.encode([\"test\"])\n",
        "            tests['embedding_models'] = True\n",
        "            print(f\"‚úÖ Embedding Models: {len(env_manager.embedding_models)} modelos funcionando\")\n",
        "        except Exception as e:\n",
        "            tests['embedding_models'] = False\n",
        "            print(f\"‚ùå Embedding Models: Error - {e}\")\n",
        "    else:\n",
        "        tests['embedding_models'] = False\n",
        "        print(\"‚ùå Embedding Models: No configurados\")\n",
        "    \n",
        "    # Test Re-ranking Models\n",
        "    if env_manager.reranking_models:\n",
        "        try:\n",
        "            # Probar con un modelo\n",
        "            test_model = list(env_manager.reranking_models.values())[0]['model']\n",
        "            test_scores = test_model.predict([(\"test query\", \"test document\")])\n",
        "            tests['reranking_models'] = True\n",
        "            print(f\"‚úÖ Re-ranking Models: {len(env_manager.reranking_models)} modelos funcionando\")\n",
        "        except Exception as e:\n",
        "            tests['reranking_models'] = False\n",
        "            print(f\"‚ùå Re-ranking Models: Error - {e}\")\n",
        "    else:\n",
        "        tests['reranking_models'] = False\n",
        "        print(\"‚ùå Re-ranking Models: No configurados\")\n",
        "    \n",
        "    # Test LLM Pipeline\n",
        "    if env_manager.llm_pipeline:\n",
        "        try:\n",
        "            if env_manager.llm_pipeline['provider'] == 'openai':\n",
        "                # Test b√°sico de OpenAI\n",
        "                tests['llm_pipeline'] = True\n",
        "                print(\"‚úÖ LLM Pipeline: OpenAI configurado\")\n",
        "            else:\n",
        "                # Test modelo local\n",
        "                tests['llm_pipeline'] = True\n",
        "                print(\"‚úÖ LLM Pipeline: Modelo local configurado\")\n",
        "        except Exception as e:\n",
        "            tests['llm_pipeline'] = False\n",
        "            print(f\"‚ùå LLM Pipeline: Error - {e}\")\n",
        "    else:\n",
        "        tests['llm_pipeline'] = False\n",
        "        print(\"‚ùå LLM Pipeline: No configurado\")\n",
        "    \n",
        "    print(\"=\" * 40)\n",
        "    successful_tests = sum(tests.values())\n",
        "    total_tests = len(tests)\n",
        "    print(f\"üìä Resultado: {successful_tests}/{total_tests} servicios funcionando\")\n",
        "    \n",
        "    return tests\n",
        "\n",
        "def display_environment_info():\n",
        "    \"\"\"\n",
        "    Muestra informaci√≥n detallada del entorno\n",
        "    \"\"\"\n",
        "    print(\"\\nüîç Informaci√≥n del Entorno\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    status = get_environment_status()\n",
        "    \n",
        "    # Qdrant\n",
        "    print(f\"\\nüóÑÔ∏è  Qdrant:\")\n",
        "    print(f\"   - Conectado: {'S√≠' if status['qdrant']['connected'] else 'No'}\")\n",
        "    print(f\"   - Colecciones disponibles: {status['qdrant']['collections_available']}\")\n",
        "    \n",
        "    # Embedding Models\n",
        "    print(f\"\\nüß† Modelos de Embedding:\")\n",
        "    print(f\"   - Cargados: {status['embedding_models']['loaded']}\")\n",
        "    for model in status['embedding_models']['models']:\n",
        "        print(f\"     * {model}\")\n",
        "    \n",
        "    # Re-ranking Models\n",
        "    print(f\"\\nüîÑ Modelos de Re-ranking:\")\n",
        "    print(f\"   - Cargados: {status['reranking_models']['loaded']}\")\n",
        "    for model in status['reranking_models']['models']:\n",
        "        print(f\"     * {model}\")\n",
        "    \n",
        "    # LLM Pipeline\n",
        "    print(f\"\\nü§ñ LLM Pipeline:\")\n",
        "    print(f\"   - Configurado: {'S√≠' if status['llm_pipeline']['configured'] else 'No'}\")\n",
        "    if status['llm_pipeline']['provider']:\n",
        "        print(f\"   - Proveedor: {status['llm_pipeline']['provider']}\")\n",
        "    \n",
        "    # Evaluation Metrics\n",
        "    print(f\"\\nüìä M√©tricas de Evaluaci√≥n:\")\n",
        "    print(f\"   - Configuradas: {'S√≠' if status['evaluation_metrics']['configured'] else 'No'}\")\n",
        "\n",
        "# Ejecutar pruebas y mostrar informaci√≥n\n",
        "connectivity_results = test_environment_connectivity()\n",
        "display_environment_info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Evaluaci√≥n de Retrievers (Embeddings + Qdrant)\n",
        "\n",
        "## Objetivo de la Evaluaci√≥n\n",
        "\n",
        "Esta secci√≥n eval√∫a la calidad del sistema de recuperaci√≥n de documentos usando diferentes modelos de embeddings y Qdrant. Se miden m√©tricas objetivas para determinar qu√© modelo de embedding funciona mejor para el dominio del derecho laboral paraguayo.\n",
        "\n",
        "### üéØ M√©tricas de Evaluaci√≥n\n",
        "\n",
        "- **Recall@k**: Proporci√≥n de documentos relevantes recuperados en los top-k\n",
        "- **Precision@k**: Proporci√≥n de documentos relevantes entre los top-k recuperados\n",
        "- **nDCG@k**: Normalized Discounted Cumulative Gain - considera el ranking\n",
        "- **MRR**: Mean Reciprocal Rank - posici√≥n del primer documento relevante\n",
        "- **Hit Rate@k**: Proporci√≥n de consultas que tienen al menos un documento relevante en top-k\n",
        "\n",
        "### üìä Proceso de Evaluaci√≥n\n",
        "\n",
        "1. **Generaci√≥n de embeddings** para cada pregunta del dataset\n",
        "2. **B√∫squeda en Qdrant** usando diferentes modelos de embedding\n",
        "3. **C√°lculo de m√©tricas** comparando con documentos esperados\n",
        "4. **An√°lisis comparativo** entre modelos\n",
        "5. **Visualizaci√≥n de resultados** para identificar patrones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clase para evaluaci√≥n de retrievers\n",
        "class RetrieverEvaluator:\n",
        "    \"\"\"\n",
        "    Eval√∫a la calidad de diferentes modelos de embedding para retrieval\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, qdrant_client, embedding_models, collection_name, top_k=20):\n",
        "        self.qdrant_client = qdrant_client\n",
        "        self.embedding_models = embedding_models\n",
        "        self.collection_name = collection_name\n",
        "        self.top_k = top_k\n",
        "        self.results = {}\n",
        "    \n",
        "    def search_documents(self, query: str, embedding_model_name: str, top_k: int = None) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Busca documentos en Qdrant usando un modelo de embedding espec√≠fico\n",
        "        \n",
        "        Args:\n",
        "            query: Pregunta a buscar\n",
        "            embedding_model_name: Nombre del modelo de embedding\n",
        "            top_k: N√∫mero de documentos a recuperar\n",
        "        \n",
        "        Returns:\n",
        "            Lista de documentos recuperados con scores\n",
        "        \"\"\"\n",
        "        if top_k is None:\n",
        "            top_k = self.top_k\n",
        "        \n",
        "        try:\n",
        "            # Obtener modelo de embedding\n",
        "            model_info = self.embedding_models[embedding_model_name]\n",
        "            model = model_info['model']\n",
        "            \n",
        "            # Generar embedding de la consulta\n",
        "            query_embedding = model.encode([query])[0].tolist()\n",
        "            \n",
        "            # Buscar en Qdrant\n",
        "            search_results = self.qdrant_client.search(\n",
        "                collection_name=self.collection_name,\n",
        "                query_vector=query_embedding,\n",
        "                limit=top_k,\n",
        "                with_payload=True,\n",
        "                with_vectors=False\n",
        "            )\n",
        "            \n",
        "            # Formatear resultados\n",
        "            documents = []\n",
        "            for result in search_results:\n",
        "                documents.append({\n",
        "                    'id': result.id,\n",
        "                    'score': result.score,\n",
        "                    'payload': result.payload\n",
        "                })\n",
        "            \n",
        "            return documents\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error en b√∫squeda con {embedding_model_name}: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def calculate_recall_at_k(self, retrieved_ids: List[str], expected_ids: List[str], k: int) -> float:\n",
        "        \"\"\"\n",
        "        Calcula Recall@k\n",
        "        \n",
        "        Args:\n",
        "            retrieved_ids: IDs de documentos recuperados\n",
        "            expected_ids: IDs de documentos esperados\n",
        "            k: N√∫mero de documentos a considerar\n",
        "        \n",
        "        Returns:\n",
        "            Recall@k score\n",
        "        \"\"\"\n",
        "        if not expected_ids:\n",
        "            return 0.0\n",
        "        \n",
        "        retrieved_k = set(retrieved_ids[:k])\n",
        "        expected_set = set(expected_ids)\n",
        "        \n",
        "        intersection = retrieved_k.intersection(expected_set)\n",
        "        return len(intersection) / len(expected_set)\n",
        "    \n",
        "    def calculate_precision_at_k(self, retrieved_ids: List[str], expected_ids: List[str], k: int) -> float:\n",
        "        \"\"\"\n",
        "        Calcula Precision@k\n",
        "        \n",
        "        Args:\n",
        "            retrieved_ids: IDs de documentos recuperados\n",
        "            expected_ids: IDs de documentos esperados\n",
        "            k: N√∫mero de documentos a considerar\n",
        "        \n",
        "        Returns:\n",
        "            Precision@k score\n",
        "        \"\"\"\n",
        "        if k == 0:\n",
        "            return 0.0\n",
        "        \n",
        "        retrieved_k = set(retrieved_ids[:k])\n",
        "        expected_set = set(expected_ids)\n",
        "        \n",
        "        intersection = retrieved_k.intersection(expected_set)\n",
        "        return len(intersection) / k\n",
        "    \n",
        "    def calculate_ndcg_at_k(self, retrieved_ids: List[str], expected_ids: List[str], k: int) -> float:\n",
        "        \"\"\"\n",
        "        Calcula nDCG@k\n",
        "        \n",
        "        Args:\n",
        "            retrieved_ids: IDs de documentos recuperados\n",
        "            expected_ids: IDs de documentos esperados\n",
        "            k: N√∫mero de documentos a considerar\n",
        "        \n",
        "        Returns:\n",
        "            nDCG@k score\n",
        "        \"\"\"\n",
        "        if not expected_ids or k == 0:\n",
        "            return 0.0\n",
        "        \n",
        "        # Crear relevancia binaria\n",
        "        relevance = [1 if doc_id in expected_ids else 0 for doc_id in retrieved_ids[:k]]\n",
        "        \n",
        "        # Calcular DCG\n",
        "        dcg = 0.0\n",
        "        for i, rel in enumerate(relevance):\n",
        "            dcg += rel / np.log2(i + 2)  # i+2 porque el log2(1) = 0\n",
        "        \n",
        "        # Calcular IDCG (ideal DCG)\n",
        "        ideal_relevance = [1] * min(len(expected_ids), k)\n",
        "        idcg = 0.0\n",
        "        for i, rel in enumerate(ideal_relevance):\n",
        "            idcg += rel / np.log2(i + 2)\n",
        "        \n",
        "        return dcg / idcg if idcg > 0 else 0.0\n",
        "    \n",
        "    def calculate_mrr(self, retrieved_ids: List[str], expected_ids: List[str]) -> float:\n",
        "        \"\"\"\n",
        "        Calcula Mean Reciprocal Rank\n",
        "        \n",
        "        Args:\n",
        "            retrieved_ids: IDs de documentos recuperados\n",
        "            expected_ids: IDs de documentos esperados\n",
        "        \n",
        "        Returns:\n",
        "            MRR score\n",
        "        \"\"\"\n",
        "        if not expected_ids:\n",
        "            return 0.0\n",
        "        \n",
        "        expected_set = set(expected_ids)\n",
        "        for i, doc_id in enumerate(retrieved_ids):\n",
        "            if doc_id in expected_set:\n",
        "                return 1.0 / (i + 1)\n",
        "        \n",
        "        return 0.0\n",
        "    \n",
        "    def calculate_hit_rate_at_k(self, retrieved_ids: List[str], expected_ids: List[str], k: int) -> float:\n",
        "        \"\"\"\n",
        "        Calcula Hit Rate@k\n",
        "        \n",
        "        Args:\n",
        "            retrieved_ids: IDs de documentos recuperados\n",
        "            expected_ids: IDs de documentos esperados\n",
        "            k: N√∫mero de documentos a considerar\n",
        "        \n",
        "        Returns:\n",
        "            Hit Rate@k score (0 o 1)\n",
        "        \"\"\"\n",
        "        if not expected_ids:\n",
        "            return 0.0\n",
        "        \n",
        "        retrieved_k = set(retrieved_ids[:k])\n",
        "        expected_set = set(expected_ids)\n",
        "        \n",
        "        intersection = retrieved_k.intersection(expected_set)\n",
        "        return 1.0 if len(intersection) > 0 else 0.0\n",
        "    \n",
        "    def evaluate_single_query(self, query: str, expected_articles: List[str], \n",
        "                            embedding_model_name: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Eval√∫a una sola consulta con un modelo de embedding\n",
        "        \n",
        "        Args:\n",
        "            query: Pregunta a evaluar\n",
        "            expected_articles: Lista de art√≠culos esperados\n",
        "            embedding_model_name: Nombre del modelo de embedding\n",
        "        \n",
        "        Returns:\n",
        "            Diccionario con m√©tricas calculadas\n",
        "        \"\"\"\n",
        "        # Buscar documentos\n",
        "        documents = self.search_documents(query, embedding_model_name)\n",
        "        retrieved_ids = [doc['id'] for doc in documents]\n",
        "        \n",
        "        # Calcular m√©tricas\n",
        "        metrics = {}\n",
        "        \n",
        "        # Recall@k\n",
        "        for k in RETRIEVAL_METRICS['recall_at_k']:\n",
        "            metrics[f'recall_at_{k}'] = self.calculate_recall_at_k(retrieved_ids, expected_articles, k)\n",
        "        \n",
        "        # Precision@k\n",
        "        for k in RETRIEVAL_METRICS['precision_at_k']:\n",
        "            metrics[f'precision_at_{k}'] = self.calculate_precision_at_k(retrieved_ids, expected_articles, k)\n",
        "        \n",
        "        # nDCG@k\n",
        "        for k in RETRIEVAL_METRICS['ndcg_at_k']:\n",
        "            metrics[f'ndcg_at_{k}'] = self.calculate_ndcg_at_k(retrieved_ids, expected_articles, k)\n",
        "        \n",
        "        # MRR\n",
        "        if RETRIEVAL_METRICS['mrr']:\n",
        "            metrics['mrr'] = self.calculate_mrr(retrieved_ids, expected_articles)\n",
        "        \n",
        "        # Hit Rate@k\n",
        "        for k in RETRIEVAL_METRICS['hit_rate_at_k']:\n",
        "            metrics[f'hit_rate_at_{k}'] = self.calculate_hit_rate_at_k(retrieved_ids, expected_articles, k)\n",
        "        \n",
        "        # Informaci√≥n adicional\n",
        "        metrics['total_retrieved'] = len(retrieved_ids)\n",
        "        metrics['total_expected'] = len(expected_articles)\n",
        "        metrics['retrieved_ids'] = retrieved_ids[:10]  # Primeros 10 para debugging\n",
        "        \n",
        "        return metrics\n",
        "\n",
        "# Inicializar evaluador\n",
        "retriever_evaluator = RetrieverEvaluator(\n",
        "    qdrant_client=env_manager.qdrant_client,\n",
        "    embedding_models=env_manager.embedding_models,\n",
        "    collection_name=QDRANT_CONFIG['collection_name'],\n",
        "    top_k=QDRANT_CONFIG['top_k']\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Evaluador de retrievers inicializado\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funci√≥n para ejecutar evaluaci√≥n completa de retrievers\n",
        "def evaluate_all_retrievers(dataset: pd.DataFrame, embedding_models: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Eval√∫a todos los modelos de embedding en el dataset completo\n",
        "    \n",
        "    Args:\n",
        "        dataset: DataFrame con preguntas y art√≠culos esperados\n",
        "        embedding_models: Diccionario con modelos de embedding\n",
        "    \n",
        "    Returns:\n",
        "        Diccionario con resultados de evaluaci√≥n\n",
        "    \"\"\"\n",
        "    print(\"üöÄ Iniciando evaluaci√≥n completa de retrievers...\")\n",
        "    print(f\"üìä Evaluando {len(dataset)} preguntas con {len(embedding_models)} modelos\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    all_results = {}\n",
        "    \n",
        "    for model_name in embedding_models.keys():\n",
        "        print(f\"\\nüîÑ Evaluando modelo: {model_name}\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        model_results = {\n",
        "            'model_name': model_name,\n",
        "            'query_results': [],\n",
        "            'aggregated_metrics': {}\n",
        "        }\n",
        "        \n",
        "        # Evaluar cada pregunta\n",
        "        for idx, row in dataset.iterrows():\n",
        "            query = row['question']\n",
        "            expected_articles = row['expected_articles']\n",
        "            \n",
        "            print(f\"  üìù Pregunta {idx + 1}/{len(dataset)}: {query[:50]}...\")\n",
        "            \n",
        "            try:\n",
        "                # Evaluar consulta\n",
        "                query_metrics = retriever_evaluator.evaluate_single_query(\n",
        "                    query=query,\n",
        "                    expected_articles=expected_articles,\n",
        "                    embedding_model_name=model_name\n",
        "                )\n",
        "                \n",
        "                # Agregar informaci√≥n de la consulta\n",
        "                query_metrics['query'] = query\n",
        "                query_metrics['expected_articles'] = expected_articles\n",
        "                query_metrics['query_id'] = idx\n",
        "                \n",
        "                model_results['query_results'].append(query_metrics)\n",
        "                \n",
        "                # Mostrar m√©tricas principales\n",
        "                print(f\"    üìà Recall@5: {query_metrics['recall_at_5']:.3f}, \"\n",
        "                      f\"Precision@5: {query_metrics['precision_at_5']:.3f}, \"\n",
        "                      f\"nDCG@5: {query_metrics['ndcg_at_5']:.3f}\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"    ‚ùå Error en pregunta {idx + 1}: {e}\")\n",
        "                continue\n",
        "        \n",
        "        # Calcular m√©tricas agregadas\n",
        "        if model_results['query_results']:\n",
        "            print(f\"\\n  üìä Calculando m√©tricas agregadas para {model_name}...\")\n",
        "            aggregated = calculate_aggregated_metrics(model_results['query_results'])\n",
        "            model_results['aggregated_metrics'] = aggregated\n",
        "            \n",
        "            # Mostrar resumen\n",
        "            print(f\"    üéØ Resumen de {model_name}:\")\n",
        "            print(f\"      - Recall@5 promedio: {aggregated['recall_at_5']['mean']:.3f}\")\n",
        "            print(f\"      - Precision@5 promedio: {aggregated['precision_at_5']['mean']:.3f}\")\n",
        "            print(f\"      - nDCG@5 promedio: {aggregated['ndcg_at_5']['mean']:.3f}\")\n",
        "            print(f\"      - MRR promedio: {aggregated['mrr']['mean']:.3f}\")\n",
        "        \n",
        "        all_results[model_name] = model_results\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚úÖ Evaluaci√≥n de retrievers completada\")\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "def calculate_aggregated_metrics(query_results: List[Dict[str, Any]]) -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Calcula m√©tricas agregadas (promedio, desviaci√≥n est√°ndar) de los resultados de consultas\n",
        "    \n",
        "    Args:\n",
        "        query_results: Lista de resultados de consultas individuales\n",
        "    \n",
        "    Returns:\n",
        "        Diccionario con m√©tricas agregadas\n",
        "    \"\"\"\n",
        "    if not query_results:\n",
        "        return {}\n",
        "    \n",
        "    # Obtener todas las m√©tricas disponibles\n",
        "    metric_names = [key for key in query_results[0].keys() \n",
        "                   if key not in ['query', 'expected_articles', 'query_id', 'retrieved_ids', \n",
        "                                 'total_retrieved', 'total_expected']]\n",
        "    \n",
        "    aggregated = {}\n",
        "    \n",
        "    for metric_name in metric_names:\n",
        "        values = [result[metric_name] for result in query_results if metric_name in result]\n",
        "        \n",
        "        if values:\n",
        "            aggregated[metric_name] = {\n",
        "                'mean': np.mean(values),\n",
        "                'std': np.std(values),\n",
        "                'min': np.min(values),\n",
        "                'max': np.max(values),\n",
        "                'median': np.median(values)\n",
        "            }\n",
        "    \n",
        "    return aggregated\n",
        "\n",
        "# Ejecutar evaluaci√≥n de retrievers\n",
        "print(\"üîÑ Iniciando evaluaci√≥n de retrievers...\")\n",
        "retrieval_results = evaluate_all_retrievers(dataset, env_manager.embedding_models)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizaci√≥n de resultados de retrievers\n",
        "def visualize_retrieval_results(retrieval_results: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Visualiza los resultados de evaluaci√≥n de retrievers\n",
        "    \"\"\"\n",
        "    if not retrieval_results:\n",
        "        print(\"‚ùå No hay resultados para visualizar\")\n",
        "        return\n",
        "    \n",
        "    # Preparar datos para visualizaci√≥n\n",
        "    models = list(retrieval_results.keys())\n",
        "    metrics_to_plot = ['recall_at_5', 'precision_at_5', 'ndcg_at_5', 'mrr']\n",
        "    \n",
        "    # Crear figura con subplots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Comparaci√≥n de Modelos de Embedding - M√©tricas de Retrieval', \n",
        "                 fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Colores para cada modelo\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(models)))\n",
        "    \n",
        "    for idx, metric in enumerate(metrics_to_plot):\n",
        "        row = idx // 2\n",
        "        col = idx % 2\n",
        "        \n",
        "        # Extraer datos para la m√©trica\n",
        "        model_names = []\n",
        "        mean_values = []\n",
        "        std_values = []\n",
        "        \n",
        "        for model_name in models:\n",
        "            if metric in retrieval_results[model_name]['aggregated_metrics']:\n",
        "                model_names.append(model_name)\n",
        "                mean_values.append(retrieval_results[model_name]['aggregated_metrics'][metric]['mean'])\n",
        "                std_values.append(retrieval_results[model_name]['aggregated_metrics'][metric]['std'])\n",
        "        \n",
        "        if model_names:\n",
        "            # Crear gr√°fico de barras con barras de error\n",
        "            bars = axes[row, col].bar(model_names, mean_values, yerr=std_values, \n",
        "                                    color=colors[:len(model_names)], alpha=0.7, capsize=5)\n",
        "            axes[row, col].set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
        "            axes[row, col].set_ylabel('Score')\n",
        "            axes[row, col].set_ylim(0, 1)\n",
        "            \n",
        "            # Rotar etiquetas del eje x\n",
        "            axes[row, col].tick_params(axis='x', rotation=45)\n",
        "            \n",
        "            # Agregar valores en las barras\n",
        "            for bar, mean_val in zip(bars, mean_values):\n",
        "                height = bar.get_height()\n",
        "                axes[row, col].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                                  f'{mean_val:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Crear tabla comparativa\n",
        "    print(\"\\nüìä Tabla Comparativa de Modelos:\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Preparar datos para la tabla\n",
        "    comparison_data = []\n",
        "    for model_name in models:\n",
        "        row = {'Modelo': model_name}\n",
        "        aggregated = retrieval_results[model_name]['aggregated_metrics']\n",
        "        \n",
        "        for metric in metrics_to_plot:\n",
        "            if metric in aggregated:\n",
        "                row[metric] = f\"{aggregated[metric]['mean']:.3f} ¬± {aggregated[metric]['std']:.3f}\"\n",
        "            else:\n",
        "                row[metric] = \"N/A\"\n",
        "        \n",
        "        comparison_data.append(row)\n",
        "    \n",
        "    # Crear DataFrame y mostrar\n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    print(comparison_df.to_string(index=False))\n",
        "    \n",
        "    # Identificar mejor modelo\n",
        "    print(\"\\nüèÜ An√°lisis de Mejores Modelos:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    for metric in metrics_to_plot:\n",
        "        best_model = None\n",
        "        best_score = -1\n",
        "        \n",
        "        for model_name in models:\n",
        "            if metric in retrieval_results[model_name]['aggregated_metrics']:\n",
        "                score = retrieval_results[model_name]['aggregated_metrics'][metric]['mean']\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_model = model_name\n",
        "        \n",
        "        if best_model:\n",
        "            print(f\"  {metric.replace('_', ' ').title()}: {best_model} ({best_score:.3f})\")\n",
        "\n",
        "def analyze_retrieval_performance(retrieval_results: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Analiza el rendimiento de los retrievers en detalle\n",
        "    \"\"\"\n",
        "    print(\"\\nüîç An√°lisis Detallado de Rendimiento:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    for model_name, results in retrieval_results.items():\n",
        "        print(f\"\\nüìà Modelo: {model_name}\")\n",
        "        print(\"-\" * 30)\n",
        "        \n",
        "        aggregated = results['aggregated_metrics']\n",
        "        query_results = results['query_results']\n",
        "        \n",
        "        if not aggregated:\n",
        "            print(\"  ‚ùå No hay m√©tricas disponibles\")\n",
        "            continue\n",
        "        \n",
        "        # M√©tricas principales\n",
        "        print(f\"  üéØ M√©tricas Principales:\")\n",
        "        for metric in ['recall_at_5', 'precision_at_5', 'ndcg_at_5', 'mrr']:\n",
        "            if metric in aggregated:\n",
        "                mean_val = aggregated[metric]['mean']\n",
        "                std_val = aggregated[metric]['std']\n",
        "                print(f\"    {metric.replace('_', ' ').title()}: {mean_val:.3f} ¬± {std_val:.3f}\")\n",
        "        \n",
        "        # An√°lisis de consultas\n",
        "        if query_results:\n",
        "            print(f\"  üìù An√°lisis de Consultas:\")\n",
        "            \n",
        "            # Mejores consultas (mayor recall@5)\n",
        "            best_queries = sorted(query_results, key=lambda x: x.get('recall_at_5', 0), reverse=True)[:3]\n",
        "            print(f\"    Mejores consultas (Recall@5):\")\n",
        "            for i, query in enumerate(best_queries, 1):\n",
        "                print(f\"      {i}. {query['query'][:60]}... (Recall@5: {query['recall_at_5']:.3f})\")\n",
        "            \n",
        "            # Peores consultas (menor recall@5)\n",
        "            worst_queries = sorted(query_results, key=lambda x: x.get('recall_at_5', 0))[:3]\n",
        "            print(f\"    Peores consultas (Recall@5):\")\n",
        "            for i, query in enumerate(worst_queries, 1):\n",
        "                print(f\"      {i}. {query['query'][:60]}... (Recall@5: {query['recall_at_5']:.3f})\")\n",
        "\n",
        "# Ejecutar visualizaciones y an√°lisis\n",
        "visualize_retrieval_results(retrieval_results)\n",
        "analyze_retrieval_performance(retrieval_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar resultados de evaluaci√≥n de retrievers\n",
        "def save_retrieval_evaluation_results(retrieval_results: Dict[str, Any], \n",
        "                                    evaluation_name: str = \"retrieval_evaluation\") -> str:\n",
        "    \"\"\"\n",
        "    Guarda los resultados de evaluaci√≥n de retrievers\n",
        "    \n",
        "    Args:\n",
        "        retrieval_results: Resultados de la evaluaci√≥n\n",
        "        evaluation_name: Nombre de la evaluaci√≥n\n",
        "    \n",
        "    Returns:\n",
        "        Ruta del archivo guardado\n",
        "    \"\"\"\n",
        "    # Preparar configuraci√≥n\n",
        "    config = {\n",
        "        'embedding_models': list(EMBEDDING_MODELS.keys()),\n",
        "        'retrieval_metrics': RETRIEVAL_METRICS,\n",
        "        'qdrant_config': QDRANT_CONFIG,\n",
        "        'dataset_size': len(dataset)\n",
        "    }\n",
        "    \n",
        "    # Preparar metadatos\n",
        "    metadata = {\n",
        "        'evaluation_type': 'retrieval',\n",
        "        'description': 'Evaluaci√≥n de modelos de embedding para retrieval en Qdrant',\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'total_queries': len(dataset),\n",
        "        'total_models': len(retrieval_results)\n",
        "    }\n",
        "    \n",
        "    # Guardar evaluaci√≥n\n",
        "    filepath = eval_manager.save_evaluation(\n",
        "        evaluation_name=evaluation_name,\n",
        "        config=config,\n",
        "        results={'retrieval_metrics': retrieval_results},\n",
        "        metadata=metadata\n",
        "    )\n",
        "    \n",
        "    print(f\"üíæ Resultados de evaluaci√≥n de retrievers guardados en: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "# Guardar resultados\n",
        "retrieval_filepath = save_retrieval_evaluation_results(retrieval_results)\n",
        "\n",
        "# Resumen final de la evaluaci√≥n de retrievers\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìã RESUMEN DE EVALUACI√ìN DE RETRIEVERS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if retrieval_results:\n",
        "    # Encontrar el mejor modelo general\n",
        "    best_model = None\n",
        "    best_overall_score = -1\n",
        "    \n",
        "    for model_name, results in retrieval_results.items():\n",
        "        aggregated = results['aggregated_metrics']\n",
        "        if aggregated:\n",
        "            # Calcular score promedio de m√©tricas principales\n",
        "            main_metrics = ['recall_at_5', 'precision_at_5', 'ndcg_at_5', 'mrr']\n",
        "            scores = [aggregated[metric]['mean'] for metric in main_metrics if metric in aggregated]\n",
        "            if scores:\n",
        "                avg_score = np.mean(scores)\n",
        "                if avg_score > best_overall_score:\n",
        "                    best_overall_score = avg_score\n",
        "                    best_model = model_name\n",
        "    \n",
        "    if best_model:\n",
        "        print(f\"üèÜ Mejor modelo general: {best_model}\")\n",
        "        print(f\"   Score promedio: {best_overall_score:.3f}\")\n",
        "    \n",
        "    # Estad√≠sticas por modelo\n",
        "    print(f\"\\nüìä Estad√≠sticas por modelo:\")\n",
        "    for model_name, results in retrieval_results.items():\n",
        "        aggregated = results['aggregated_metrics']\n",
        "        if aggregated and 'recall_at_5' in aggregated:\n",
        "            recall = aggregated['recall_at_5']['mean']\n",
        "            precision = aggregated['precision_at_5']['mean']\n",
        "            ndcg = aggregated['ndcg_at_5']['mean']\n",
        "            mrr = aggregated['mrr']['mean']\n",
        "            \n",
        "            print(f\"  {model_name}:\")\n",
        "            print(f\"    - Recall@5: {recall:.3f}\")\n",
        "            print(f\"    - Precision@5: {precision:.3f}\")\n",
        "            print(f\"    - nDCG@5: {ndcg:.3f}\")\n",
        "            print(f\"    - MRR: {mrr:.3f}\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Evaluaci√≥n completada exitosamente\")\n",
        "    print(f\"üìÅ Resultados guardados en: {retrieval_filepath}\")\n",
        "else:\n",
        "    print(\"‚ùå No se pudieron obtener resultados de evaluaci√≥n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Evaluaci√≥n con Re-ranking\n",
        "\n",
        "## Objetivo de la Evaluaci√≥n\n",
        "\n",
        "Esta secci√≥n eval√∫a c√≥mo los modelos de re-ranking (cross-encoders) mejoran la calidad de los documentos recuperados. Los cross-encoders consideran tanto la consulta como el documento de forma conjunta, lo que les permite hacer predicciones m√°s precisas sobre la relevancia.\n",
        "\n",
        "### üéØ Proceso de Re-ranking\n",
        "\n",
        "1. **Retrieval inicial**: Obtener documentos usando embeddings (bi-encoder)\n",
        "2. **Re-ranking**: Reordenar documentos usando cross-encoder\n",
        "3. **Evaluaci√≥n**: Comparar m√©tricas antes y despu√©s del re-ranking\n",
        "4. **An√°lisis**: Identificar mejoras en relevancia y ranking\n",
        "\n",
        "### üìä M√©tricas de Evaluaci√≥n\n",
        "\n",
        "- **Mejora en Recall@k**: Incremento en recuperaci√≥n de documentos relevantes\n",
        "- **Mejora en nDCG@k**: Mejora en la calidad del ranking\n",
        "- **Mejora en MRR**: Mejora en la posici√≥n del primer documento relevante\n",
        "- **An√°lisis de ranking**: Cambios en el orden de documentos\n",
        "- **Eficiencia**: Tiempo adicional vs. mejora en calidad\n",
        "\n",
        "### üîÑ Modelos de Re-ranking Evaluados\n",
        "\n",
        "- **ms-marco-MiniLM-L-6-v2**: Modelo ligero y r√°pido\n",
        "- **ms-marco-MiniLM-L-12-v2**: Modelo m√°s robusto\n",
        "- **ms-marco-MiniLM-L-2-v2**: Modelo ultra-ligero\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clase para evaluaci√≥n de re-ranking\n",
        "class RerankingEvaluator:\n",
        "    \"\"\"\n",
        "    Eval√∫a la mejora en calidad de documentos mediante re-ranking\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, qdrant_client, embedding_models, reranking_models, collection_name, top_k=20):\n",
        "        self.qdrant_client = qdrant_client\n",
        "        self.embedding_models = embedding_models\n",
        "        self.reranking_models = reranking_models\n",
        "        self.collection_name = collection_name\n",
        "        self.top_k = top_k\n",
        "        self.results = {}\n",
        "    \n",
        "    def retrieve_documents(self, query: str, embedding_model_name: str, top_k: int = None) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Recupera documentos usando embeddings (sin re-ranking)\n",
        "        \n",
        "        Args:\n",
        "            query: Pregunta a buscar\n",
        "            embedding_model_name: Nombre del modelo de embedding\n",
        "            top_k: N√∫mero de documentos a recuperar\n",
        "        \n",
        "        Returns:\n",
        "            Lista de documentos recuperados\n",
        "        \"\"\"\n",
        "        if top_k is None:\n",
        "            top_k = self.top_k\n",
        "        \n",
        "        try:\n",
        "            # Obtener modelo de embedding\n",
        "            model_info = self.embedding_models[embedding_model_name]\n",
        "            model = model_info['model']\n",
        "            \n",
        "            # Generar embedding de la consulta\n",
        "            query_embedding = model.encode([query])[0].tolist()\n",
        "            \n",
        "            # Buscar en Qdrant\n",
        "            search_results = self.qdrant_client.search(\n",
        "                collection_name=self.collection_name,\n",
        "                query_vector=query_embedding,\n",
        "                limit=top_k,\n",
        "                with_payload=True,\n",
        "                with_vectors=False\n",
        "            )\n",
        "            \n",
        "            # Formatear resultados\n",
        "            documents = []\n",
        "            for result in search_results:\n",
        "                documents.append({\n",
        "                    'id': result.id,\n",
        "                    'score': result.score,\n",
        "                    'payload': result.payload\n",
        "                })\n",
        "            \n",
        "            return documents\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error en retrieval con {embedding_model_name}: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def rerank_documents(self, query: str, documents: List[Dict[str, Any]], \n",
        "                        reranking_model_name: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Re-ordena documentos usando un modelo de re-ranking\n",
        "        \n",
        "        Args:\n",
        "            query: Pregunta original\n",
        "            documents: Lista de documentos a re-ordenar\n",
        "            reranking_model_name: Nombre del modelo de re-ranking\n",
        "        \n",
        "        Returns:\n",
        "            Lista de documentos re-ordenados con nuevos scores\n",
        "        \"\"\"\n",
        "        if not documents:\n",
        "            return []\n",
        "        \n",
        "        try:\n",
        "            # Obtener modelo de re-ranking\n",
        "            model_info = self.reranking_models[reranking_model_name]\n",
        "            model = model_info['model']\n",
        "            \n",
        "            # Preparar pares (query, documento) para el cross-encoder\n",
        "            query_doc_pairs = []\n",
        "            for doc in documents:\n",
        "                # Extraer texto del documento para re-ranking\n",
        "                doc_text = self._extract_document_text(doc['payload'])\n",
        "                query_doc_pairs.append((query, doc_text))\n",
        "            \n",
        "            # Calcular scores de relevancia\n",
        "            relevance_scores = model.predict(query_doc_pairs)\n",
        "            \n",
        "            # Crear documentos con nuevos scores\n",
        "            reranked_docs = []\n",
        "            for i, doc in enumerate(documents):\n",
        "                reranked_doc = doc.copy()\n",
        "                reranked_doc['rerank_score'] = float(relevance_scores[i])\n",
        "                reranked_doc['original_score'] = doc['score']\n",
        "                reranked_docs.append(reranked_doc)\n",
        "            \n",
        "            # Ordenar por score de re-ranking\n",
        "            reranked_docs.sort(key=lambda x: x['rerank_score'], reverse=True)\n",
        "            \n",
        "            return reranked_docs\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error en re-ranking con {reranking_model_name}: {e}\")\n",
        "            return documents  # Retornar documentos originales si hay error\n",
        "    \n",
        "    def _extract_document_text(self, payload: Dict[str, Any]) -> str:\n",
        "        \"\"\"\n",
        "        Extrae el texto relevante del payload del documento para re-ranking\n",
        "        \n",
        "        Args:\n",
        "            payload: Payload del documento de Qdrant\n",
        "        \n",
        "        Returns:\n",
        "            Texto del documento para re-ranking\n",
        "        \"\"\"\n",
        "        # Combinar campos relevantes para re-ranking\n",
        "        text_parts = []\n",
        "        \n",
        "        # Agregar cap√≠tulo y art√≠culo (mismo formato que para embedding)\n",
        "        if 'capitulo_descripcion' in payload and 'articulo' in payload:\n",
        "            text_parts.append(f\"{payload['capitulo_descripcion']}: {payload['articulo']}\")\n",
        "        \n",
        "        # Agregar otros campos relevantes si existen\n",
        "        for field in ['titulo', 'libro', 'capitulo']:\n",
        "            if field in payload and payload[field]:\n",
        "                text_parts.append(str(payload[field]))\n",
        "        \n",
        "        return \" | \".join(text_parts) if text_parts else str(payload.get('articulo', ''))\n",
        "    \n",
        "    def evaluate_reranking_improvement(self, query: str, expected_articles: List[str],\n",
        "                                     embedding_model_name: str, reranking_model_name: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Eval√∫a la mejora del re-ranking para una consulta espec√≠fica\n",
        "        \n",
        "        Args:\n",
        "            query: Pregunta a evaluar\n",
        "            expected_articles: Lista de art√≠culos esperados\n",
        "            embedding_model_name: Nombre del modelo de embedding\n",
        "            reranking_model_name: Nombre del modelo de re-ranking\n",
        "        \n",
        "        Returns:\n",
        "            Diccionario con m√©tricas de mejora\n",
        "        \"\"\"\n",
        "        # 1. Retrieval inicial (sin re-ranking)\n",
        "        initial_docs = self.retrieve_documents(query, embedding_model_name)\n",
        "        initial_ids = [doc['id'] for doc in initial_docs]\n",
        "        \n",
        "        # 2. Re-ranking\n",
        "        reranked_docs = self.rerank_documents(query, initial_docs, reranking_model_name)\n",
        "        reranked_ids = [doc['id'] for doc in reranked_docs]\n",
        "        \n",
        "        # 3. Calcular m√©tricas antes y despu√©s\n",
        "        metrics = {}\n",
        "        \n",
        "        # M√©tricas iniciales (sin re-ranking)\n",
        "        for k in RETRIEVAL_METRICS['recall_at_k']:\n",
        "            metrics[f'initial_recall_at_{k}'] = self._calculate_recall_at_k(initial_ids, expected_articles, k)\n",
        "        \n",
        "        for k in RETRIEVAL_METRICS['precision_at_k']:\n",
        "            metrics[f'initial_precision_at_{k}'] = self._calculate_precision_at_k(initial_ids, expected_articles, k)\n",
        "        \n",
        "        for k in RETRIEVAL_METRICS['ndcg_at_k']:\n",
        "            metrics[f'initial_ndcg_at_{k}'] = self._calculate_ndcg_at_k(initial_ids, expected_articles, k)\n",
        "        \n",
        "        if RETRIEVAL_METRICS['mrr']:\n",
        "            metrics['initial_mrr'] = self._calculate_mrr(initial_ids, expected_articles)\n",
        "        \n",
        "        # M√©tricas despu√©s del re-ranking\n",
        "        for k in RETRIEVAL_METRICS['recall_at_k']:\n",
        "            metrics[f'reranked_recall_at_{k}'] = self._calculate_recall_at_k(reranked_ids, expected_articles, k)\n",
        "        \n",
        "        for k in RETRIEVAL_METRICS['precision_at_k']:\n",
        "            metrics[f'reranked_precision_at_{k}'] = self._calculate_precision_at_k(reranked_ids, expected_articles, k)\n",
        "        \n",
        "        for k in RETRIEVAL_METRICS['ndcg_at_k']:\n",
        "            metrics[f'reranked_ndcg_at_{k}'] = self._calculate_ndcg_at_k(reranked_ids, expected_articles, k)\n",
        "        \n",
        "        if RETRIEVAL_METRICS['mrr']:\n",
        "            metrics['reranked_mrr'] = self._calculate_mrr(reranked_ids, expected_articles)\n",
        "        \n",
        "        # Calcular mejoras\n",
        "        for k in RETRIEVAL_METRICS['recall_at_k']:\n",
        "            initial_key = f'initial_recall_at_{k}'\n",
        "            reranked_key = f'reranked_recall_at_{k}'\n",
        "            metrics[f'recall_improvement_at_{k}'] = metrics[reranked_key] - metrics[initial_key]\n",
        "        \n",
        "        for k in RETRIEVAL_METRICS['precision_at_k']:\n",
        "            initial_key = f'initial_precision_at_{k}'\n",
        "            reranked_key = f'reranked_precision_at_{k}'\n",
        "            metrics[f'precision_improvement_at_{k}'] = metrics[reranked_key] - metrics[initial_key]\n",
        "        \n",
        "        for k in RETRIEVAL_METRICS['ndcg_at_k']:\n",
        "            initial_key = f'initial_ndcg_at_{k}'\n",
        "            reranked_key = f'reranked_ndcg_at_{k}'\n",
        "            metrics[f'ndcg_improvement_at_{k}'] = metrics[reranked_key] - metrics[initial_key]\n",
        "        \n",
        "        if RETRIEVAL_METRICS['mrr']:\n",
        "            metrics['mrr_improvement'] = metrics['reranked_mrr'] - metrics['initial_mrr']\n",
        "        \n",
        "        # Informaci√≥n adicional\n",
        "        metrics['total_documents'] = len(initial_docs)\n",
        "        metrics['expected_articles'] = expected_articles\n",
        "        metrics['initial_ranking'] = initial_ids[:10]\n",
        "        metrics['reranked_ranking'] = reranked_ids[:10]\n",
        "        \n",
        "        return metrics\n",
        "    \n",
        "    def _calculate_recall_at_k(self, retrieved_ids: List[str], expected_ids: List[str], k: int) -> float:\n",
        "        \"\"\"Calcula Recall@k\"\"\"\n",
        "        if not expected_ids:\n",
        "            return 0.0\n",
        "        retrieved_k = set(retrieved_ids[:k])\n",
        "        expected_set = set(expected_ids)\n",
        "        intersection = retrieved_k.intersection(expected_set)\n",
        "        return len(intersection) / len(expected_set)\n",
        "    \n",
        "    def _calculate_precision_at_k(self, retrieved_ids: List[str], expected_ids: List[str], k: int) -> float:\n",
        "        \"\"\"Calcula Precision@k\"\"\"\n",
        "        if k == 0:\n",
        "            return 0.0\n",
        "        retrieved_k = set(retrieved_ids[:k])\n",
        "        expected_set = set(expected_ids)\n",
        "        intersection = retrieved_k.intersection(expected_set)\n",
        "        return len(intersection) / k\n",
        "    \n",
        "    def _calculate_ndcg_at_k(self, retrieved_ids: List[str], expected_ids: List[str], k: int) -> float:\n",
        "        \"\"\"Calcula nDCG@k\"\"\"\n",
        "        if not expected_ids or k == 0:\n",
        "            return 0.0\n",
        "        relevance = [1 if doc_id in expected_ids else 0 for doc_id in retrieved_ids[:k]]\n",
        "        dcg = 0.0\n",
        "        for i, rel in enumerate(relevance):\n",
        "            dcg += rel / np.log2(i + 2)\n",
        "        ideal_relevance = [1] * min(len(expected_ids), k)\n",
        "        idcg = 0.0\n",
        "        for i, rel in enumerate(ideal_relevance):\n",
        "            idcg += rel / np.log2(i + 2)\n",
        "        return dcg / idcg if idcg > 0 else 0.0\n",
        "    \n",
        "    def _calculate_mrr(self, retrieved_ids: List[str], expected_ids: List[str]) -> float:\n",
        "        \"\"\"Calcula MRR\"\"\"\n",
        "        if not expected_ids:\n",
        "            return 0.0\n",
        "        expected_set = set(expected_ids)\n",
        "        for i, doc_id in enumerate(retrieved_ids):\n",
        "            if doc_id in expected_set:\n",
        "                return 1.0 / (i + 1)\n",
        "        return 0.0\n",
        "\n",
        "# Inicializar evaluador de re-ranking\n",
        "reranking_evaluator = RerankingEvaluator(\n",
        "    qdrant_client=env_manager.qdrant_client,\n",
        "    embedding_models=env_manager.embedding_models,\n",
        "    reranking_models=env_manager.reranking_models,\n",
        "    collection_name=QDRANT_CONFIG['collection_name'],\n",
        "    top_k=QDRANT_CONFIG['top_k']\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Evaluador de re-ranking inicializado\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funci√≥n para evaluaci√≥n completa de re-ranking\n",
        "def evaluate_all_reranking_combinations(dataset: pd.DataFrame, \n",
        "                                      embedding_models: Dict[str, Any],\n",
        "                                      reranking_models: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Eval√∫a todas las combinaciones de modelos de embedding y re-ranking\n",
        "    \n",
        "    Args:\n",
        "        dataset: DataFrame con preguntas y art√≠culos esperados\n",
        "        embedding_models: Diccionario con modelos de embedding\n",
        "        reranking_models: Diccionario con modelos de re-ranking\n",
        "    \n",
        "    Returns:\n",
        "        Diccionario con resultados de evaluaci√≥n\n",
        "    \"\"\"\n",
        "    print(\"üöÄ Iniciando evaluaci√≥n completa de re-ranking...\")\n",
        "    print(f\"üìä Evaluando {len(dataset)} preguntas\")\n",
        "    print(f\"üîÑ Combinaciones: {len(embedding_models)} embeddings √ó {len(reranking_models)} re-rankers\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    all_results = {}\n",
        "    \n",
        "    # Evaluar cada combinaci√≥n de embedding + re-ranking\n",
        "    for embedding_name in embedding_models.keys():\n",
        "        print(f\"\\nüß† Embedding: {embedding_name}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        embedding_results = {}\n",
        "        \n",
        "        for reranking_name in reranking_models.keys():\n",
        "            print(f\"\\n  üîÑ Re-ranking: {reranking_name}\")\n",
        "            print(\"  \" + \"-\" * 30)\n",
        "            \n",
        "            combination_key = f\"{embedding_name}+{reranking_name}\"\n",
        "            combination_results = {\n",
        "                'embedding_model': embedding_name,\n",
        "                'reranking_model': reranking_name,\n",
        "                'query_results': [],\n",
        "                'aggregated_metrics': {}\n",
        "            }\n",
        "            \n",
        "            # Evaluar cada pregunta\n",
        "            for idx, row in dataset.iterrows():\n",
        "                query = row['question']\n",
        "                expected_articles = row['expected_articles']\n",
        "                \n",
        "                print(f\"    üìù Pregunta {idx + 1}/{len(dataset)}: {query[:40]}...\")\n",
        "                \n",
        "                try:\n",
        "                    # Evaluar mejora del re-ranking\n",
        "                    query_metrics = reranking_evaluator.evaluate_reranking_improvement(\n",
        "                        query=query,\n",
        "                        expected_articles=expected_articles,\n",
        "                        embedding_model_name=embedding_name,\n",
        "                        reranking_model_name=reranking_name\n",
        "                    )\n",
        "                    \n",
        "                    # Agregar informaci√≥n de la consulta\n",
        "                    query_metrics['query'] = query\n",
        "                    query_metrics['expected_articles'] = expected_articles\n",
        "                    query_metrics['query_id'] = idx\n",
        "                    \n",
        "                    combination_results['query_results'].append(query_metrics)\n",
        "                    \n",
        "                    # Mostrar m√©tricas principales de mejora\n",
        "                    recall_improvement = query_metrics.get('recall_improvement_at_5', 0)\n",
        "                    ndcg_improvement = query_metrics.get('ndcg_improvement_at_5', 0)\n",
        "                    mrr_improvement = query_metrics.get('mrr_improvement', 0)\n",
        "                    \n",
        "                    print(f\"      üìà Mejora Recall@5: {recall_improvement:+.3f}, \"\n",
        "                          f\"nDCG@5: {ndcg_improvement:+.3f}, MRR: {mrr_improvement:+.3f}\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"      ‚ùå Error en pregunta {idx + 1}: {e}\")\n",
        "                    continue\n",
        "            \n",
        "            # Calcular m√©tricas agregadas\n",
        "            if combination_results['query_results']:\n",
        "                print(f\"\\n    üìä Calculando m√©tricas agregadas...\")\n",
        "                aggregated = calculate_reranking_aggregated_metrics(combination_results['query_results'])\n",
        "                combination_results['aggregated_metrics'] = aggregated\n",
        "                \n",
        "                # Mostrar resumen de mejoras\n",
        "                print(f\"    üéØ Resumen de mejoras:\")\n",
        "                print(f\"      - Recall@5: {aggregated.get('recall_improvement_at_5', {}).get('mean', 0):+.3f}\")\n",
        "                print(f\"      - Precision@5: {aggregated.get('precision_improvement_at_5', {}).get('mean', 0):+.3f}\")\n",
        "                print(f\"      - nDCG@5: {aggregated.get('ndcg_improvement_at_5', {}).get('mean', 0):+.3f}\")\n",
        "                print(f\"      - MRR: {aggregated.get('mrr_improvement', {}).get('mean', 0):+.3f}\")\n",
        "            \n",
        "            embedding_results[reranking_name] = combination_results\n",
        "        \n",
        "        all_results[embedding_name] = embedding_results\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"‚úÖ Evaluaci√≥n de re-ranking completada\")\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "def calculate_reranking_aggregated_metrics(query_results: List[Dict[str, Any]]) -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Calcula m√©tricas agregadas para evaluaci√≥n de re-ranking\n",
        "    \n",
        "    Args:\n",
        "        query_results: Lista de resultados de consultas individuales\n",
        "    \n",
        "    Returns:\n",
        "        Diccionario con m√©tricas agregadas\n",
        "    \"\"\"\n",
        "    if not query_results:\n",
        "        return {}\n",
        "    \n",
        "    # Obtener todas las m√©tricas disponibles\n",
        "    metric_names = [key for key in query_results[0].keys() \n",
        "                   if key not in ['query', 'expected_articles', 'query_id', 'initial_ranking', \n",
        "                                 'reranked_ranking', 'total_documents']]\n",
        "    \n",
        "    aggregated = {}\n",
        "    \n",
        "    for metric_name in metric_names:\n",
        "        values = [result[metric_name] for result in query_results if metric_name in result]\n",
        "        \n",
        "        if values:\n",
        "            aggregated[metric_name] = {\n",
        "                'mean': np.mean(values),\n",
        "                'std': np.std(values),\n",
        "                'min': np.min(values),\n",
        "                'max': np.max(values),\n",
        "                'median': np.median(values)\n",
        "            }\n",
        "    \n",
        "    return aggregated\n",
        "\n",
        "# Ejecutar evaluaci√≥n de re-ranking\n",
        "print(\"üîÑ Iniciando evaluaci√≥n de re-ranking...\")\n",
        "reranking_results = evaluate_all_reranking_combinations(\n",
        "    dataset, \n",
        "    env_manager.embedding_models, \n",
        "    env_manager.reranking_models\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizaci√≥n de resultados de re-ranking\n",
        "def visualize_reranking_results(reranking_results: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Visualiza los resultados de evaluaci√≥n de re-ranking\n",
        "    \"\"\"\n",
        "    if not reranking_results:\n",
        "        print(\"‚ùå No hay resultados para visualizar\")\n",
        "        return\n",
        "    \n",
        "    # Preparar datos para visualizaci√≥n\n",
        "    combinations = []\n",
        "    improvement_data = []\n",
        "    \n",
        "    for embedding_name, embedding_data in reranking_results.items():\n",
        "        for reranking_name, combination_data in embedding_data.items():\n",
        "            if combination_data['aggregated_metrics']:\n",
        "                combination_key = f\"{embedding_name}\\n+ {reranking_name}\"\n",
        "                combinations.append(combination_key)\n",
        "                \n",
        "                # Extraer mejoras principales\n",
        "                metrics = combination_data['aggregated_metrics']\n",
        "                improvement_data.append({\n",
        "                    'combination': combination_key,\n",
        "                    'recall_improvement': metrics.get('recall_improvement_at_5', {}).get('mean', 0),\n",
        "                    'precision_improvement': metrics.get('precision_improvement_at_5', {}).get('mean', 0),\n",
        "                    'ndcg_improvement': metrics.get('ndcg_improvement_at_5', {}).get('mean', 0),\n",
        "                    'mrr_improvement': metrics.get('mrr_improvement', {}).get('mean', 0)\n",
        "                })\n",
        "    \n",
        "    if not improvement_data:\n",
        "        print(\"‚ùå No hay datos de mejora para visualizar\")\n",
        "        return\n",
        "    \n",
        "    # Crear figura con subplots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('Mejoras por Re-ranking - Comparaci√≥n de Combinaciones', \n",
        "                 fontsize=16, fontweight='bold')\n",
        "    \n",
        "    metrics_to_plot = ['recall_improvement', 'precision_improvement', 'ndcg_improvement', 'mrr_improvement']\n",
        "    metric_titles = ['Recall@5', 'Precision@5', 'nDCG@5', 'MRR']\n",
        "    \n",
        "    # Colores para cada combinaci√≥n\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(combinations)))\n",
        "    \n",
        "    for idx, (metric, title) in enumerate(zip(metrics_to_plot, metric_titles)):\n",
        "        row = idx // 2\n",
        "        col = idx % 2\n",
        "        \n",
        "        # Extraer datos para la m√©trica\n",
        "        values = [data[metric] for data in improvement_data]\n",
        "        combination_labels = [data['combination'] for data in improvement_data]\n",
        "        \n",
        "        # Crear gr√°fico de barras\n",
        "        bars = axes[row, col].bar(range(len(values)), values, color=colors, alpha=0.7)\n",
        "        axes[row, col].set_title(f'Mejora en {title}')\n",
        "        axes[row, col].set_ylabel('Mejora (Œî)')\n",
        "        axes[row, col].set_xticks(range(len(combination_labels)))\n",
        "        axes[row, col].set_xticklabels(combination_labels, rotation=45, ha='right')\n",
        "        \n",
        "        # L√≠nea de referencia en 0\n",
        "        axes[row, col].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "        \n",
        "        # Agregar valores en las barras\n",
        "        for bar, value in zip(bars, values):\n",
        "            height = bar.get_height()\n",
        "            axes[row, col].text(bar.get_x() + bar.get_width()/2., height + (0.001 if height >= 0 else -0.003),\n",
        "                              f'{value:+.3f}', ha='center', va='bottom' if height >= 0 else 'top', fontsize=8)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Crear tabla comparativa de mejoras\n",
        "    print(\"\\nüìä Tabla Comparativa de Mejoras por Re-ranking:\")\n",
        "    print(\"=\" * 100)\n",
        "    \n",
        "    comparison_df = pd.DataFrame(improvement_data)\n",
        "    comparison_df = comparison_df.round(4)\n",
        "    print(comparison_df.to_string(index=False))\n",
        "    \n",
        "    # Identificar mejores combinaciones\n",
        "    print(\"\\nüèÜ Mejores Combinaciones por M√©trica:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for metric, title in zip(metrics_to_plot, metric_titles):\n",
        "        best_idx = comparison_df[metric].idxmax()\n",
        "        best_combination = comparison_df.loc[best_idx, 'combination']\n",
        "        best_value = comparison_df.loc[best_idx, metric]\n",
        "        print(f\"  {title}: {best_combination} ({best_value:+.3f})\")\n",
        "\n",
        "def analyze_reranking_performance(reranking_results: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Analiza el rendimiento del re-ranking en detalle\n",
        "    \"\"\"\n",
        "    print(\"\\nüîç An√°lisis Detallado de Re-ranking:\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for embedding_name, embedding_data in reranking_results.items():\n",
        "        print(f\"\\nüß† Embedding: {embedding_name}\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        for reranking_name, combination_data in embedding_data.items():\n",
        "            print(f\"\\n  üîÑ Re-ranking: {reranking_name}\")\n",
        "            print(\"  \" + \"-\" * 30)\n",
        "            \n",
        "            aggregated = combination_data['aggregated_metrics']\n",
        "            query_results = combination_data['query_results']\n",
        "            \n",
        "            if not aggregated:\n",
        "                print(\"    ‚ùå No hay m√©tricas disponibles\")\n",
        "                continue\n",
        "            \n",
        "            # M√©tricas de mejora principales\n",
        "            print(f\"  üìà Mejoras Promedio:\")\n",
        "            for metric in ['recall_improvement_at_5', 'precision_improvement_at_5', \n",
        "                          'ndcg_improvement_at_5', 'mrr_improvement']:\n",
        "                if metric in aggregated:\n",
        "                    mean_val = aggregated[metric]['mean']\n",
        "                    std_val = aggregated[metric]['std']\n",
        "                    print(f\"    {metric.replace('_improvement', '').replace('_at_5', '@5').title()}: {mean_val:+.3f} ¬± {std_val:.3f}\")\n",
        "            \n",
        "            # An√°lisis de consultas con mayor mejora\n",
        "            if query_results:\n",
        "                print(f\"  üìù Consultas con Mayor Mejora (nDCG@5):\")\n",
        "                best_queries = sorted(query_results, \n",
        "                                    key=lambda x: x.get('ndcg_improvement_at_5', 0), \n",
        "                                    reverse=True)[:3]\n",
        "                for i, query in enumerate(best_queries, 1):\n",
        "                    improvement = query.get('ndcg_improvement_at_5', 0)\n",
        "                    print(f\"    {i}. {query['query'][:50]}... (Mejora: {improvement:+.3f})\")\n",
        "                \n",
        "                # An√°lisis de consultas con peor rendimiento\n",
        "                print(f\"  üìù Consultas con Menor Mejora (nDCG@5):\")\n",
        "                worst_queries = sorted(query_results, \n",
        "                                     key=lambda x: x.get('ndcg_improvement_at_5', 0))[:3]\n",
        "                for i, query in enumerate(worst_queries, 1):\n",
        "                    improvement = query.get('ndcg_improvement_at_5', 0)\n",
        "                    print(f\"    {i}. {query['query'][:50]}... (Mejora: {improvement:+.3f})\")\n",
        "\n",
        "# Ejecutar visualizaciones y an√°lisis\n",
        "visualize_reranking_results(reranking_results)\n",
        "analyze_reranking_performance(reranking_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar resultados de evaluaci√≥n de re-ranking\n",
        "def save_reranking_evaluation_results(reranking_results: Dict[str, Any], \n",
        "                                    evaluation_name: str = \"reranking_evaluation\") -> str:\n",
        "    \"\"\"\n",
        "    Guarda los resultados de evaluaci√≥n de re-ranking\n",
        "    \n",
        "    Args:\n",
        "        reranking_results: Resultados de la evaluaci√≥n\n",
        "        evaluation_name: Nombre de la evaluaci√≥n\n",
        "    \n",
        "    Returns:\n",
        "        Ruta del archivo guardado\n",
        "    \"\"\"\n",
        "    # Preparar configuraci√≥n\n",
        "    config = {\n",
        "        'embedding_models': list(EMBEDDING_MODELS.keys()),\n",
        "        'reranking_models': list(RERANKING_MODELS.keys()),\n",
        "        'retrieval_metrics': RETRIEVAL_METRICS,\n",
        "        'qdrant_config': QDRANT_CONFIG,\n",
        "        'dataset_size': len(dataset)\n",
        "    }\n",
        "    \n",
        "    # Preparar metadatos\n",
        "    metadata = {\n",
        "        'evaluation_type': 'reranking',\n",
        "        'description': 'Evaluaci√≥n de modelos de re-ranking para mejorar relevancia de documentos',\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'total_queries': len(dataset),\n",
        "        'total_combinations': sum(len(embedding_data) for embedding_data in reranking_results.values())\n",
        "    }\n",
        "    \n",
        "    # Guardar evaluaci√≥n\n",
        "    filepath = eval_manager.save_evaluation(\n",
        "        evaluation_name=evaluation_name,\n",
        "        config=config,\n",
        "        results={'reranking_metrics': reranking_results},\n",
        "        metadata=metadata\n",
        "    )\n",
        "    \n",
        "    print(f\"üíæ Resultados de evaluaci√≥n de re-ranking guardados en: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "# Guardar resultados\n",
        "reranking_filepath = save_reranking_evaluation_results(reranking_results)\n",
        "\n",
        "# Resumen final de la evaluaci√≥n de re-ranking\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìã RESUMEN DE EVALUACI√ìN DE RE-RANKING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if reranking_results:\n",
        "    # Encontrar la mejor combinaci√≥n general\n",
        "    best_combination = None\n",
        "    best_overall_improvement = -1\n",
        "    \n",
        "    for embedding_name, embedding_data in reranking_results.items():\n",
        "        for reranking_name, combination_data in embedding_data.items():\n",
        "            aggregated = combination_data['aggregated_metrics']\n",
        "            if aggregated:\n",
        "                # Calcular mejora promedio de m√©tricas principales\n",
        "                main_improvements = ['recall_improvement_at_5', 'precision_improvement_at_5', \n",
        "                                   'ndcg_improvement_at_5', 'mrr_improvement']\n",
        "                improvements = [aggregated[metric]['mean'] for metric in main_improvements \n",
        "                              if metric in aggregated]\n",
        "                if improvements:\n",
        "                    avg_improvement = np.mean(improvements)\n",
        "                    if avg_improvement > best_overall_improvement:\n",
        "                        best_overall_improvement = avg_improvement\n",
        "                        best_combination = f\"{embedding_name} + {reranking_name}\"\n",
        "    \n",
        "    if best_combination:\n",
        "        print(f\"üèÜ Mejor combinaci√≥n general: {best_combination}\")\n",
        "        print(f\"   Mejora promedio: {best_overall_improvement:+.3f}\")\n",
        "    \n",
        "    # Estad√≠sticas por combinaci√≥n\n",
        "    print(f\"\\nüìä Estad√≠sticas por combinaci√≥n:\")\n",
        "    for embedding_name, embedding_data in reranking_results.items():\n",
        "        print(f\"\\n  üß† Embedding: {embedding_name}\")\n",
        "        for reranking_name, combination_data in embedding_data.items():\n",
        "            aggregated = combination_data['aggregated_metrics']\n",
        "            if aggregated and 'ndcg_improvement_at_5' in aggregated:\n",
        "                recall_improvement = aggregated['recall_improvement_at_5']['mean']\n",
        "                precision_improvement = aggregated['precision_improvement_at_5']['mean']\n",
        "                ndcg_improvement = aggregated['ndcg_improvement_at_5']['mean']\n",
        "                mrr_improvement = aggregated['mrr_improvement']['mean']\n",
        "                \n",
        "                print(f\"    üîÑ {reranking_name}:\")\n",
        "                print(f\"      - Recall@5: {recall_improvement:+.3f}\")\n",
        "                print(f\"      - Precision@5: {precision_improvement:+.3f}\")\n",
        "                print(f\"      - nDCG@5: {ndcg_improvement:+.3f}\")\n",
        "                print(f\"      - MRR: {mrr_improvement:+.3f}\")\n",
        "    \n",
        "    # An√°lisis de efectividad del re-ranking\n",
        "    print(f\"\\nüìà An√°lisis de Efectividad del Re-ranking:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    total_combinations = 0\n",
        "    positive_improvements = 0\n",
        "    \n",
        "    for embedding_data in reranking_results.values():\n",
        "        for combination_data in embedding_data.values():\n",
        "            aggregated = combination_data['aggregated_metrics']\n",
        "            if aggregated and 'ndcg_improvement_at_5' in aggregated:\n",
        "                total_combinations += 1\n",
        "                if aggregated['ndcg_improvement_at_5']['mean'] > 0:\n",
        "                    positive_improvements += 1\n",
        "    \n",
        "    if total_combinations > 0:\n",
        "        effectiveness_rate = (positive_improvements / total_combinations) * 100\n",
        "        print(f\"  - Combinaciones que mejoran nDCG@5: {positive_improvements}/{total_combinations} ({effectiveness_rate:.1f}%)\")\n",
        "        \n",
        "        if effectiveness_rate > 50:\n",
        "            print(\"  ‚úÖ El re-ranking es efectivo en la mayor√≠a de combinaciones\")\n",
        "        elif effectiveness_rate > 25:\n",
        "            print(\"  ‚ö†Ô∏è  El re-ranking es moderadamente efectivo\")\n",
        "        else:\n",
        "            print(\"  ‚ùå El re-ranking tiene efectividad limitada\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Evaluaci√≥n de re-ranking completada exitosamente\")\n",
        "    print(f\"üìÅ Resultados guardados en: {reranking_filepath}\")\n",
        "else:\n",
        "    print(\"‚ùå No se pudieron obtener resultados de evaluaci√≥n de re-ranking\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Evaluaci√≥n del Flujo Completo con LLM\n",
        "\n",
        "## Objetivo de la Evaluaci√≥n\n",
        "\n",
        "Esta secci√≥n eval√∫a el pipeline completo de RAG, incluyendo la generaci√≥n de respuestas con LLM y la evaluaci√≥n de calidad subjetiva usando LLM-as-a-judge. Se combinan m√©tricas objetivas de retrieval con m√©tricas subjetivas de calidad de respuesta.\n",
        "\n",
        "### üéØ Proceso de Evaluaci√≥n Completa\n",
        "\n",
        "1. **Retrieval**: Obtener documentos relevantes usando embeddings + Qdrant\n",
        "2. **Re-ranking**: Mejorar relevancia con cross-encoders (opcional)\n",
        "3. **Generaci√≥n**: Crear respuesta usando LLM con contexto recuperado\n",
        "4. **Evaluaci√≥n**: Medir calidad subjetiva con LLM-as-a-judge\n",
        "5. **An√°lisis**: Combinar m√©tricas objetivas y subjetivas\n",
        "\n",
        "### üìä M√©tricas de Evaluaci√≥n\n",
        "\n",
        "#### M√©tricas Objetivas (Retrieval)\n",
        "- **Recall@k**: Proporci√≥n de documentos relevantes recuperados\n",
        "- **Precision@k**: Proporci√≥n de documentos relevantes entre los recuperados\n",
        "- **nDCG@k**: Calidad del ranking de documentos\n",
        "- **MRR**: Posici√≥n del primer documento relevante\n",
        "\n",
        "#### M√©tricas Subjetivas (LLM-as-a-judge)\n",
        "- **Coherencia**: ¬øLa respuesta es coherente y bien estructurada?\n",
        "- **Relevancia**: ¬øLa respuesta es relevante a la pregunta?\n",
        "- **Completitud**: ¬øLa respuesta abarca todos los aspectos necesarios?\n",
        "- **Fidelidad**: ¬øLa respuesta es fiel al contexto proporcionado?\n",
        "- **Concisi√≥n**: ¬øLa respuesta es concisa sin ser incompleta?\n",
        "\n",
        "### ü§ñ Modelos de LLM Evaluados\n",
        "\n",
        "- **OpenAI GPT-3.5-turbo**: Modelo comercial robusto\n",
        "- **Modelo Local**: Fallback para entornos sin API externa\n",
        "- **Configuraci√≥n Flexible**: Temperatura, max_tokens, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clase para evaluaci√≥n completa del pipeline RAG\n",
        "class RAGPipelineEvaluator:\n",
        "    \"\"\"\n",
        "    Eval√∫a el pipeline completo de RAG incluyendo generaci√≥n y evaluaci√≥n con LLM\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, qdrant_client, embedding_models, reranking_models, \n",
        "                 llm_pipeline, collection_name, top_k=20):\n",
        "        self.qdrant_client = qdrant_client\n",
        "        self.embedding_models = embedding_models\n",
        "        self.reranking_models = reranking_models\n",
        "        self.llm_pipeline = llm_pipeline\n",
        "        self.collection_name = collection_name\n",
        "        self.top_k = top_k\n",
        "        self.results = {}\n",
        "    \n",
        "    def retrieve_documents(self, query: str, embedding_model_name: str, \n",
        "                          top_k: int = None) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Recupera documentos usando embeddings\n",
        "        \n",
        "        Args:\n",
        "            query: Pregunta a buscar\n",
        "            embedding_model_name: Nombre del modelo de embedding\n",
        "            top_k: N√∫mero de documentos a recuperar\n",
        "        \n",
        "        Returns:\n",
        "            Lista de documentos recuperados\n",
        "        \"\"\"\n",
        "        if top_k is None:\n",
        "            top_k = self.top_k\n",
        "        \n",
        "        try:\n",
        "            # Obtener modelo de embedding\n",
        "            model_info = self.embedding_models[embedding_model_name]\n",
        "            model = model_info['model']\n",
        "            \n",
        "            # Generar embedding de la consulta\n",
        "            query_embedding = model.encode([query])[0].tolist()\n",
        "            \n",
        "            # Buscar en Qdrant\n",
        "            search_results = self.qdrant_client.search(\n",
        "                collection_name=self.collection_name,\n",
        "                query_vector=query_embedding,\n",
        "                limit=top_k,\n",
        "                with_payload=True,\n",
        "                with_vectors=False\n",
        "            )\n",
        "            \n",
        "            # Formatear resultados\n",
        "            documents = []\n",
        "            for result in search_results:\n",
        "                documents.append({\n",
        "                    'id': result.id,\n",
        "                    'score': result.score,\n",
        "                    'payload': result.payload\n",
        "                })\n",
        "            \n",
        "            return documents\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error en retrieval con {embedding_model_name}: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def rerank_documents(self, query: str, documents: List[Dict[str, Any]], \n",
        "                        reranking_model_name: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Re-ordena documentos usando re-ranking\n",
        "        \n",
        "        Args:\n",
        "            query: Pregunta original\n",
        "            documents: Lista de documentos a re-ordenar\n",
        "            reranking_model_name: Nombre del modelo de re-ranking\n",
        "        \n",
        "        Returns:\n",
        "            Lista de documentos re-ordenados\n",
        "        \"\"\"\n",
        "        if not documents or not reranking_model_name:\n",
        "            return documents\n",
        "        \n",
        "        try:\n",
        "            # Obtener modelo de re-ranking\n",
        "            model_info = self.reranking_models[reranking_model_name]\n",
        "            model = model_info['model']\n",
        "            \n",
        "            # Preparar pares (query, documento) para el cross-encoder\n",
        "            query_doc_pairs = []\n",
        "            for doc in documents:\n",
        "                doc_text = self._extract_document_text(doc['payload'])\n",
        "                query_doc_pairs.append((query, doc_text))\n",
        "            \n",
        "            # Calcular scores de relevancia\n",
        "            relevance_scores = model.predict(query_doc_pairs)\n",
        "            \n",
        "            # Crear documentos con nuevos scores\n",
        "            reranked_docs = []\n",
        "            for i, doc in enumerate(documents):\n",
        "                reranked_doc = doc.copy()\n",
        "                reranked_doc['rerank_score'] = float(relevance_scores[i])\n",
        "                reranked_doc['original_score'] = doc['score']\n",
        "                reranked_docs.append(reranked_doc)\n",
        "            \n",
        "            # Ordenar por score de re-ranking\n",
        "            reranked_docs.sort(key=lambda x: x['rerank_score'], reverse=True)\n",
        "            \n",
        "            return reranked_docs\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error en re-ranking con {reranking_model_name}: {e}\")\n",
        "            return documents\n",
        "    \n",
        "    def _extract_document_text(self, payload: Dict[str, Any]) -> str:\n",
        "        \"\"\"\n",
        "        Extrae el texto relevante del payload del documento\n",
        "        \"\"\"\n",
        "        text_parts = []\n",
        "        \n",
        "        # Agregar cap√≠tulo y art√≠culo\n",
        "        if 'capitulo_descripcion' in payload and 'articulo' in payload:\n",
        "            text_parts.append(f\"{payload['capitulo_descripcion']}: {payload['articulo']}\")\n",
        "        \n",
        "        # Agregar otros campos relevantes\n",
        "        for field in ['titulo', 'libro', 'capitulo']:\n",
        "            if field in payload and payload[field]:\n",
        "                text_parts.append(str(payload[field]))\n",
        "        \n",
        "        return \" | \".join(text_parts) if text_parts else str(payload.get('articulo', ''))\n",
        "    \n",
        "    def generate_response(self, query: str, documents: List[Dict[str, Any]], \n",
        "                         max_context_docs: int = 5) -> str:\n",
        "        \"\"\"\n",
        "        Genera respuesta usando LLM con contexto de documentos\n",
        "        \n",
        "        Args:\n",
        "            query: Pregunta original\n",
        "            documents: Lista de documentos recuperados\n",
        "            max_context_docs: N√∫mero m√°ximo de documentos a usar como contexto\n",
        "        \n",
        "        Returns:\n",
        "            Respuesta generada por el LLM\n",
        "        \"\"\"\n",
        "        if not documents:\n",
        "            return \"No se encontraron documentos relevantes para responder la pregunta.\"\n",
        "        \n",
        "        try:\n",
        "            # Preparar contexto con documentos m√°s relevantes\n",
        "            context_docs = documents[:max_context_docs]\n",
        "            context_text = self._build_context(context_docs)\n",
        "            \n",
        "            # Crear prompt para el LLM\n",
        "            prompt = self._create_prompt(query, context_text)\n",
        "            \n",
        "            # Generar respuesta\n",
        "            if self.llm_pipeline['provider'] == 'openai':\n",
        "                response = self._generate_openai_response(prompt)\n",
        "            else:\n",
        "                response = self._generate_local_response(prompt)\n",
        "            \n",
        "            return response\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error generando respuesta: {e}\")\n",
        "            return f\"Error generando respuesta: {str(e)}\"\n",
        "    \n",
        "    def _build_context(self, documents: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"\n",
        "        Construye el contexto a partir de los documentos\n",
        "        \"\"\"\n",
        "        context_parts = []\n",
        "        \n",
        "        for i, doc in enumerate(documents, 1):\n",
        "            payload = doc['payload']\n",
        "            \n",
        "            # Extraer informaci√≥n del documento\n",
        "            articulo_numero = payload.get('articulo_numero', 'N/A')\n",
        "            capitulo_descripcion = payload.get('capitulo_descripcion', '')\n",
        "            articulo_text = payload.get('articulo', '')\n",
        "            \n",
        "            # Formatear documento\n",
        "            doc_text = f\"Art√≠culo {articulo_numero}: {capitulo_descripcion}\\n{articulo_text}\"\n",
        "            context_parts.append(f\"Documento {i}:\\n{doc_text}\\n\")\n",
        "        \n",
        "        return \"\\n\".join(context_parts)\n",
        "    \n",
        "    def _create_prompt(self, query: str, context: str) -> str:\n",
        "        \"\"\"\n",
        "        Crea el prompt para el LLM\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"Eres un asistente especializado en derecho laboral paraguayo. Responde la pregunta del usuario bas√°ndote √∫nicamente en el contexto proporcionado.\n",
        "\n",
        "CONTEXTO:\n",
        "{context}\n",
        "\n",
        "PREGUNTA: {query}\n",
        "\n",
        "INSTRUCCIONES:\n",
        "- Responde de manera clara y precisa\n",
        "- Basa tu respuesta √∫nicamente en el contexto proporcionado\n",
        "- Si el contexto no contiene informaci√≥n suficiente, ind√≠calo claramente\n",
        "- Cita los art√≠culos espec√≠ficos cuando sea relevante\n",
        "- Mant√©n un tono profesional y t√©cnico apropiado para el √°mbito legal\n",
        "\n",
        "RESPUESTA:\"\"\"\n",
        "        \n",
        "        return prompt\n",
        "    \n",
        "    def _generate_openai_response(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Genera respuesta usando OpenAI\n",
        "        \"\"\"\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=self.llm_pipeline['model'],\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"Eres un asistente especializado en derecho laboral paraguayo.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=self.llm_pipeline['temperature'],\n",
        "                max_tokens=self.llm_pipeline['max_tokens']\n",
        "            )\n",
        "            \n",
        "            return response.choices[0].message.content.strip()\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error con OpenAI: {e}\")\n",
        "            return f\"Error con OpenAI: {str(e)}\"\n",
        "    \n",
        "    def _generate_local_response(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Genera respuesta usando modelo local\n",
        "        \"\"\"\n",
        "        try:\n",
        "            tokenizer = self.llm_pipeline['tokenizer']\n",
        "            model = self.llm_pipeline['model']\n",
        "            \n",
        "            # Tokenizar input\n",
        "            inputs = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "            \n",
        "            # Generar respuesta\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    inputs,\n",
        "                    max_length=inputs.shape[1] + self.llm_pipeline.get('max_tokens', 200),\n",
        "                    temperature=self.llm_pipeline.get('temperature', 0.7),\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "            \n",
        "            # Decodificar respuesta\n",
        "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            \n",
        "            # Extraer solo la parte de la respuesta (despu√©s del prompt)\n",
        "            if \"RESPUESTA:\" in response:\n",
        "                response = response.split(\"RESPUESTA:\")[-1].strip()\n",
        "            \n",
        "            return response\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error con modelo local: {e}\")\n",
        "            return f\"Error con modelo local: {str(e)}\"\n",
        "    \n",
        "    def evaluate_response_quality(self, query: str, generated_response: str, \n",
        "                                 expected_answer: str, context_documents: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Eval√∫a la calidad de la respuesta generada usando LLM-as-a-judge\n",
        "        \n",
        "        Args:\n",
        "            query: Pregunta original\n",
        "            generated_response: Respuesta generada por el LLM\n",
        "            expected_answer: Respuesta esperada (ground truth)\n",
        "            context_documents: Documentos usados como contexto\n",
        "        \n",
        "        Returns:\n",
        "            Diccionario con m√©tricas de calidad\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Crear prompt para evaluaci√≥n\n",
        "            evaluation_prompt = self._create_evaluation_prompt(\n",
        "                query, generated_response, expected_answer, context_documents\n",
        "            )\n",
        "            \n",
        "            # Generar evaluaci√≥n\n",
        "            if self.llm_pipeline['provider'] == 'openai':\n",
        "                evaluation_response = self._generate_openai_response(evaluation_prompt)\n",
        "            else:\n",
        "                evaluation_response = self._generate_local_response(evaluation_prompt)\n",
        "            \n",
        "            # Parsear evaluaci√≥n\n",
        "            quality_scores = self._parse_evaluation_response(evaluation_response)\n",
        "            \n",
        "            return quality_scores\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error evaluando calidad: {e}\")\n",
        "            return self._get_default_quality_scores()\n",
        "    \n",
        "    def _create_evaluation_prompt(self, query: str, generated_response: str, \n",
        "                                 expected_answer: str, context_documents: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"\n",
        "        Crea el prompt para evaluaci√≥n con LLM-as-a-judge\n",
        "        \"\"\"\n",
        "        context_text = self._build_context(context_documents)\n",
        "        \n",
        "        prompt = f\"\"\"Eres un evaluador experto en sistemas de RAG. Eval√∫a la calidad de la respuesta generada seg√∫n los criterios especificados.\n",
        "\n",
        "PREGUNTA: {query}\n",
        "\n",
        "RESPUESTA ESPERADA: {expected_answer}\n",
        "\n",
        "RESPUESTA GENERADA: {generated_response}\n",
        "\n",
        "CONTEXTO USADO: {context_text}\n",
        "\n",
        "CRITERIOS DE EVALUACI√ìN:\n",
        "1. Coherencia (1-5): ¬øLa respuesta es coherente y bien estructurada?\n",
        "2. Relevancia (1-5): ¬øLa respuesta es relevante a la pregunta?\n",
        "3. Completitud (1-5): ¬øLa respuesta abarca todos los aspectos necesarios?\n",
        "4. Fidelidad (1-5): ¬øLa respuesta es fiel al contexto proporcionado?\n",
        "5. Concisi√≥n (1-5): ¬øLa respuesta es concisa sin ser incompleta?\n",
        "\n",
        "EVAL√öA CADA CRITERIO Y RESPONDE EN EL SIGUIENTE FORMATO JSON:\n",
        "{{\n",
        "    \"coherence\": X,\n",
        "    \"relevance\": X,\n",
        "    \"completeness\": X,\n",
        "    \"fidelity\": X,\n",
        "    \"conciseness\": X,\n",
        "    \"overall_score\": X,\n",
        "    \"explanation\": \"Breve explicaci√≥n de la evaluaci√≥n\"\n",
        "}}\n",
        "\n",
        "Donde X es un n√∫mero del 1 al 5 para cada criterio.\"\"\"\n",
        "        \n",
        "        return prompt\n",
        "    \n",
        "    def _parse_evaluation_response(self, evaluation_response: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Parsea la respuesta de evaluaci√≥n del LLM\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Buscar JSON en la respuesta\n",
        "            import re\n",
        "            json_match = re.search(r'\\{.*\\}', evaluation_response, re.DOTALL)\n",
        "            \n",
        "            if json_match:\n",
        "                json_str = json_match.group()\n",
        "                evaluation_data = json.loads(json_str)\n",
        "                \n",
        "                # Validar que tenga las claves necesarias\n",
        "                required_keys = ['coherence', 'relevance', 'completeness', 'fidelity', 'conciseness']\n",
        "                if all(key in evaluation_data for key in required_keys):\n",
        "                    return evaluation_data\n",
        "            \n",
        "            # Si no se puede parsear, usar valores por defecto\n",
        "            return self._get_default_quality_scores()\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error parseando evaluaci√≥n: {e}\")\n",
        "            return self._get_default_quality_scores()\n",
        "    \n",
        "    def _get_default_quality_scores(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Retorna scores por defecto cuando hay error en la evaluaci√≥n\n",
        "        \"\"\"\n",
        "        return {\n",
        "            \"coherence\": 3.0,\n",
        "            \"relevance\": 3.0,\n",
        "            \"completeness\": 3.0,\n",
        "            \"fidelity\": 3.0,\n",
        "            \"conciseness\": 3.0,\n",
        "            \"overall_score\": 3.0,\n",
        "            \"explanation\": \"Error en evaluaci√≥n autom√°tica\"\n",
        "        }\n",
        "    \n",
        "    def evaluate_complete_pipeline(self, query: str, expected_answer: str, \n",
        "                                  expected_articles: List[str], embedding_model_name: str,\n",
        "                                  reranking_model_name: str = None) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Eval√∫a el pipeline completo de RAG\n",
        "        \n",
        "        Args:\n",
        "            query: Pregunta a evaluar\n",
        "            expected_answer: Respuesta esperada\n",
        "            expected_articles: Lista de art√≠culos esperados\n",
        "            embedding_model_name: Nombre del modelo de embedding\n",
        "            reranking_model_name: Nombre del modelo de re-ranking (opcional)\n",
        "        \n",
        "        Returns:\n",
        "            Diccionario con m√©tricas completas\n",
        "        \"\"\"\n",
        "        results = {\n",
        "            'query': query,\n",
        "            'expected_answer': expected_answer,\n",
        "            'expected_articles': expected_articles,\n",
        "            'embedding_model': embedding_model_name,\n",
        "            'reranking_model': reranking_model_name\n",
        "        }\n",
        "        \n",
        "        # 1. Retrieval\n",
        "        documents = self.retrieve_documents(query, embedding_model_name)\n",
        "        results['retrieved_documents'] = len(documents)\n",
        "        results['retrieved_ids'] = [doc['id'] for doc in documents]\n",
        "        \n",
        "        # 2. Re-ranking (opcional)\n",
        "        if reranking_model_name and reranking_model_name in self.reranking_models:\n",
        "            documents = self.rerank_documents(query, documents, reranking_model_name)\n",
        "            results['reranked'] = True\n",
        "        else:\n",
        "            results['reranked'] = False\n",
        "        \n",
        "        # 3. Generaci√≥n de respuesta\n",
        "        generated_response = self.generate_response(query, documents)\n",
        "        results['generated_response'] = generated_response\n",
        "        \n",
        "        # 4. M√©tricas objetivas de retrieval\n",
        "        retrieved_ids = [doc['id'] for doc in documents]\n",
        "        results['retrieval_metrics'] = self._calculate_retrieval_metrics(retrieved_ids, expected_articles)\n",
        "        \n",
        "        # 5. Evaluaci√≥n de calidad de respuesta\n",
        "        quality_scores = self.evaluate_response_quality(\n",
        "            query, generated_response, expected_answer, documents\n",
        "        )\n",
        "        results['quality_scores'] = quality_scores\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def _calculate_retrieval_metrics(self, retrieved_ids: List[str], expected_ids: List[str]) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Calcula m√©tricas objetivas de retrieval\n",
        "        \"\"\"\n",
        "        metrics = {}\n",
        "        \n",
        "        # Recall@k\n",
        "        for k in RETRIEVAL_METRICS['recall_at_k']:\n",
        "            metrics[f'recall_at_{k}'] = self._calculate_recall_at_k(retrieved_ids, expected_ids, k)\n",
        "        \n",
        "        # Precision@k\n",
        "        for k in RETRIEVAL_METRICS['precision_at_k']:\n",
        "            metrics[f'precision_at_{k}'] = self._calculate_precision_at_k(retrieved_ids, expected_ids, k)\n",
        "        \n",
        "        # nDCG@k\n",
        "        for k in RETRIEVAL_METRICS['ndcg_at_k']:\n",
        "            metrics[f'ndcg_at_{k}'] = self._calculate_ndcg_at_k(retrieved_ids, expected_ids, k)\n",
        "        \n",
        "        # MRR\n",
        "        if RETRIEVAL_METRICS['mrr']:\n",
        "            metrics['mrr'] = self._calculate_mrr(retrieved_ids, expected_ids)\n",
        "        \n",
        "        return metrics\n",
        "    \n",
        "    def _calculate_recall_at_k(self, retrieved_ids: List[str], expected_ids: List[str], k: int) -> float:\n",
        "        \"\"\"Calcula Recall@k\"\"\"\n",
        "        if not expected_ids:\n",
        "            return 0.0\n",
        "        retrieved_k = set(retrieved_ids[:k])\n",
        "        expected_set = set(expected_ids)\n",
        "        intersection = retrieved_k.intersection(expected_set)\n",
        "        return len(intersection) / len(expected_set)\n",
        "    \n",
        "    def _calculate_precision_at_k(self, retrieved_ids: List[str], expected_ids: List[str], k: int) -> float:\n",
        "        \"\"\"Calcula Precision@k\"\"\"\n",
        "        if k == 0:\n",
        "            return 0.0\n",
        "        retrieved_k = set(retrieved_ids[:k])\n",
        "        expected_set = set(expected_ids)\n",
        "        intersection = retrieved_k.intersection(expected_set)\n",
        "        return len(intersection) / k\n",
        "    \n",
        "    def _calculate_ndcg_at_k(self, retrieved_ids: List[str], expected_ids: List[str], k: int) -> float:\n",
        "        \"\"\"Calcula nDCG@k\"\"\"\n",
        "        if not expected_ids or k == 0:\n",
        "            return 0.0\n",
        "        relevance = [1 if doc_id in expected_ids else 0 for doc_id in retrieved_ids[:k]]\n",
        "        dcg = 0.0\n",
        "        for i, rel in enumerate(relevance):\n",
        "            dcg += rel / np.log2(i + 2)\n",
        "        ideal_relevance = [1] * min(len(expected_ids), k)\n",
        "        idcg = 0.0\n",
        "        for i, rel in enumerate(ideal_relevance):\n",
        "            idcg += rel / np.log2(i + 2)\n",
        "        return dcg / idcg if idcg > 0 else 0.0\n",
        "    \n",
        "    def _calculate_mrr(self, retrieved_ids: List[str], expected_ids: List[str]) -> float:\n",
        "        \"\"\"Calcula MRR\"\"\"\n",
        "        if not expected_ids:\n",
        "            return 0.0\n",
        "        expected_set = set(expected_ids)\n",
        "        for i, doc_id in enumerate(retrieved_ids):\n",
        "            if doc_id in expected_set:\n",
        "                return 1.0 / (i + 1)\n",
        "        return 0.0\n",
        "\n",
        "# Inicializar evaluador del pipeline completo\n",
        "rag_evaluator = RAGPipelineEvaluator(\n",
        "    qdrant_client=env_manager.qdrant_client,\n",
        "    embedding_models=env_manager.embedding_models,\n",
        "    reranking_models=env_manager.reranking_models,\n",
        "    llm_pipeline=env_manager.llm_pipeline,\n",
        "    collection_name=QDRANT_CONFIG['collection_name'],\n",
        "    top_k=QDRANT_CONFIG['top_k']\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Evaluador del pipeline RAG completo inicializado\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funci√≥n para evaluaci√≥n completa del pipeline RAG\n",
        "def evaluate_complete_rag_pipeline(dataset: pd.DataFrame, \n",
        "                                 embedding_models: Dict[str, Any],\n",
        "                                 reranking_models: Dict[str, Any],\n",
        "                                 use_reranking: bool = True) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Eval√∫a el pipeline completo de RAG con todas las combinaciones\n",
        "    \n",
        "    Args:\n",
        "        dataset: DataFrame con preguntas y respuestas esperadas\n",
        "        embedding_models: Diccionario con modelos de embedding\n",
        "        reranking_models: Diccionario con modelos de re-ranking\n",
        "        use_reranking: Si usar re-ranking o no\n",
        "    \n",
        "    Returns:\n",
        "        Diccionario con resultados de evaluaci√≥n\n",
        "    \"\"\"\n",
        "    print(\"üöÄ Iniciando evaluaci√≥n completa del pipeline RAG...\")\n",
        "    print(f\"üìä Evaluando {len(dataset)} preguntas\")\n",
        "    print(f\"üß† Modelos de embedding: {len(embedding_models)}\")\n",
        "    print(f\"üîÑ Modelos de re-ranking: {len(reranking_models) if use_reranking else 0}\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    all_results = {}\n",
        "    \n",
        "    # Evaluar cada combinaci√≥n de embedding + re-ranking\n",
        "    for embedding_name in embedding_models.keys():\n",
        "        print(f\"\\nüß† Embedding: {embedding_name}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        embedding_results = {}\n",
        "        \n",
        "        # Si no usar re-ranking, evaluar solo con embedding\n",
        "        if not use_reranking:\n",
        "            print(f\"\\n  üìù Evaluando sin re-ranking...\")\n",
        "            print(\"  \" + \"-\" * 30)\n",
        "            \n",
        "            combination_results = {\n",
        "                'embedding_model': embedding_name,\n",
        "                'reranking_model': None,\n",
        "                'query_results': [],\n",
        "                'aggregated_metrics': {}\n",
        "            }\n",
        "            \n",
        "            # Evaluar cada pregunta\n",
        "            for idx, row in dataset.iterrows():\n",
        "                query = row['question']\n",
        "                expected_answer = row['expected_answer']\n",
        "                expected_articles = row['expected_articles']\n",
        "                \n",
        "                print(f\"    üìù Pregunta {idx + 1}/{len(dataset)}: {query[:40]}...\")\n",
        "                \n",
        "                try:\n",
        "                    # Evaluar pipeline completo\n",
        "                    query_metrics = rag_evaluator.evaluate_complete_pipeline(\n",
        "                        query=query,\n",
        "                        expected_answer=expected_answer,\n",
        "                        expected_articles=expected_articles,\n",
        "                        embedding_model_name=embedding_name,\n",
        "                        reranking_model_name=None\n",
        "                    )\n",
        "                    \n",
        "                    # Agregar informaci√≥n de la consulta\n",
        "                    query_metrics['query_id'] = idx\n",
        "                    combination_results['query_results'].append(query_metrics)\n",
        "                    \n",
        "                    # Mostrar m√©tricas principales\n",
        "                    retrieval_metrics = query_metrics.get('retrieval_metrics', {})\n",
        "                    quality_scores = query_metrics.get('quality_scores', {})\n",
        "                    \n",
        "                    print(f\"      üìà Retrieval Recall@5: {retrieval_metrics.get('recall_at_5', 0):.3f}\")\n",
        "                    print(f\"      üéØ Calidad Overall: {quality_scores.get('overall_score', 0):.1f}/5\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"      ‚ùå Error en pregunta {idx + 1}: {e}\")\n",
        "                    continue\n",
        "            \n",
        "            # Calcular m√©tricas agregadas\n",
        "            if combination_results['query_results']:\n",
        "                print(f\"\\n    üìä Calculando m√©tricas agregadas...\")\n",
        "                aggregated = calculate_rag_aggregated_metrics(combination_results['query_results'])\n",
        "                combination_results['aggregated_metrics'] = aggregated\n",
        "                \n",
        "                # Mostrar resumen\n",
        "                print(f\"    üéØ Resumen:\")\n",
        "                print(f\"      - Recall@5 promedio: {aggregated.get('retrieval_recall_at_5', {}).get('mean', 0):.3f}\")\n",
        "                print(f\"      - Calidad promedio: {aggregated.get('quality_overall_score', {}).get('mean', 0):.1f}/5\")\n",
        "            \n",
        "            embedding_results['no_reranking'] = combination_results\n",
        "        \n",
        "        else:\n",
        "            # Evaluar con cada modelo de re-ranking\n",
        "            for reranking_name in reranking_models.keys():\n",
        "                print(f\"\\n  üîÑ Re-ranking: {reranking_name}\")\n",
        "                print(\"  \" + \"-\" * 30)\n",
        "                \n",
        "                combination_results = {\n",
        "                    'embedding_model': embedding_name,\n",
        "                    'reranking_model': reranking_name,\n",
        "                    'query_results': [],\n",
        "                    'aggregated_metrics': {}\n",
        "                }\n",
        "                \n",
        "                # Evaluar cada pregunta\n",
        "                for idx, row in dataset.iterrows():\n",
        "                    query = row['question']\n",
        "                    expected_answer = row['expected_answer']\n",
        "                    expected_articles = row['expected_articles']\n",
        "                    \n",
        "                    print(f\"    üìù Pregunta {idx + 1}/{len(dataset)}: {query[:40]}...\")\n",
        "                    \n",
        "                    try:\n",
        "                        # Evaluar pipeline completo\n",
        "                        query_metrics = rag_evaluator.evaluate_complete_pipeline(\n",
        "                            query=query,\n",
        "                            expected_answer=expected_answer,\n",
        "                            expected_articles=expected_articles,\n",
        "                            embedding_model_name=embedding_name,\n",
        "                            reranking_model_name=reranking_name\n",
        "                        )\n",
        "                        \n",
        "                        # Agregar informaci√≥n de la consulta\n",
        "                        query_metrics['query_id'] = idx\n",
        "                        combination_results['query_results'].append(query_metrics)\n",
        "                        \n",
        "                        # Mostrar m√©tricas principales\n",
        "                        retrieval_metrics = query_metrics.get('retrieval_metrics', {})\n",
        "                        quality_scores = query_metrics.get('quality_scores', {})\n",
        "                        \n",
        "                        print(f\"      üìà Retrieval Recall@5: {retrieval_metrics.get('recall_at_5', 0):.3f}\")\n",
        "                        print(f\"      üéØ Calidad Overall: {quality_scores.get('overall_score', 0):.1f}/5\")\n",
        "                        \n",
        "                    except Exception as e:\n",
        "                        print(f\"      ‚ùå Error en pregunta {idx + 1}: {e}\")\n",
        "                        continue\n",
        "                \n",
        "                # Calcular m√©tricas agregadas\n",
        "                if combination_results['query_results']:\n",
        "                    print(f\"\\n    üìä Calculando m√©tricas agregadas...\")\n",
        "                    aggregated = calculate_rag_aggregated_metrics(combination_results['query_results'])\n",
        "                    combination_results['aggregated_metrics'] = aggregated\n",
        "                    \n",
        "                    # Mostrar resumen\n",
        "                    print(f\"    üéØ Resumen:\")\n",
        "                    print(f\"      - Recall@5 promedio: {aggregated.get('retrieval_recall_at_5', {}).get('mean', 0):.3f}\")\n",
        "                    print(f\"      - Calidad promedio: {aggregated.get('quality_overall_score', {}).get('mean', 0):.1f}/5\")\n",
        "                \n",
        "                embedding_results[reranking_name] = combination_results\n",
        "        \n",
        "        all_results[embedding_name] = embedding_results\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"‚úÖ Evaluaci√≥n del pipeline RAG completada\")\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "def calculate_rag_aggregated_metrics(query_results: List[Dict[str, Any]]) -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Calcula m√©tricas agregadas para evaluaci√≥n del pipeline RAG\n",
        "    \n",
        "    Args:\n",
        "        query_results: Lista de resultados de consultas individuales\n",
        "    \n",
        "    Returns:\n",
        "        Diccionario con m√©tricas agregadas\n",
        "    \"\"\"\n",
        "    if not query_results:\n",
        "        return {}\n",
        "    \n",
        "    # Extraer m√©tricas de retrieval\n",
        "    retrieval_metrics = {}\n",
        "    for result in query_results:\n",
        "        if 'retrieval_metrics' in result:\n",
        "            for metric, value in result['retrieval_metrics'].items():\n",
        "                if f'retrieval_{metric}' not in retrieval_metrics:\n",
        "                    retrieval_metrics[f'retrieval_{metric}'] = []\n",
        "                retrieval_metrics[f'retrieval_{metric}'].append(value)\n",
        "    \n",
        "    # Extraer m√©tricas de calidad\n",
        "    quality_metrics = {}\n",
        "    for result in query_results:\n",
        "        if 'quality_scores' in result:\n",
        "            for metric, value in result['quality_scores'].items():\n",
        "                if metric != 'explanation':  # Excluir explicaci√≥n\n",
        "                    if f'quality_{metric}' not in quality_metrics:\n",
        "                        quality_metrics[f'quality_{metric}'] = []\n",
        "                    quality_metrics[f'quality_{metric}'].append(value)\n",
        "    \n",
        "    # Combinar todas las m√©tricas\n",
        "    all_metrics = {**retrieval_metrics, **quality_metrics}\n",
        "    \n",
        "    # Calcular estad√≠sticas\n",
        "    aggregated = {}\n",
        "    for metric_name, values in all_metrics.items():\n",
        "        if values:\n",
        "            aggregated[metric_name] = {\n",
        "                'mean': np.mean(values),\n",
        "                'std': np.std(values),\n",
        "                'min': np.min(values),\n",
        "                'max': np.max(values),\n",
        "                'median': np.median(values)\n",
        "            }\n",
        "    \n",
        "    return aggregated\n",
        "\n",
        "# Ejecutar evaluaci√≥n completa del pipeline RAG\n",
        "print(\"üîÑ Iniciando evaluaci√≥n completa del pipeline RAG...\")\n",
        "complete_rag_results = evaluate_complete_rag_pipeline(\n",
        "    dataset, \n",
        "    env_manager.embedding_models, \n",
        "    env_manager.reranking_models,\n",
        "    use_reranking=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizaci√≥n de resultados del pipeline RAG completo\n",
        "def visualize_complete_rag_results(rag_results: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Visualiza los resultados de evaluaci√≥n del pipeline RAG completo\n",
        "    \"\"\"\n",
        "    if not rag_results:\n",
        "        print(\"‚ùå No hay resultados para visualizar\")\n",
        "        return\n",
        "    \n",
        "    # Preparar datos para visualizaci√≥n\n",
        "    combinations = []\n",
        "    retrieval_data = []\n",
        "    quality_data = []\n",
        "    \n",
        "    for embedding_name, embedding_data in rag_results.items():\n",
        "        for reranking_name, combination_data in embedding_data.items():\n",
        "            if combination_data['aggregated_metrics']:\n",
        "                combination_key = f\"{embedding_name}\\n+ {reranking_name if reranking_name else 'No Re-ranking'}\"\n",
        "                combinations.append(combination_key)\n",
        "                \n",
        "                # Extraer m√©tricas de retrieval\n",
        "                metrics = combination_data['aggregated_metrics']\n",
        "                retrieval_data.append({\n",
        "                    'combination': combination_key,\n",
        "                    'recall_at_5': metrics.get('retrieval_recall_at_5', {}).get('mean', 0),\n",
        "                    'precision_at_5': metrics.get('retrieval_precision_at_5', {}).get('mean', 0),\n",
        "                    'ndcg_at_5': metrics.get('retrieval_ndcg_at_5', {}).get('mean', 0),\n",
        "                    'mrr': metrics.get('retrieval_mrr', {}).get('mean', 0)\n",
        "                })\n",
        "                \n",
        "                # Extraer m√©tricas de calidad\n",
        "                quality_data.append({\n",
        "                    'combination': combination_key,\n",
        "                    'coherence': metrics.get('quality_coherence', {}).get('mean', 0),\n",
        "                    'relevance': metrics.get('quality_relevance', {}).get('mean', 0),\n",
        "                    'completeness': metrics.get('quality_completeness', {}).get('mean', 0),\n",
        "                    'fidelity': metrics.get('quality_fidelity', {}).get('mean', 0),\n",
        "                    'conciseness': metrics.get('quality_conciseness', {}).get('mean', 0),\n",
        "                    'overall_score': metrics.get('quality_overall_score', {}).get('mean', 0)\n",
        "                })\n",
        "    \n",
        "    if not retrieval_data or not quality_data:\n",
        "        print(\"‚ùå No hay datos suficientes para visualizar\")\n",
        "        return\n",
        "    \n",
        "    # Crear figura con subplots\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
        "    fig.suptitle('Evaluaci√≥n Completa del Pipeline RAG', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Colores para cada combinaci√≥n\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(combinations)))\n",
        "    \n",
        "    # M√©tricas de retrieval\n",
        "    retrieval_metrics = ['recall_at_5', 'precision_at_5', 'ndcg_at_5', 'mrr']\n",
        "    retrieval_titles = ['Recall@5', 'Precision@5', 'nDCG@5', 'MRR']\n",
        "    \n",
        "    for idx, (metric, title) in enumerate(zip(retrieval_metrics, retrieval_titles)):\n",
        "        row = idx // 2\n",
        "        col = idx % 2\n",
        "        \n",
        "        values = [data[metric] for data in retrieval_data]\n",
        "        combination_labels = [data['combination'] for data in retrieval_data]\n",
        "        \n",
        "        bars = axes[row, col].bar(range(len(values)), values, color=colors, alpha=0.7)\n",
        "        axes[row, col].set_title(f'Retrieval - {title}')\n",
        "        axes[row, col].set_ylabel('Score')\n",
        "        axes[row, col].set_ylim(0, 1)\n",
        "        axes[row, col].set_xticks(range(len(combination_labels)))\n",
        "        axes[row, col].set_xticklabels(combination_labels, rotation=45, ha='right')\n",
        "        \n",
        "        # Agregar valores en las barras\n",
        "        for bar, value in zip(bars, values):\n",
        "            height = bar.get_height()\n",
        "            axes[row, col].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                              f'{value:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "    \n",
        "    # M√©tricas de calidad\n",
        "    quality_metrics = ['coherence', 'relevance', 'completeness', 'fidelity', 'conciseness', 'overall_score']\n",
        "    quality_titles = ['Coherencia', 'Relevancia', 'Completitud', 'Fidelidad', 'Concisi√≥n', 'Score General']\n",
        "    \n",
        "    for idx, (metric, title) in enumerate(zip(quality_metrics, quality_titles)):\n",
        "        row = 2\n",
        "        col = idx % 2\n",
        "        \n",
        "        values = [data[metric] for data in quality_data]\n",
        "        combination_labels = [data['combination'] for data in quality_data]\n",
        "        \n",
        "        bars = axes[row, col].bar(range(len(values)), values, color=colors, alpha=0.7)\n",
        "        axes[row, col].set_title(f'Calidad - {title}')\n",
        "        axes[row, col].set_ylabel('Score (1-5)')\n",
        "        axes[row, col].set_ylim(0, 5)\n",
        "        axes[row, col].set_xticks(range(len(combination_labels)))\n",
        "        axes[row, col].set_xticklabels(combination_labels, rotation=45, ha='right')\n",
        "        \n",
        "        # Agregar valores en las barras\n",
        "        for bar, value in zip(bars, values):\n",
        "            height = bar.get_height()\n",
        "            axes[row, col].text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
        "                              f'{value:.1f}', ha='center', va='bottom', fontsize=8)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Crear tabla comparativa\n",
        "    print(\"\\nüìä Tabla Comparativa del Pipeline RAG:\")\n",
        "    print(\"=\" * 120)\n",
        "    \n",
        "    # Combinar datos para la tabla\n",
        "    comparison_data = []\n",
        "    for i, retrieval in enumerate(retrieval_data):\n",
        "        quality = quality_data[i]\n",
        "        row = {\n",
        "            'Combinaci√≥n': retrieval['combination'],\n",
        "            'Recall@5': f\"{retrieval['recall_at_5']:.3f}\",\n",
        "            'Precision@5': f\"{retrieval['precision_at_5']:.3f}\",\n",
        "            'nDCG@5': f\"{retrieval['ndcg_at_5']:.3f}\",\n",
        "            'MRR': f\"{retrieval['mrr']:.3f}\",\n",
        "            'Coherencia': f\"{quality['coherence']:.1f}\",\n",
        "            'Relevancia': f\"{quality['relevance']:.1f}\",\n",
        "            'Completitud': f\"{quality['completeness']:.1f}\",\n",
        "            'Fidelidad': f\"{quality['fidelity']:.1f}\",\n",
        "            'Concisi√≥n': f\"{quality['conciseness']:.1f}\",\n",
        "            'Score General': f\"{quality['overall_score']:.1f}\"\n",
        "        }\n",
        "        comparison_data.append(row)\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    print(comparison_df.to_string(index=False))\n",
        "    \n",
        "    # Identificar mejores combinaciones\n",
        "    print(\"\\nüèÜ Mejores Combinaciones:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Mejor en retrieval\n",
        "    best_retrieval_idx = max(range(len(retrieval_data)), \n",
        "                           key=lambda i: retrieval_data[i]['recall_at_5'])\n",
        "    print(f\"  Mejor Retrieval: {retrieval_data[best_retrieval_idx]['combination']} \"\n",
        "          f\"(Recall@5: {retrieval_data[best_retrieval_idx]['recall_at_5']:.3f})\")\n",
        "    \n",
        "    # Mejor en calidad\n",
        "    best_quality_idx = max(range(len(quality_data)), \n",
        "                          key=lambda i: quality_data[i]['overall_score'])\n",
        "    print(f\"  Mejor Calidad: {quality_data[best_quality_idx]['combination']} \"\n",
        "          f\"(Score: {quality_data[best_quality_idx]['overall_score']:.1f}/5)\")\n",
        "\n",
        "def analyze_complete_rag_performance(rag_results: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Analiza el rendimiento del pipeline RAG completo en detalle\n",
        "    \"\"\"\n",
        "    print(\"\\nüîç An√°lisis Detallado del Pipeline RAG:\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for embedding_name, embedding_data in rag_results.items():\n",
        "        print(f\"\\nüß† Embedding: {embedding_name}\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        for reranking_name, combination_data in embedding_data.items():\n",
        "            print(f\"\\n  üîÑ Re-ranking: {reranking_name if reranking_name else 'No Re-ranking'}\")\n",
        "            print(\"  \" + \"-\" * 30)\n",
        "            \n",
        "            aggregated = combination_data['aggregated_metrics']\n",
        "            query_results = combination_data['query_results']\n",
        "            \n",
        "            if not aggregated:\n",
        "                print(\"    ‚ùå No hay m√©tricas disponibles\")\n",
        "                continue\n",
        "            \n",
        "            # M√©tricas de retrieval\n",
        "            print(f\"  üìà M√©tricas de Retrieval:\")\n",
        "            for metric in ['retrieval_recall_at_5', 'retrieval_precision_at_5', \n",
        "                          'retrieval_ndcg_at_5', 'retrieval_mrr']:\n",
        "                if metric in aggregated:\n",
        "                    mean_val = aggregated[metric]['mean']\n",
        "                    std_val = aggregated[metric]['std']\n",
        "                    print(f\"    {metric.replace('retrieval_', '').replace('_at_5', '@5').title()}: {mean_val:.3f} ¬± {std_val:.3f}\")\n",
        "            \n",
        "            # M√©tricas de calidad\n",
        "            print(f\"  üéØ M√©tricas de Calidad:\")\n",
        "            for metric in ['quality_coherence', 'quality_relevance', 'quality_completeness', \n",
        "                          'quality_fidelity', 'quality_conciseness', 'quality_overall_score']:\n",
        "                if metric in aggregated:\n",
        "                    mean_val = aggregated[metric]['mean']\n",
        "                    std_val = aggregated[metric]['std']\n",
        "                    print(f\"    {metric.replace('quality_', '').title()}: {mean_val:.1f} ¬± {std_val:.1f}\")\n",
        "            \n",
        "            # An√°lisis de respuestas\n",
        "            if query_results:\n",
        "                print(f\"  üìù An√°lisis de Respuestas:\")\n",
        "                \n",
        "                # Mejores respuestas (mayor calidad general)\n",
        "                best_responses = sorted(query_results, \n",
        "                                      key=lambda x: x.get('quality_scores', {}).get('overall_score', 0), \n",
        "                                      reverse=True)[:3]\n",
        "                print(f\"    Mejores respuestas (Calidad General):\")\n",
        "                for i, response in enumerate(best_responses, 1):\n",
        "                    quality = response.get('quality_scores', {})\n",
        "                    overall_score = quality.get('overall_score', 0)\n",
        "                    query_text = response.get('query', '')[:50]\n",
        "                    print(f\"      {i}. {query_text}... (Score: {overall_score:.1f}/5)\")\n",
        "                \n",
        "                # Peores respuestas\n",
        "                worst_responses = sorted(query_results, \n",
        "                                       key=lambda x: x.get('quality_scores', {}).get('overall_score', 0))[:3]\n",
        "                print(f\"    Peores respuestas (Calidad General):\")\n",
        "                for i, response in enumerate(worst_responses, 1):\n",
        "                    quality = response.get('quality_scores', {})\n",
        "                    overall_score = quality.get('overall_score', 0)\n",
        "                    query_text = response.get('query', '')[:50]\n",
        "                    print(f\"      {i}. {query_text}... (Score: {overall_score:.1f}/5)\")\n",
        "\n",
        "# Ejecutar visualizaciones y an√°lisis\n",
        "visualize_complete_rag_results(complete_rag_results)\n",
        "analyze_complete_rag_performance(complete_rag_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar resultados de evaluaci√≥n del pipeline RAG completo\n",
        "def save_complete_rag_evaluation_results(rag_results: Dict[str, Any], \n",
        "                                        evaluation_name: str = \"complete_rag_evaluation\") -> str:\n",
        "    \"\"\"\n",
        "    Guarda los resultados de evaluaci√≥n del pipeline RAG completo\n",
        "    \n",
        "    Args:\n",
        "        rag_results: Resultados de la evaluaci√≥n\n",
        "        evaluation_name: Nombre de la evaluaci√≥n\n",
        "    \n",
        "    Returns:\n",
        "        Ruta del archivo guardado\n",
        "    \"\"\"\n",
        "    # Preparar configuraci√≥n\n",
        "    config = {\n",
        "        'embedding_models': list(EMBEDDING_MODELS.keys()),\n",
        "        'reranking_models': list(RERANKING_MODELS.keys()),\n",
        "        'retrieval_metrics': RETRIEVAL_METRICS,\n",
        "        'llm_evaluation_criteria': LLM_EVALUATION_CRITERIA,\n",
        "        'qdrant_config': QDRANT_CONFIG,\n",
        "        'llm_config': LLM_CONFIG,\n",
        "        'dataset_size': len(dataset)\n",
        "    }\n",
        "    \n",
        "    # Preparar metadatos\n",
        "    metadata = {\n",
        "        'evaluation_type': 'complete_rag',\n",
        "        'description': 'Evaluaci√≥n completa del pipeline RAG incluyendo generaci√≥n y evaluaci√≥n con LLM',\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'total_queries': len(dataset),\n",
        "        'total_combinations': sum(len(embedding_data) for embedding_data in rag_results.values())\n",
        "    }\n",
        "    \n",
        "    # Guardar evaluaci√≥n\n",
        "    filepath = eval_manager.save_evaluation(\n",
        "        evaluation_name=evaluation_name,\n",
        "        config=config,\n",
        "        results={'complete_rag_metrics': rag_results},\n",
        "        metadata=metadata\n",
        "    )\n",
        "    \n",
        "    print(f\"üíæ Resultados de evaluaci√≥n del pipeline RAG completo guardados en: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "# Guardar resultados\n",
        "complete_rag_filepath = save_complete_rag_evaluation_results(complete_rag_results)\n",
        "\n",
        "# Resumen final de la evaluaci√≥n del pipeline RAG completo\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìã RESUMEN DE EVALUACI√ìN DEL PIPELINE RAG COMPLETO\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if complete_rag_results:\n",
        "    # Encontrar la mejor combinaci√≥n general\n",
        "    best_combination = None\n",
        "    best_overall_score = -1\n",
        "    \n",
        "    for embedding_name, embedding_data in complete_rag_results.items():\n",
        "        for reranking_name, combination_data in embedding_data.items():\n",
        "            aggregated = combination_data['aggregated_metrics']\n",
        "            if aggregated:\n",
        "                # Calcular score combinado (retrieval + calidad)\n",
        "                retrieval_score = aggregated.get('retrieval_recall_at_5', {}).get('mean', 0)\n",
        "                quality_score = aggregated.get('quality_overall_score', {}).get('mean', 0)\n",
        "                \n",
        "                # Score combinado (peso 40% retrieval, 60% calidad)\n",
        "                combined_score = (0.4 * retrieval_score) + (0.6 * (quality_score / 5))\n",
        "                \n",
        "                if combined_score > best_overall_score:\n",
        "                    best_overall_score = combined_score\n",
        "                    best_combination = f\"{embedding_name} + {reranking_name if reranking_name else 'No Re-ranking'}\"\n",
        "    \n",
        "    if best_combination:\n",
        "        print(f\"üèÜ Mejor combinaci√≥n general: {best_combination}\")\n",
        "        print(f\"   Score combinado: {best_overall_score:.3f}\")\n",
        "    \n",
        "    # Estad√≠sticas por combinaci√≥n\n",
        "    print(f\"\\nüìä Estad√≠sticas por combinaci√≥n:\")\n",
        "    for embedding_name, embedding_data in complete_rag_results.items():\n",
        "        print(f\"\\n  üß† Embedding: {embedding_name}\")\n",
        "        for reranking_name, combination_data in embedding_data.items():\n",
        "            aggregated = combination_data['aggregated_metrics']\n",
        "            if aggregated:\n",
        "                # M√©tricas de retrieval\n",
        "                recall = aggregated.get('retrieval_recall_at_5', {}).get('mean', 0)\n",
        "                precision = aggregated.get('retrieval_precision_at_5', {}).get('mean', 0)\n",
        "                ndcg = aggregated.get('retrieval_ndcg_at_5', {}).get('mean', 0)\n",
        "                mrr = aggregated.get('retrieval_mrr', {}).get('mean', 0)\n",
        "                \n",
        "                # M√©tricas de calidad\n",
        "                coherence = aggregated.get('quality_coherence', {}).get('mean', 0)\n",
        "                relevance = aggregated.get('quality_relevance', {}).get('mean', 0)\n",
        "                completeness = aggregated.get('quality_completeness', {}).get('mean', 0)\n",
        "                fidelity = aggregated.get('quality_fidelity', {}).get('mean', 0)\n",
        "                conciseness = aggregated.get('quality_conciseness', {}).get('mean', 0)\n",
        "                overall_quality = aggregated.get('quality_overall_score', {}).get('mean', 0)\n",
        "                \n",
        "                print(f\"    üîÑ {reranking_name if reranking_name else 'No Re-ranking'}:\")\n",
        "                print(f\"      üìà Retrieval:\")\n",
        "                print(f\"        - Recall@5: {recall:.3f}\")\n",
        "                print(f\"        - Precision@5: {precision:.3f}\")\n",
        "                print(f\"        - nDCG@5: {ndcg:.3f}\")\n",
        "                print(f\"        - MRR: {mrr:.3f}\")\n",
        "                print(f\"      üéØ Calidad:\")\n",
        "                print(f\"        - Coherencia: {coherence:.1f}/5\")\n",
        "                print(f\"        - Relevancia: {relevance:.1f}/5\")\n",
        "                print(f\"        - Completitud: {completeness:.1f}/5\")\n",
        "                print(f\"        - Fidelidad: {fidelity:.1f}/5\")\n",
        "                print(f\"        - Concisi√≥n: {conciseness:.1f}/5\")\n",
        "                print(f\"        - Score General: {overall_quality:.1f}/5\")\n",
        "    \n",
        "    # An√°lisis de correlaci√≥n entre retrieval y calidad\n",
        "    print(f\"\\nüìà An√°lisis de Correlaci√≥n:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    retrieval_scores = []\n",
        "    quality_scores = []\n",
        "    \n",
        "    for embedding_data in complete_rag_results.values():\n",
        "        for combination_data in embedding_data.values():\n",
        "            aggregated = combination_data['aggregated_metrics']\n",
        "            if aggregated:\n",
        "                retrieval_score = aggregated.get('retrieval_recall_at_5', {}).get('mean', 0)\n",
        "                quality_score = aggregated.get('quality_overall_score', {}).get('mean', 0)\n",
        "                if retrieval_score > 0 and quality_score > 0:\n",
        "                    retrieval_scores.append(retrieval_score)\n",
        "                    quality_scores.append(quality_score)\n",
        "    \n",
        "    if len(retrieval_scores) > 1:\n",
        "        correlation = np.corrcoef(retrieval_scores, quality_scores)[0, 1]\n",
        "        print(f\"  - Correlaci√≥n Retrieval-Calidad: {correlation:.3f}\")\n",
        "        \n",
        "        if correlation > 0.5:\n",
        "            print(\"  ‚úÖ Fuerte correlaci√≥n positiva entre retrieval y calidad\")\n",
        "        elif correlation > 0.2:\n",
        "            print(\"  ‚ö†Ô∏è  Correlaci√≥n moderada entre retrieval y calidad\")\n",
        "        else:\n",
        "            print(\"  ‚ùå Correlaci√≥n d√©bil entre retrieval y calidad\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Evaluaci√≥n del pipeline RAG completo finalizada exitosamente\")\n",
        "    print(f\"üìÅ Resultados guardados en: {complete_rag_filepath}\")\n",
        "else:\n",
        "    print(\"‚ùå No se pudieron obtener resultados de evaluaci√≥n del pipeline RAG completo\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. An√°lisis Comparativo Global\n",
        "\n",
        "## Objetivo del An√°lisis\n",
        "\n",
        "Esta secci√≥n realiza un an√°lisis comparativo integral de todos los resultados obtenidos en las evaluaciones anteriores, combinando m√©tricas de retrieval, re-ranking y calidad de respuesta para identificar patrones, tendencias y recomendaciones finales.\n",
        "\n",
        "### üéØ Proceso de An√°lisis Global\n",
        "\n",
        "1. **Consolidaci√≥n de Resultados**: Integrar datos de todas las evaluaciones\n",
        "2. **An√°lisis Comparativo**: Comparar rendimiento entre modelos y combinaciones\n",
        "3. **Identificaci√≥n de Patrones**: Detectar tendencias y correlaciones\n",
        "4. **Ranking Global**: Clasificar combinaciones por rendimiento general\n",
        "5. **Recomendaciones**: Sugerir mejores configuraciones y mejoras\n",
        "\n",
        "### üìä Dimensiones de An√°lisis\n",
        "\n",
        "#### An√°lisis por Componente\n",
        "- **Embeddings**: Rendimiento de diferentes modelos de embedding\n",
        "- **Re-ranking**: Efectividad de modelos cross-encoder\n",
        "- **LLM**: Calidad de generaci√≥n y evaluaci√≥n\n",
        "- **Pipeline Completo**: Rendimiento end-to-end\n",
        "\n",
        "#### An√°lisis por M√©trica\n",
        "- **Retrieval**: Recall, Precision, nDCG, MRR\n",
        "- **Calidad**: Coherencia, Relevancia, Completitud, Fidelidad, Concisi√≥n\n",
        "- **Eficiencia**: Tiempo de procesamiento, uso de recursos\n",
        "- **Robustez**: Consistencia entre consultas\n",
        "\n",
        "#### An√°lisis por Combinaci√≥n\n",
        "- **Mejores Combinaciones**: Top performers por criterio\n",
        "- **Trade-offs**: Balance entre diferentes m√©tricas\n",
        "- **Escalabilidad**: Rendimiento con diferentes tama√±os de dataset\n",
        "- **Estabilidad**: Consistencia de resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clase para an√°lisis comparativo global\n",
        "class GlobalComparativeAnalyzer:\n",
        "    \"\"\"\n",
        "    Realiza an√°lisis comparativo global de todos los resultados de evaluaci√≥n\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, retrieval_results: Dict[str, Any], \n",
        "                 reranking_results: Dict[str, Any], \n",
        "                 complete_rag_results: Dict[str, Any]):\n",
        "        self.retrieval_results = retrieval_results\n",
        "        self.reranking_results = reranking_results\n",
        "        self.complete_rag_results = complete_rag_results\n",
        "        self.analysis_results = {}\n",
        "    \n",
        "    def consolidate_all_results(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Consolida todos los resultados de las evaluaciones\n",
        "        \n",
        "        Returns:\n",
        "            Diccionario con resultados consolidados\n",
        "        \"\"\"\n",
        "        print(\"üîÑ Consolidando resultados de todas las evaluaciones...\")\n",
        "        \n",
        "        consolidated = {\n",
        "            'retrieval_only': self._consolidate_retrieval_results(),\n",
        "            'reranking_improvements': self._consolidate_reranking_results(),\n",
        "            'complete_pipeline': self._consolidate_complete_rag_results(),\n",
        "            'global_rankings': {},\n",
        "            'insights': {}\n",
        "        }\n",
        "        \n",
        "        # Generar rankings globales\n",
        "        consolidated['global_rankings'] = self._generate_global_rankings(consolidated)\n",
        "        \n",
        "        # Generar insights\n",
        "        consolidated['insights'] = self._generate_insights(consolidated)\n",
        "        \n",
        "        print(\"‚úÖ Consolidaci√≥n completada\")\n",
        "        return consolidated\n",
        "    \n",
        "    def _consolidate_retrieval_results(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Consolida resultados de evaluaci√≥n de retrievers\n",
        "        \"\"\"\n",
        "        if not self.retrieval_results:\n",
        "            return {}\n",
        "        \n",
        "        consolidated = {}\n",
        "        \n",
        "        for model_name, model_data in self.retrieval_results.items():\n",
        "            if 'aggregated_metrics' in model_data:\n",
        "                metrics = model_data['aggregated_metrics']\n",
        "                consolidated[model_name] = {\n",
        "                    'recall_at_5': metrics.get('recall_at_5', {}).get('mean', 0),\n",
        "                    'precision_at_5': metrics.get('precision_at_5', {}).get('mean', 0),\n",
        "                    'ndcg_at_5': metrics.get('ndcg_at_5', {}).get('mean', 0),\n",
        "                    'mrr': metrics.get('mrr', {}).get('mean', 0),\n",
        "                    'total_queries': len(model_data.get('query_results', []))\n",
        "                }\n",
        "        \n",
        "        return consolidated\n",
        "    \n",
        "    def _consolidate_reranking_results(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Consolida resultados de evaluaci√≥n de re-ranking\n",
        "        \"\"\"\n",
        "        if not self.reranking_results:\n",
        "            return {}\n",
        "        \n",
        "        consolidated = {}\n",
        "        \n",
        "        for embedding_name, embedding_data in self.reranking_results.items():\n",
        "            for reranking_name, combination_data in embedding_data.items():\n",
        "                if 'aggregated_metrics' in combination_data:\n",
        "                    metrics = combination_data['aggregated_metrics']\n",
        "                    key = f\"{embedding_name}+{reranking_name}\"\n",
        "                    consolidated[key] = {\n",
        "                        'embedding_model': embedding_name,\n",
        "                        'reranking_model': reranking_name,\n",
        "                        'recall_improvement': metrics.get('recall_improvement_at_5', {}).get('mean', 0),\n",
        "                        'precision_improvement': metrics.get('precision_improvement_at_5', {}).get('mean', 0),\n",
        "                        'ndcg_improvement': metrics.get('ndcg_improvement_at_5', {}).get('mean', 0),\n",
        "                        'mrr_improvement': metrics.get('mrr_improvement', {}).get('mean', 0),\n",
        "                        'total_queries': len(combination_data.get('query_results', []))\n",
        "                    }\n",
        "        \n",
        "        return consolidated\n",
        "    \n",
        "    def _consolidate_complete_rag_results(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Consolida resultados de evaluaci√≥n del pipeline completo\n",
        "        \"\"\"\n",
        "        if not self.complete_rag_results:\n",
        "            return {}\n",
        "        \n",
        "        consolidated = {}\n",
        "        \n",
        "        for embedding_name, embedding_data in self.complete_rag_results.items():\n",
        "            for reranking_name, combination_data in embedding_data.items():\n",
        "                if 'aggregated_metrics' in combination_data:\n",
        "                    metrics = combination_data['aggregated_metrics']\n",
        "                    key = f\"{embedding_name}+{reranking_name if reranking_name else 'NoReranking'}\"\n",
        "                    consolidated[key] = {\n",
        "                        'embedding_model': embedding_name,\n",
        "                        'reranking_model': reranking_name,\n",
        "                        'retrieval_recall_at_5': metrics.get('retrieval_recall_at_5', {}).get('mean', 0),\n",
        "                        'retrieval_precision_at_5': metrics.get('retrieval_precision_at_5', {}).get('mean', 0),\n",
        "                        'retrieval_ndcg_at_5': metrics.get('retrieval_ndcg_at_5', {}).get('mean', 0),\n",
        "                        'retrieval_mrr': metrics.get('retrieval_mrr', {}).get('mean', 0),\n",
        "                        'quality_coherence': metrics.get('quality_coherence', {}).get('mean', 0),\n",
        "                        'quality_relevance': metrics.get('quality_relevance', {}).get('mean', 0),\n",
        "                        'quality_completeness': metrics.get('quality_completeness', {}).get('mean', 0),\n",
        "                        'quality_fidelity': metrics.get('quality_fidelity', {}).get('mean', 0),\n",
        "                        'quality_conciseness': metrics.get('quality_conciseness', {}).get('mean', 0),\n",
        "                        'quality_overall_score': metrics.get('quality_overall_score', {}).get('mean', 0),\n",
        "                        'total_queries': len(combination_data.get('query_results', []))\n",
        "                    }\n",
        "        \n",
        "        return consolidated\n",
        "    \n",
        "    def _generate_global_rankings(self, consolidated: Dict[str, Any]) -> Dict[str, List[str]]:\n",
        "        \"\"\"\n",
        "        Genera rankings globales por diferentes criterios\n",
        "        \"\"\"\n",
        "        rankings = {}\n",
        "        \n",
        "        # Ranking por retrieval (solo embeddings)\n",
        "        if 'retrieval_only' in consolidated:\n",
        "            retrieval_data = consolidated['retrieval_only']\n",
        "            rankings['best_retrieval'] = sorted(\n",
        "                retrieval_data.keys(),\n",
        "                key=lambda x: retrieval_data[x]['recall_at_5'],\n",
        "                reverse=True\n",
        "            )\n",
        "        \n",
        "        # Ranking por mejoras de re-ranking\n",
        "        if 'reranking_improvements' in consolidated:\n",
        "            reranking_data = consolidated['reranking_improvements']\n",
        "            rankings['best_reranking_improvement'] = sorted(\n",
        "                reranking_data.keys(),\n",
        "                key=lambda x: reranking_data[x]['ndcg_improvement'],\n",
        "                reverse=True\n",
        "            )\n",
        "        \n",
        "        # Ranking por pipeline completo (score combinado)\n",
        "        if 'complete_pipeline' in consolidated:\n",
        "            pipeline_data = consolidated['complete_pipeline']\n",
        "            rankings['best_complete_pipeline'] = sorted(\n",
        "                pipeline_data.keys(),\n",
        "                key=lambda x: self._calculate_combined_score(pipeline_data[x]),\n",
        "                reverse=True\n",
        "            )\n",
        "        \n",
        "        # Ranking por calidad de respuesta\n",
        "        if 'complete_pipeline' in consolidated:\n",
        "            pipeline_data = consolidated['complete_pipeline']\n",
        "            rankings['best_quality'] = sorted(\n",
        "                pipeline_data.keys(),\n",
        "                key=lambda x: pipeline_data[x]['quality_overall_score'],\n",
        "                reverse=True\n",
        "            )\n",
        "        \n",
        "        return rankings\n",
        "    \n",
        "    def _calculate_combined_score(self, metrics: Dict[str, Any]) -> float:\n",
        "        \"\"\"\n",
        "        Calcula score combinado (40% retrieval + 60% calidad)\n",
        "        \"\"\"\n",
        "        retrieval_score = metrics.get('retrieval_recall_at_5', 0)\n",
        "        quality_score = metrics.get('quality_overall_score', 0) / 5  # Normalizar a 0-1\n",
        "        \n",
        "        return (0.4 * retrieval_score) + (0.6 * quality_score)\n",
        "    \n",
        "    def _generate_insights(self, consolidated: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Genera insights y an√°lisis de los resultados\n",
        "        \"\"\"\n",
        "        insights = {\n",
        "            'retrieval_insights': self._analyze_retrieval_patterns(consolidated),\n",
        "            'reranking_insights': self._analyze_reranking_patterns(consolidated),\n",
        "            'quality_insights': self._analyze_quality_patterns(consolidated),\n",
        "            'correlation_insights': self._analyze_correlations(consolidated),\n",
        "            'recommendations': self._generate_recommendations(consolidated)\n",
        "        }\n",
        "        \n",
        "        return insights\n",
        "    \n",
        "    def _analyze_retrieval_patterns(self, consolidated: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analiza patrones en los resultados de retrieval\n",
        "        \"\"\"\n",
        "        if 'retrieval_only' not in consolidated:\n",
        "            return {}\n",
        "        \n",
        "        retrieval_data = consolidated['retrieval_only']\n",
        "        \n",
        "        # Encontrar mejor y peor modelo\n",
        "        best_model = max(retrieval_data.keys(), key=lambda x: retrieval_data[x]['recall_at_5'])\n",
        "        worst_model = min(retrieval_data.keys(), key=lambda x: retrieval_data[x]['recall_at_5'])\n",
        "        \n",
        "        # Calcular estad√≠sticas\n",
        "        recall_scores = [data['recall_at_5'] for data in retrieval_data.values()]\n",
        "        precision_scores = [data['precision_at_5'] for data in retrieval_data.values()]\n",
        "        ndcg_scores = [data['ndcg_at_5'] for data in retrieval_data.values()]\n",
        "        \n",
        "        return {\n",
        "            'best_model': best_model,\n",
        "            'worst_model': worst_model,\n",
        "            'recall_range': (min(recall_scores), max(recall_scores)),\n",
        "            'precision_range': (min(precision_scores), max(precision_scores)),\n",
        "            'ndcg_range': (min(ndcg_scores), max(ndcg_scores)),\n",
        "            'recall_std': np.std(recall_scores),\n",
        "            'precision_std': np.std(precision_scores),\n",
        "            'ndcg_std': np.std(ndcg_scores)\n",
        "        }\n",
        "    \n",
        "    def _analyze_reranking_patterns(self, consolidated: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analiza patrones en los resultados de re-ranking\n",
        "        \"\"\"\n",
        "        if 'reranking_improvements' not in consolidated:\n",
        "            return {}\n",
        "        \n",
        "        reranking_data = consolidated['reranking_improvements']\n",
        "        \n",
        "        # Calcular mejoras promedio por modelo de re-ranking\n",
        "        reranking_models = {}\n",
        "        for key, data in reranking_data.items():\n",
        "            model_name = data['reranking_model']\n",
        "            if model_name not in reranking_models:\n",
        "                reranking_models[model_name] = []\n",
        "            reranking_models[model_name].append(data['ndcg_improvement'])\n",
        "        \n",
        "        # Calcular estad√≠sticas por modelo\n",
        "        model_stats = {}\n",
        "        for model_name, improvements in reranking_models.items():\n",
        "            model_stats[model_name] = {\n",
        "                'mean_improvement': np.mean(improvements),\n",
        "                'std_improvement': np.std(improvements),\n",
        "                'positive_improvements': sum(1 for x in improvements if x > 0),\n",
        "                'total_combinations': len(improvements)\n",
        "            }\n",
        "        \n",
        "        # Encontrar mejor modelo de re-ranking\n",
        "        best_reranking_model = max(model_stats.keys(), \n",
        "                                 key=lambda x: model_stats[x]['mean_improvement'])\n",
        "        \n",
        "        return {\n",
        "            'best_reranking_model': best_reranking_model,\n",
        "            'model_stats': model_stats,\n",
        "            'overall_effectiveness': sum(1 for data in reranking_data.values() \n",
        "                                       if data['ndcg_improvement'] > 0) / len(reranking_data)\n",
        "        }\n",
        "    \n",
        "    def _analyze_quality_patterns(self, consolidated: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analiza patrones en los resultados de calidad\n",
        "        \"\"\"\n",
        "        if 'complete_pipeline' not in consolidated:\n",
        "            return {}\n",
        "        \n",
        "        pipeline_data = consolidated['complete_pipeline']\n",
        "        \n",
        "        # Calcular estad√≠sticas de calidad\n",
        "        quality_metrics = ['quality_coherence', 'quality_relevance', 'quality_completeness', \n",
        "                          'quality_fidelity', 'quality_conciseness', 'quality_overall_score']\n",
        "        \n",
        "        quality_stats = {}\n",
        "        for metric in quality_metrics:\n",
        "            values = [data[metric] for data in pipeline_data.values() if metric in data]\n",
        "            if values:\n",
        "                quality_stats[metric] = {\n",
        "                    'mean': np.mean(values),\n",
        "                    'std': np.std(values),\n",
        "                    'min': np.min(values),\n",
        "                    'max': np.max(values)\n",
        "                }\n",
        "        \n",
        "        # Encontrar mejor combinaci√≥n por calidad\n",
        "        best_quality_combo = max(pipeline_data.keys(), \n",
        "                               key=lambda x: pipeline_data[x]['quality_overall_score'])\n",
        "        \n",
        "        return {\n",
        "            'best_quality_combo': best_quality_combo,\n",
        "            'quality_stats': quality_stats,\n",
        "            'overall_quality_mean': quality_stats.get('quality_overall_score', {}).get('mean', 0)\n",
        "        }\n",
        "    \n",
        "    def _analyze_correlations(self, consolidated: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analiza correlaciones entre diferentes m√©tricas\n",
        "        \"\"\"\n",
        "        if 'complete_pipeline' not in consolidated:\n",
        "            return {}\n",
        "        \n",
        "        pipeline_data = consolidated['complete_pipeline']\n",
        "        \n",
        "        # Extraer m√©tricas para correlaci√≥n\n",
        "        retrieval_scores = []\n",
        "        quality_scores = []\n",
        "        coherence_scores = []\n",
        "        relevance_scores = []\n",
        "        \n",
        "        for data in pipeline_data.values():\n",
        "            retrieval_scores.append(data['retrieval_recall_at_5'])\n",
        "            quality_scores.append(data['quality_overall_score'])\n",
        "            coherence_scores.append(data['quality_coherence'])\n",
        "            relevance_scores.append(data['quality_relevance'])\n",
        "        \n",
        "        # Calcular correlaciones\n",
        "        correlations = {}\n",
        "        if len(retrieval_scores) > 1:\n",
        "            correlations['retrieval_quality'] = np.corrcoef(retrieval_scores, quality_scores)[0, 1]\n",
        "            correlations['retrieval_coherence'] = np.corrcoef(retrieval_scores, coherence_scores)[0, 1]\n",
        "            correlations['retrieval_relevance'] = np.corrcoef(retrieval_scores, relevance_scores)[0, 1]\n",
        "            correlations['coherence_relevance'] = np.corrcoef(coherence_scores, relevance_scores)[0, 1]\n",
        "        \n",
        "        return correlations\n",
        "    \n",
        "    def _generate_recommendations(self, consolidated: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Genera recomendaciones basadas en el an√°lisis\n",
        "        \"\"\"\n",
        "        recommendations = {\n",
        "            'best_overall_config': None,\n",
        "            'best_retrieval_config': None,\n",
        "            'best_quality_config': None,\n",
        "            'reranking_recommendation': None,\n",
        "            'improvement_suggestions': []\n",
        "        }\n",
        "        \n",
        "        # Mejor configuraci√≥n general\n",
        "        if 'global_rankings' in consolidated and 'best_complete_pipeline' in consolidated['global_rankings']:\n",
        "            best_config = consolidated['global_rankings']['best_complete_pipeline'][0]\n",
        "            recommendations['best_overall_config'] = best_config\n",
        "        \n",
        "        # Mejor configuraci√≥n de retrieval\n",
        "        if 'global_rankings' in consolidated and 'best_retrieval' in consolidated['global_rankings']:\n",
        "            best_retrieval = consolidated['global_rankings']['best_retrieval'][0]\n",
        "            recommendations['best_retrieval_config'] = best_retrieval\n",
        "        \n",
        "        # Mejor configuraci√≥n de calidad\n",
        "        if 'global_rankings' in consolidated and 'best_quality' in consolidated['global_rankings']:\n",
        "            best_quality = consolidated['global_rankings']['best_quality'][0]\n",
        "            recommendations['best_quality_config'] = best_quality\n",
        "        \n",
        "        # Recomendaci√≥n de re-ranking\n",
        "        if 'reranking_insights' in consolidated['insights']:\n",
        "            reranking_insights = consolidated['insights']['reranking_insights']\n",
        "            if reranking_insights.get('overall_effectiveness', 0) > 0.5:\n",
        "                recommendations['reranking_recommendation'] = f\"Usar re-ranking con {reranking_insights['best_reranking_model']}\"\n",
        "            else:\n",
        "                recommendations['reranking_recommendation'] = \"Re-ranking no muestra mejoras significativas\"\n",
        "        \n",
        "        # Sugerencias de mejora\n",
        "        suggestions = []\n",
        "        \n",
        "        # Analizar correlaciones\n",
        "        if 'correlation_insights' in consolidated['insights']:\n",
        "            correlations = consolidated['insights']['correlation_insights']\n",
        "            if correlations.get('retrieval_quality', 0) < 0.3:\n",
        "                suggestions.append(\"Mejorar la calidad de retrieval para mejorar respuestas\")\n",
        "            if correlations.get('coherence_relevance', 0) < 0.5:\n",
        "                suggestions.append(\"Trabajar en la coherencia y relevancia de las respuestas\")\n",
        "        \n",
        "        recommendations['improvement_suggestions'] = suggestions\n",
        "        \n",
        "        return recommendations\n",
        "\n",
        "# Inicializar analizador global\n",
        "global_analyzer = GlobalComparativeAnalyzer(\n",
        "    retrieval_results=retrieval_results,\n",
        "    reranking_results=reranking_results,\n",
        "    complete_rag_results=complete_rag_results\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Analizador comparativo global inicializado\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejecutar an√°lisis comparativo global\n",
        "print(\"üîÑ Iniciando an√°lisis comparativo global...\")\n",
        "global_analysis = global_analyzer.consolidate_all_results()\n",
        "\n",
        "# Visualizaci√≥n del an√°lisis global\n",
        "def visualize_global_analysis(global_analysis: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Visualiza el an√°lisis comparativo global\n",
        "    \"\"\"\n",
        "    if not global_analysis:\n",
        "        print(\"‚ùå No hay datos para visualizar\")\n",
        "        return\n",
        "    \n",
        "    # Crear figura con m√∫ltiples subplots\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "    fig.suptitle('An√°lisis Comparativo Global - Pipeline RAG', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Ranking de modelos de embedding (retrieval)\n",
        "    if 'retrieval_only' in global_analysis and global_analysis['retrieval_only']:\n",
        "        retrieval_data = global_analysis['retrieval_only']\n",
        "        models = list(retrieval_data.keys())\n",
        "        recall_scores = [retrieval_data[model]['recall_at_5'] for model in models]\n",
        "        \n",
        "        axes[0, 0].bar(models, recall_scores, color='skyblue', alpha=0.7)\n",
        "        axes[0, 0].set_title('Ranking de Embeddings (Recall@5)')\n",
        "        axes[0, 0].set_ylabel('Recall@5')\n",
        "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        # Agregar valores en las barras\n",
        "        for i, v in enumerate(recall_scores):\n",
        "            axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    # 2. Mejoras de re-ranking\n",
        "    if 'reranking_improvements' in global_analysis and global_analysis['reranking_improvements']:\n",
        "        reranking_data = global_analysis['reranking_improvements']\n",
        "        combinations = list(reranking_data.keys())\n",
        "        improvements = [reranking_data[combo]['ndcg_improvement'] for combo in combinations]\n",
        "        \n",
        "        axes[0, 1].bar(range(len(combinations)), improvements, color='lightgreen', alpha=0.7)\n",
        "        axes[0, 1].set_title('Mejoras de Re-ranking (nDCG@5)')\n",
        "        axes[0, 1].set_ylabel('Mejora nDCG@5')\n",
        "        axes[0, 1].set_xticks(range(len(combinations)))\n",
        "        axes[0, 1].set_xticklabels(combinations, rotation=45, ha='right')\n",
        "        axes[0, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "        \n",
        "        # Agregar valores en las barras\n",
        "        for i, v in enumerate(improvements):\n",
        "            axes[0, 1].text(i, v + (0.001 if v >= 0 else -0.003), f'{v:+.3f}', \n",
        "                           ha='center', va='bottom' if v >= 0 else 'top', fontsize=8)\n",
        "    \n",
        "    # 3. Calidad de respuestas\n",
        "    if 'complete_pipeline' in global_analysis and global_analysis['complete_pipeline']:\n",
        "        pipeline_data = global_analysis['complete_pipeline']\n",
        "        combinations = list(pipeline_data.keys())\n",
        "        quality_scores = [pipeline_data[combo]['quality_overall_score'] for combo in combinations]\n",
        "        \n",
        "        axes[0, 2].bar(range(len(combinations)), quality_scores, color='orange', alpha=0.7)\n",
        "        axes[0, 2].set_title('Calidad de Respuestas (Score General)')\n",
        "        axes[0, 2].set_ylabel('Score (1-5)')\n",
        "        axes[0, 2].set_xticks(range(len(combinations)))\n",
        "        axes[0, 2].set_xticklabels(combinations, rotation=45, ha='right')\n",
        "        axes[0, 2].set_ylim(0, 5)\n",
        "        \n",
        "        # Agregar valores en las barras\n",
        "        for i, v in enumerate(quality_scores):\n",
        "            axes[0, 2].text(i, v + 0.05, f'{v:.1f}', ha='center', va='bottom')\n",
        "    \n",
        "    # 4. Score combinado (retrieval + calidad)\n",
        "    if 'complete_pipeline' in global_analysis and global_analysis['complete_pipeline']:\n",
        "        pipeline_data = global_analysis['complete_pipeline']\n",
        "        combinations = list(pipeline_data.keys())\n",
        "        combined_scores = [global_analyzer._calculate_combined_score(pipeline_data[combo]) for combo in combinations]\n",
        "        \n",
        "        axes[1, 0].bar(range(len(combinations)), combined_scores, color='purple', alpha=0.7)\n",
        "        axes[1, 0].set_title('Score Combinado (40% Retrieval + 60% Calidad)')\n",
        "        axes[1, 0].set_ylabel('Score Combinado')\n",
        "        axes[1, 0].set_xticks(range(len(combinations)))\n",
        "        axes[1, 0].set_xticklabels(combinations, rotation=45, ha='right')\n",
        "        \n",
        "        # Agregar valores en las barras\n",
        "        for i, v in enumerate(combined_scores):\n",
        "            axes[1, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    # 5. Correlaci√≥n Retrieval vs Calidad\n",
        "    if 'complete_pipeline' in global_analysis and global_analysis['complete_pipeline']:\n",
        "        pipeline_data = global_analysis['complete_pipeline']\n",
        "        retrieval_scores = [pipeline_data[combo]['retrieval_recall_at_5'] for combo in pipeline_data.keys()]\n",
        "        quality_scores = [pipeline_data[combo]['quality_overall_score'] for combo in pipeline_data.keys()]\n",
        "        \n",
        "        axes[1, 1].scatter(retrieval_scores, quality_scores, alpha=0.7, s=100)\n",
        "        axes[1, 1].set_title('Correlaci√≥n Retrieval vs Calidad')\n",
        "        axes[1, 1].set_xlabel('Recall@5')\n",
        "        axes[1, 1].set_ylabel('Calidad General')\n",
        "        \n",
        "        # Calcular y mostrar correlaci√≥n\n",
        "        if len(retrieval_scores) > 1:\n",
        "            correlation = np.corrcoef(retrieval_scores, quality_scores)[0, 1]\n",
        "            axes[1, 1].text(0.05, 0.95, f'Correlaci√≥n: {correlation:.3f}', \n",
        "                           transform=axes[1, 1].transAxes, fontsize=12, \n",
        "                           bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
        "    \n",
        "    # 6. Distribuci√≥n de m√©tricas de calidad\n",
        "    if 'complete_pipeline' in global_analysis and global_analysis['complete_pipeline']:\n",
        "        pipeline_data = global_analysis['complete_pipeline']\n",
        "        quality_metrics = ['quality_coherence', 'quality_relevance', 'quality_completeness', \n",
        "                          'quality_fidelity', 'quality_conciseness']\n",
        "        metric_names = ['Coherencia', 'Relevancia', 'Completitud', 'Fidelidad', 'Concisi√≥n']\n",
        "        \n",
        "        # Calcular promedios por m√©trica\n",
        "        metric_means = []\n",
        "        for metric in quality_metrics:\n",
        "            values = [pipeline_data[combo][metric] for combo in pipeline_data.keys() if metric in pipeline_data[combo]]\n",
        "            metric_means.append(np.mean(values) if values else 0)\n",
        "        \n",
        "        axes[1, 2].bar(metric_names, metric_means, color='lightcoral', alpha=0.7)\n",
        "        axes[1, 2].set_title('Distribuci√≥n de M√©tricas de Calidad')\n",
        "        axes[1, 2].set_ylabel('Score Promedio (1-5)')\n",
        "        axes[1, 2].set_ylim(0, 5)\n",
        "        axes[1, 2].tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        # Agregar valores en las barras\n",
        "        for i, v in enumerate(metric_means):\n",
        "            axes[1, 2].text(i, v + 0.05, f'{v:.1f}', ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Ejecutar visualizaci√≥n\n",
        "visualize_global_analysis(global_analysis)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lisis detallado y reporte final\n",
        "def generate_comprehensive_report(global_analysis: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Genera un reporte comprensivo del an√°lisis global\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"üìä REPORTE COMPREHENSIVO - AN√ÅLISIS COMPARATIVO GLOBAL\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # 1. Rankings Globales\n",
        "    print(\"\\nüèÜ RANKINGS GLOBALES\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    if 'global_rankings' in global_analysis:\n",
        "        rankings = global_analysis['global_rankings']\n",
        "        \n",
        "        # Mejor retrieval\n",
        "        if 'best_retrieval' in rankings and rankings['best_retrieval']:\n",
        "            print(f\"ü•á Mejor Modelo de Embedding (Retrieval): {rankings['best_retrieval'][0]}\")\n",
        "            if len(rankings['best_retrieval']) > 1:\n",
        "                print(f\"   Top 3: {', '.join(rankings['best_retrieval'][:3])}\")\n",
        "        \n",
        "        # Mejor re-ranking\n",
        "        if 'best_reranking_improvement' in rankings and rankings['best_reranking_improvement']:\n",
        "            print(f\"ü•á Mejor Mejora de Re-ranking: {rankings['best_reranking_improvement'][0]}\")\n",
        "            if len(rankings['best_reranking_improvement']) > 1:\n",
        "                print(f\"   Top 3: {', '.join(rankings['best_reranking_improvement'][:3])}\")\n",
        "        \n",
        "        # Mejor pipeline completo\n",
        "        if 'best_complete_pipeline' in rankings and rankings['best_complete_pipeline']:\n",
        "            print(f\"ü•á Mejor Pipeline Completo: {rankings['best_complete_pipeline'][0]}\")\n",
        "            if len(rankings['best_complete_pipeline']) > 1:\n",
        "                print(f\"   Top 3: {', '.join(rankings['best_complete_pipeline'][:3])}\")\n",
        "        \n",
        "        # Mejor calidad\n",
        "        if 'best_quality' in rankings and rankings['best_quality']:\n",
        "            print(f\"ü•á Mejor Calidad de Respuesta: {rankings['best_quality'][0]}\")\n",
        "            if len(rankings['best_quality']) > 1:\n",
        "                print(f\"   Top 3: {', '.join(rankings['best_quality'][:3])}\")\n",
        "    \n",
        "    # 2. Insights por Componente\n",
        "    print(\"\\nüîç INSIGHTS POR COMPONENTE\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    if 'insights' in global_analysis:\n",
        "        insights = global_analysis['insights']\n",
        "        \n",
        "        # Insights de retrieval\n",
        "        if 'retrieval_insights' in insights and insights['retrieval_insights']:\n",
        "            retrieval_insights = insights['retrieval_insights']\n",
        "            print(f\"\\nüìà Retrieval:\")\n",
        "            print(f\"   - Mejor modelo: {retrieval_insights.get('best_model', 'N/A')}\")\n",
        "            print(f\"   - Peor modelo: {retrieval_insights.get('worst_model', 'N/A')}\")\n",
        "            print(f\"   - Rango Recall@5: {retrieval_insights.get('recall_range', (0, 0))}\")\n",
        "            print(f\"   - Desviaci√≥n est√°ndar Recall: {retrieval_insights.get('recall_std', 0):.3f}\")\n",
        "        \n",
        "        # Insights de re-ranking\n",
        "        if 'reranking_insights' in insights and insights['reranking_insights']:\n",
        "            reranking_insights = insights['reranking_insights']\n",
        "            print(f\"\\nüîÑ Re-ranking:\")\n",
        "            print(f\"   - Mejor modelo: {reranking_insights.get('best_reranking_model', 'N/A')}\")\n",
        "            print(f\"   - Efectividad general: {reranking_insights.get('overall_effectiveness', 0):.1%}\")\n",
        "            \n",
        "            if 'model_stats' in reranking_insights:\n",
        "                print(f\"   - Estad√≠sticas por modelo:\")\n",
        "                for model, stats in reranking_insights['model_stats'].items():\n",
        "                    print(f\"     * {model}: {stats['mean_improvement']:+.3f} ¬± {stats['std_improvement']:.3f}\")\n",
        "        \n",
        "        # Insights de calidad\n",
        "        if 'quality_insights' in insights and insights['quality_insights']:\n",
        "            quality_insights = insights['quality_insights']\n",
        "            print(f\"\\nüéØ Calidad:\")\n",
        "            print(f\"   - Mejor combinaci√≥n: {quality_insights.get('best_quality_combo', 'N/A')}\")\n",
        "            print(f\"   - Calidad promedio: {quality_insights.get('overall_quality_mean', 0):.1f}/5\")\n",
        "            \n",
        "            if 'quality_stats' in quality_insights:\n",
        "                print(f\"   - Estad√≠sticas por m√©trica:\")\n",
        "                for metric, stats in quality_insights['quality_stats'].items():\n",
        "                    metric_name = metric.replace('quality_', '').title()\n",
        "                    print(f\"     * {metric_name}: {stats['mean']:.1f} ¬± {stats['std']:.1f}\")\n",
        "    \n",
        "    # 3. An√°lisis de Correlaciones\n",
        "    print(\"\\nüìä AN√ÅLISIS DE CORRELACIONES\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    if 'insights' in global_analysis and 'correlation_insights' in global_analysis['insights']:\n",
        "        correlations = global_analysis['insights']['correlation_insights']\n",
        "        \n",
        "        print(f\"üîó Correlaciones entre m√©tricas:\")\n",
        "        for correlation_name, value in correlations.items():\n",
        "            correlation_display = correlation_name.replace('_', ' vs ').title()\n",
        "            strength = \"Fuerte\" if abs(value) > 0.7 else \"Moderada\" if abs(value) > 0.3 else \"D√©bil\"\n",
        "            direction = \"Positiva\" if value > 0 else \"Negativa\"\n",
        "            print(f\"   - {correlation_display}: {value:.3f} ({strength} {direction})\")\n",
        "    \n",
        "    # 4. Recomendaciones\n",
        "    print(\"\\nüí° RECOMENDACIONES\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    if 'insights' in global_analysis and 'recommendations' in global_analysis['insights']:\n",
        "        recommendations = global_analysis['insights']['recommendations']\n",
        "        \n",
        "        print(f\"üéØ Configuraciones Recomendadas:\")\n",
        "        if recommendations.get('best_overall_config'):\n",
        "            print(f\"   - Mejor configuraci√≥n general: {recommendations['best_overall_config']}\")\n",
        "        if recommendations.get('best_retrieval_config'):\n",
        "            print(f\"   - Mejor configuraci√≥n de retrieval: {recommendations['best_retrieval_config']}\")\n",
        "        if recommendations.get('best_quality_config'):\n",
        "            print(f\"   - Mejor configuraci√≥n de calidad: {recommendations['best_quality_config']}\")\n",
        "        \n",
        "        print(f\"\\nüîÑ Re-ranking:\")\n",
        "        if recommendations.get('reranking_recommendation'):\n",
        "            print(f\"   - {recommendations['reranking_recommendation']}\")\n",
        "        \n",
        "        print(f\"\\nüöÄ Sugerencias de Mejora:\")\n",
        "        if recommendations.get('improvement_suggestions'):\n",
        "            for i, suggestion in enumerate(recommendations['improvement_suggestions'], 1):\n",
        "                print(f\"   {i}. {suggestion}\")\n",
        "        else:\n",
        "            print(\"   - No se identificaron mejoras espec√≠ficas necesarias\")\n",
        "    \n",
        "    # 5. Resumen Ejecutivo\n",
        "    print(\"\\nüìã RESUMEN EJECUTIVO\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Contar total de combinaciones evaluadas\n",
        "    total_combinations = 0\n",
        "    if 'complete_pipeline' in global_analysis:\n",
        "        total_combinations = len(global_analysis['complete_pipeline'])\n",
        "    \n",
        "    print(f\"üìä Evaluaci√≥n completada:\")\n",
        "    print(f\"   - Total de combinaciones evaluadas: {total_combinations}\")\n",
        "    print(f\"   - Modelos de embedding: {len(EMBEDDING_MODELS)}\")\n",
        "    print(f\"   - Modelos de re-ranking: {len(RERANKING_MODELS)}\")\n",
        "    print(f\"   - Consultas evaluadas: {len(dataset)}\")\n",
        "    \n",
        "    # Mejor configuraci√≥n general\n",
        "    if 'global_rankings' in global_analysis and 'best_complete_pipeline' in global_analysis['global_rankings']:\n",
        "        best_config = global_analysis['global_rankings']['best_complete_pipeline'][0]\n",
        "        print(f\"\\nüèÜ Recomendaci√≥n Final:\")\n",
        "        print(f\"   - Usar configuraci√≥n: {best_config}\")\n",
        "        print(f\"   - Esta configuraci√≥n ofrece el mejor balance entre retrieval y calidad de respuesta\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ An√°lisis comparativo global completado exitosamente\")\n",
        "\n",
        "# Ejecutar reporte comprensivo\n",
        "generate_comprehensive_report(global_analysis)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar resultados del an√°lisis global\n",
        "def save_global_analysis_results(global_analysis: Dict[str, Any], \n",
        "                                evaluation_name: str = \"global_comparative_analysis\") -> str:\n",
        "    \"\"\"\n",
        "    Guarda los resultados del an√°lisis comparativo global\n",
        "    \n",
        "    Args:\n",
        "        global_analysis: Resultados del an√°lisis global\n",
        "        evaluation_name: Nombre de la evaluaci√≥n\n",
        "    \n",
        "    Returns:\n",
        "        Ruta del archivo guardado\n",
        "    \"\"\"\n",
        "    # Preparar configuraci√≥n\n",
        "    config = {\n",
        "        'embedding_models': list(EMBEDDING_MODELS.keys()),\n",
        "        'reranking_models': list(RERANKING_MODELS.keys()),\n",
        "        'retrieval_metrics': RETRIEVAL_METRICS,\n",
        "        'llm_evaluation_criteria': LLM_EVALUATION_CRITERIA,\n",
        "        'qdrant_config': QDRANT_CONFIG,\n",
        "        'llm_config': LLM_CONFIG,\n",
        "        'dataset_size': len(dataset)\n",
        "    }\n",
        "    \n",
        "    # Preparar metadatos\n",
        "    metadata = {\n",
        "        'evaluation_type': 'global_comparative_analysis',\n",
        "        'description': 'An√°lisis comparativo global de todas las evaluaciones del pipeline RAG',\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'total_queries': len(dataset),\n",
        "        'total_combinations': len(global_analysis.get('complete_pipeline', {})),\n",
        "        'analysis_components': ['retrieval', 'reranking', 'complete_pipeline', 'rankings', 'insights']\n",
        "    }\n",
        "    \n",
        "    # Guardar an√°lisis\n",
        "    filepath = eval_manager.save_evaluation(\n",
        "        evaluation_name=evaluation_name,\n",
        "        config=config,\n",
        "        results={'global_analysis': global_analysis},\n",
        "        metadata=metadata\n",
        "    )\n",
        "    \n",
        "    print(f\"üíæ Resultados del an√°lisis global guardados en: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "# Guardar resultados del an√°lisis global\n",
        "global_analysis_filepath = save_global_analysis_results(global_analysis)\n",
        "\n",
        "# Resumen final del an√°lisis comparativo global\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìã RESUMEN FINAL - AN√ÅLISIS COMPARATIVO GLOBAL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if global_analysis:\n",
        "    # Estad√≠sticas generales\n",
        "    print(f\"\\nüìä Estad√≠sticas Generales:\")\n",
        "    print(f\"   - Evaluaciones realizadas: 4 (Retrieval, Re-ranking, Pipeline Completo, An√°lisis Global)\")\n",
        "    print(f\"   - Modelos de embedding evaluados: {len(EMBEDDING_MODELS)}\")\n",
        "    print(f\"   - Modelos de re-ranking evaluados: {len(RERANKING_MODELS)}\")\n",
        "    print(f\"   - Consultas evaluadas: {len(dataset)}\")\n",
        "    print(f\"   - Combinaciones totales: {len(global_analysis.get('complete_pipeline', {}))}\")\n",
        "    \n",
        "    # Mejores configuraciones identificadas\n",
        "    if 'global_rankings' in global_analysis:\n",
        "        rankings = global_analysis['global_rankings']\n",
        "        \n",
        "        print(f\"\\nüèÜ Mejores Configuraciones Identificadas:\")\n",
        "        \n",
        "        if 'best_retrieval' in rankings and rankings['best_retrieval']:\n",
        "            print(f\"   - Mejor Embedding: {rankings['best_retrieval'][0]}\")\n",
        "        \n",
        "        if 'best_reranking_improvement' in rankings and rankings['best_reranking_improvement']:\n",
        "            print(f\"   - Mejor Re-ranking: {rankings['best_reranking_improvement'][0]}\")\n",
        "        \n",
        "        if 'best_complete_pipeline' in rankings and rankings['best_complete_pipeline']:\n",
        "            print(f\"   - Mejor Pipeline Completo: {rankings['best_complete_pipeline'][0]}\")\n",
        "        \n",
        "        if 'best_quality' in rankings and rankings['best_quality']:\n",
        "            print(f\"   - Mejor Calidad: {rankings['best_quality'][0]}\")\n",
        "    \n",
        "    # Insights clave\n",
        "    if 'insights' in global_analysis:\n",
        "        insights = global_analysis['insights']\n",
        "        \n",
        "        print(f\"\\nüîç Insights Clave:\")\n",
        "        \n",
        "        # Insight de retrieval\n",
        "        if 'retrieval_insights' in insights and insights['retrieval_insights']:\n",
        "            retrieval_insights = insights['retrieval_insights']\n",
        "            best_model = retrieval_insights.get('best_model', 'N/A')\n",
        "            recall_range = retrieval_insights.get('recall_range', (0, 0))\n",
        "            print(f\"   - Mejor modelo de embedding: {best_model} (Recall@5: {recall_range[1]:.3f})\")\n",
        "        \n",
        "        # Insight de re-ranking\n",
        "        if 'reranking_insights' in insights and insights['reranking_insights']:\n",
        "            reranking_insights = insights['reranking_insights']\n",
        "            effectiveness = reranking_insights.get('overall_effectiveness', 0)\n",
        "            best_reranking = reranking_insights.get('best_reranking_model', 'N/A')\n",
        "            print(f\"   - Efectividad del re-ranking: {effectiveness:.1%} (Mejor: {best_reranking})\")\n",
        "        \n",
        "        # Insight de calidad\n",
        "        if 'quality_insights' in insights and insights['quality_insights']:\n",
        "            quality_insights = insights['quality_insights']\n",
        "            overall_quality = quality_insights.get('overall_quality_mean', 0)\n",
        "            best_quality_combo = quality_insights.get('best_quality_combo', 'N/A')\n",
        "            print(f\"   - Calidad promedio: {overall_quality:.1f}/5 (Mejor: {best_quality_combo})\")\n",
        "        \n",
        "        # Insight de correlaciones\n",
        "        if 'correlation_insights' in insights and insights['correlation_insights']:\n",
        "            correlations = insights['correlation_insights']\n",
        "            retrieval_quality_corr = correlations.get('retrieval_quality', 0)\n",
        "            print(f\"   - Correlaci√≥n Retrieval-Calidad: {retrieval_quality_corr:.3f}\")\n",
        "    \n",
        "    # Recomendaciones finales\n",
        "    if 'insights' in global_analysis and 'recommendations' in global_analysis['insights']:\n",
        "        recommendations = global_analysis['insights']['recommendations']\n",
        "        \n",
        "        print(f\"\\nüí° Recomendaciones Finales:\")\n",
        "        \n",
        "        if recommendations.get('best_overall_config'):\n",
        "            print(f\"   - Configuraci√≥n recomendada: {recommendations['best_overall_config']}\")\n",
        "        \n",
        "        if recommendations.get('reranking_recommendation'):\n",
        "            print(f\"   - Re-ranking: {recommendations['reranking_recommendation']}\")\n",
        "        \n",
        "        if recommendations.get('improvement_suggestions'):\n",
        "            print(f\"   - Mejoras sugeridas: {len(recommendations['improvement_suggestions'])} identificadas\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ An√°lisis comparativo global completado exitosamente\")\n",
        "    print(f\"üìÅ Resultados guardados en: {global_analysis_filepath}\")\n",
        "    \n",
        "    # Pr√≥ximos pasos sugeridos\n",
        "    print(f\"\\nüöÄ Pr√≥ximos Pasos Sugeridos:\")\n",
        "    print(f\"   1. Implementar la configuraci√≥n recomendada en producci√≥n\")\n",
        "    print(f\"   2. Realizar pruebas A/B con las mejores combinaciones\")\n",
        "    print(f\"   3. Monitorear el rendimiento en tiempo real\")\n",
        "    print(f\"   4. Iterar y mejorar bas√°ndose en feedback de usuarios\")\n",
        "    print(f\"   5. Expandir el dataset de evaluaci√≥n con m√°s consultas\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No se pudieron obtener resultados del an√°lisis global\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. Conclusiones\n",
        "\n",
        "## Resumen Ejecutivo\n",
        "\n",
        "Este notebook presenta una evaluaci√≥n comprehensiva del pipeline RAG para b√∫squeda sem√°ntica en derecho laboral paraguayo. A trav√©s de 8 secciones estructuradas, se evaluaron m√∫ltiples modelos de embedding, re-ranking y generaci√≥n de respuestas, proporcionando insights valiosos para la optimizaci√≥n del sistema.\n",
        "\n",
        "### üéØ Objetivos Cumplidos\n",
        "\n",
        "1. **Evaluaci√≥n Sistem√°tica**: Se implement√≥ un framework robusto para evaluar componentes individuales y el pipeline completo\n",
        "2. **An√°lisis Comparativo**: Se compararon m√∫ltiples configuraciones para identificar las mejores combinaciones\n",
        "3. **M√©tricas Objetivas y Subjetivas**: Se combinaron m√©tricas de retrieval con evaluaci√≥n de calidad usando LLM-as-a-judge\n",
        "4. **Recomendaciones Accionables**: Se generaron recomendaciones espec√≠ficas basadas en evidencia emp√≠rica\n",
        "\n",
        "### üìä Alcance de la Evaluaci√≥n\n",
        "\n",
        "- **Modelos de Embedding**: 3 modelos evaluados (sentence-transformers, multilingual, especializados)\n",
        "- **Modelos de Re-ranking**: 3 modelos cross-encoder evaluados\n",
        "- **Pipeline Completo**: Evaluaci√≥n end-to-end con generaci√≥n y evaluaci√≥n de respuestas\n",
        "- **M√©tricas**: 15+ m√©tricas diferentes (retrieval, calidad, correlaciones)\n",
        "- **Combinaciones**: Todas las combinaciones posibles evaluadas sistem√°ticamente\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hallazgos Clave\n",
        "\n",
        "### üèÜ Mejores Configuraciones Identificadas\n",
        "\n",
        "#### 1. Modelos de Embedding\n",
        "- **Mejor Rendimiento**: [Se actualizar√° con resultados reales]\n",
        "- **Caracter√≠sticas**: [Se actualizar√° con an√°lisis de patrones]\n",
        "- **Recomendaci√≥n**: [Se actualizar√° con insights espec√≠ficos]\n",
        "\n",
        "#### 2. Modelos de Re-ranking\n",
        "- **Efectividad General**: [Se actualizar√° con porcentaje de mejoras]\n",
        "- **Mejor Modelo**: [Se actualizar√° con modelo recomendado]\n",
        "- **Impacto en Calidad**: [Se actualizar√° con an√°lisis de correlaciones]\n",
        "\n",
        "#### 3. Pipeline Completo\n",
        "- **Configuraci√≥n √ìptima**: [Se actualizar√° con mejor combinaci√≥n]\n",
        "- **Score Combinado**: [Se actualizar√° con m√©trica final]\n",
        "- **Balance Retrieval-Calidad**: [Se actualizar√° con an√°lisis de trade-offs]\n",
        "\n",
        "### üìà Insights de Rendimiento\n",
        "\n",
        "#### Correlaciones Identificadas\n",
        "- **Retrieval vs Calidad**: [Se actualizar√° con coeficiente de correlaci√≥n]\n",
        "- **Coherencia vs Relevancia**: [Se actualizar√° con an√°lisis de relaci√≥n]\n",
        "- **Re-ranking vs Mejoras**: [Se actualizar√° con efectividad medida]\n",
        "\n",
        "#### Patrones de Comportamiento\n",
        "- **Consistencia**: [Se actualizar√° con an√°lisis de estabilidad]\n",
        "- **Escalabilidad**: [Se actualizar√° con rendimiento por consulta]\n",
        "- **Robustez**: [Se actualizar√° con an√°lisis de errores]\n",
        "\n",
        "### üéØ M√©tricas de √âxito\n",
        "\n",
        "#### Retrieval\n",
        "- **Recall@5 Promedio**: [Se actualizar√° con valor]\n",
        "- **Precision@5 Promedio**: [Se actualizar√° con valor]\n",
        "- **nDCG@5 Promedio**: [Se actualizar√° con valor]\n",
        "- **MRR Promedio**: [Se actualizar√° con valor]\n",
        "\n",
        "#### Calidad de Respuestas\n",
        "- **Score General Promedio**: [Se actualizar√° con valor]/5\n",
        "- **Coherencia Promedio**: [Se actualizar√° con valor]/5\n",
        "- **Relevancia Promedio**: [Se actualizar√° con valor]/5\n",
        "- **Completitud Promedio**: [Se actualizar√° con valor]/5\n",
        "- **Fidelidad Promedio**: [Se actualizar√° con valor]/5\n",
        "- **Concisi√≥n Promedio**: [Se actualizar√° con valor]/5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generar conclusiones din√°micas basadas en los resultados\n",
        "def generate_dynamic_conclusions(global_analysis: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Genera conclusiones din√°micas basadas en los resultados reales\n",
        "    \"\"\"\n",
        "    conclusions = {\n",
        "        'best_configurations': {},\n",
        "        'performance_insights': {},\n",
        "        'success_metrics': {},\n",
        "        'recommendations': {},\n",
        "        'limitations': {},\n",
        "        'future_work': {}\n",
        "    }\n",
        "    \n",
        "    if not global_analysis:\n",
        "        return conclusions\n",
        "    \n",
        "    # 1. Mejores configuraciones\n",
        "    if 'global_rankings' in global_analysis:\n",
        "        rankings = global_analysis['global_rankings']\n",
        "        \n",
        "        conclusions['best_configurations'] = {\n",
        "            'best_embedding': rankings.get('best_retrieval', ['N/A'])[0],\n",
        "            'best_reranking': rankings.get('best_reranking_improvement', ['N/A'])[0],\n",
        "            'best_complete_pipeline': rankings.get('best_complete_pipeline', ['N/A'])[0],\n",
        "            'best_quality': rankings.get('best_quality', ['N/A'])[0]\n",
        "        }\n",
        "    \n",
        "    # 2. Insights de rendimiento\n",
        "    if 'insights' in global_analysis:\n",
        "        insights = global_analysis['insights']\n",
        "        \n",
        "        # Insights de retrieval\n",
        "        if 'retrieval_insights' in insights and insights['retrieval_insights']:\n",
        "            retrieval_insights = insights['retrieval_insights']\n",
        "            conclusions['performance_insights']['retrieval'] = {\n",
        "                'best_model': retrieval_insights.get('best_model', 'N/A'),\n",
        "                'recall_range': retrieval_insights.get('recall_range', (0, 0)),\n",
        "                'recall_std': retrieval_insights.get('recall_std', 0)\n",
        "            }\n",
        "        \n",
        "        # Insights de re-ranking\n",
        "        if 'reranking_insights' in insights and insights['reranking_insights']:\n",
        "            reranking_insights = insights['reranking_insights']\n",
        "            conclusions['performance_insights']['reranking'] = {\n",
        "                'best_model': reranking_insights.get('best_reranking_model', 'N/A'),\n",
        "                'effectiveness': reranking_insights.get('overall_effectiveness', 0)\n",
        "            }\n",
        "        \n",
        "        # Insights de calidad\n",
        "        if 'quality_insights' in insights and insights['quality_insights']:\n",
        "            quality_insights = insights['quality_insights']\n",
        "            conclusions['performance_insights']['quality'] = {\n",
        "                'best_combo': quality_insights.get('best_quality_combo', 'N/A'),\n",
        "                'overall_mean': quality_insights.get('overall_quality_mean', 0)\n",
        "            }\n",
        "        \n",
        "        # Correlaciones\n",
        "        if 'correlation_insights' in insights and insights['correlation_insights']:\n",
        "            correlations = insights['correlation_insights']\n",
        "            conclusions['performance_insights']['correlations'] = correlations\n",
        "    \n",
        "    # 3. M√©tricas de √©xito\n",
        "    if 'complete_pipeline' in global_analysis:\n",
        "        pipeline_data = global_analysis['complete_pipeline']\n",
        "        \n",
        "        # Calcular promedios de m√©tricas\n",
        "        retrieval_metrics = ['retrieval_recall_at_5', 'retrieval_precision_at_5', \n",
        "                           'retrieval_ndcg_at_5', 'retrieval_mrr']\n",
        "        quality_metrics = ['quality_coherence', 'quality_relevance', 'quality_completeness', \n",
        "                          'quality_fidelity', 'quality_conciseness', 'quality_overall_score']\n",
        "        \n",
        "        conclusions['success_metrics'] = {\n",
        "            'retrieval': {},\n",
        "            'quality': {}\n",
        "        }\n",
        "        \n",
        "        # M√©tricas de retrieval\n",
        "        for metric in retrieval_metrics:\n",
        "            values = [data[metric] for data in pipeline_data.values() if metric in data]\n",
        "            if values:\n",
        "                conclusions['success_metrics']['retrieval'][metric] = {\n",
        "                    'mean': np.mean(values),\n",
        "                    'std': np.std(values)\n",
        "                }\n",
        "        \n",
        "        # M√©tricas de calidad\n",
        "        for metric in quality_metrics:\n",
        "            values = [data[metric] for data in pipeline_data.values() if metric in data]\n",
        "            if values:\n",
        "                conclusions['success_metrics']['quality'][metric] = {\n",
        "                    'mean': np.mean(values),\n",
        "                    'std': np.std(values)\n",
        "                }\n",
        "    \n",
        "    # 4. Recomendaciones\n",
        "    if 'insights' in global_analysis and 'recommendations' in global_analysis['insights']:\n",
        "        recommendations = global_analysis['insights']['recommendations']\n",
        "        conclusions['recommendations'] = {\n",
        "            'best_overall_config': recommendations.get('best_overall_config', 'N/A'),\n",
        "            'reranking_recommendation': recommendations.get('reranking_recommendation', 'N/A'),\n",
        "            'improvement_suggestions': recommendations.get('improvement_suggestions', [])\n",
        "        }\n",
        "    \n",
        "    # 5. Limitaciones identificadas\n",
        "    conclusions['limitations'] = {\n",
        "        'dataset_size': f\"Dataset limitado a {len(dataset)} consultas\",\n",
        "        'model_coverage': f\"Solo {len(EMBEDDING_MODELS)} modelos de embedding evaluados\",\n",
        "        'domain_specificity': \"Evaluaci√≥n espec√≠fica para derecho laboral paraguayo\",\n",
        "        'llm_dependency': \"Dependencia de LLM para evaluaci√≥n de calidad\",\n",
        "        'computational_cost': \"Evaluaci√≥n computacionalmente intensiva\"\n",
        "    }\n",
        "    \n",
        "    # 6. Trabajo futuro\n",
        "    conclusions['future_work'] = {\n",
        "        'dataset_expansion': \"Expandir dataset con m√°s consultas y casos edge\",\n",
        "        'model_diversification': \"Evaluar m√°s modelos de embedding y re-ranking\",\n",
        "        'domain_generalization': \"Probar en otros dominios legales\",\n",
        "        'efficiency_optimization': \"Optimizar para menor costo computacional\",\n",
        "        'real_world_validation': \"Validar en entorno de producci√≥n real\"\n",
        "    }\n",
        "    \n",
        "    return conclusions\n",
        "\n",
        "# Generar conclusiones din√°micas\n",
        "dynamic_conclusions = generate_dynamic_conclusions(global_analysis)\n",
        "\n",
        "print(\"‚úÖ Conclusiones din√°micas generadas basadas en resultados reales\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mostrar conclusiones din√°micas\n",
        "def display_dynamic_conclusions(conclusions: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Muestra las conclusiones din√°micas de forma estructurada\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"üìä CONCLUSIONES DIN√ÅMICAS - BASADAS EN RESULTADOS REALES\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # 1. Mejores configuraciones\n",
        "    if 'best_configurations' in conclusions and conclusions['best_configurations']:\n",
        "        print(\"\\nüèÜ MEJORES CONFIGURACIONES IDENTIFICADAS\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        configs = conclusions['best_configurations']\n",
        "        print(f\"ü•á Mejor Modelo de Embedding: {configs.get('best_embedding', 'N/A')}\")\n",
        "        print(f\"ü•á Mejor Modelo de Re-ranking: {configs.get('best_reranking', 'N/A')}\")\n",
        "        print(f\"ü•á Mejor Pipeline Completo: {configs.get('best_complete_pipeline', 'N/A')}\")\n",
        "        print(f\"ü•á Mejor Calidad de Respuesta: {configs.get('best_quality', 'N/A')}\")\n",
        "    \n",
        "    # 2. Insights de rendimiento\n",
        "    if 'performance_insights' in conclusions and conclusions['performance_insights']:\n",
        "        print(\"\\nüìà INSIGHTS DE RENDIMIENTO\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        insights = conclusions['performance_insights']\n",
        "        \n",
        "        # Retrieval insights\n",
        "        if 'retrieval' in insights:\n",
        "            retrieval = insights['retrieval']\n",
        "            print(f\"\\nüìä Retrieval:\")\n",
        "            print(f\"   - Mejor modelo: {retrieval.get('best_model', 'N/A')}\")\n",
        "            print(f\"   - Rango Recall@5: {retrieval.get('recall_range', (0, 0))}\")\n",
        "            print(f\"   - Desviaci√≥n est√°ndar: {retrieval.get('recall_std', 0):.3f}\")\n",
        "        \n",
        "        # Re-ranking insights\n",
        "        if 'reranking' in insights:\n",
        "            reranking = insights['reranking']\n",
        "            print(f\"\\nüîÑ Re-ranking:\")\n",
        "            print(f\"   - Mejor modelo: {reranking.get('best_model', 'N/A')}\")\n",
        "            print(f\"   - Efectividad: {reranking.get('effectiveness', 0):.1%}\")\n",
        "        \n",
        "        # Quality insights\n",
        "        if 'quality' in insights:\n",
        "            quality = insights['quality']\n",
        "            print(f\"\\nüéØ Calidad:\")\n",
        "            print(f\"   - Mejor combinaci√≥n: {quality.get('best_combo', 'N/A')}\")\n",
        "            print(f\"   - Calidad promedio: {quality.get('overall_mean', 0):.1f}/5\")\n",
        "        \n",
        "        # Correlaciones\n",
        "        if 'correlations' in insights:\n",
        "            correlations = insights['correlations']\n",
        "            print(f\"\\nüîó Correlaciones:\")\n",
        "            for corr_name, value in correlations.items():\n",
        "                corr_display = corr_name.replace('_', ' vs ').title()\n",
        "                strength = \"Fuerte\" if abs(value) > 0.7 else \"Moderada\" if abs(value) > 0.3 else \"D√©bil\"\n",
        "                direction = \"Positiva\" if value > 0 else \"Negativa\"\n",
        "                print(f\"   - {corr_display}: {value:.3f} ({strength} {direction})\")\n",
        "    \n",
        "    # 3. M√©tricas de √©xito\n",
        "    if 'success_metrics' in conclusions and conclusions['success_metrics']:\n",
        "        print(\"\\nüéØ M√âTRICAS DE √âXITO\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        metrics = conclusions['success_metrics']\n",
        "        \n",
        "        # M√©tricas de retrieval\n",
        "        if 'retrieval' in metrics and metrics['retrieval']:\n",
        "            print(f\"\\nüìä Retrieval:\")\n",
        "            for metric, stats in metrics['retrieval'].items():\n",
        "                metric_name = metric.replace('retrieval_', '').replace('_at_5', '@5').title()\n",
        "                print(f\"   - {metric_name}: {stats['mean']:.3f} ¬± {stats['std']:.3f}\")\n",
        "        \n",
        "        # M√©tricas de calidad\n",
        "        if 'quality' in metrics and metrics['quality']:\n",
        "            print(f\"\\nüéØ Calidad:\")\n",
        "            for metric, stats in metrics['quality'].items():\n",
        "                metric_name = metric.replace('quality_', '').title()\n",
        "                print(f\"   - {metric_name}: {stats['mean']:.1f} ¬± {stats['std']:.1f}\")\n",
        "    \n",
        "    # 4. Recomendaciones\n",
        "    if 'recommendations' in conclusions and conclusions['recommendations']:\n",
        "        print(\"\\nüí° RECOMENDACIONES\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        recs = conclusions['recommendations']\n",
        "        print(f\"üéØ Configuraci√≥n Recomendada: {recs.get('best_overall_config', 'N/A')}\")\n",
        "        print(f\"üîÑ Re-ranking: {recs.get('reranking_recommendation', 'N/A')}\")\n",
        "        \n",
        "        if recs.get('improvement_suggestions'):\n",
        "            print(f\"\\nüöÄ Sugerencias de Mejora:\")\n",
        "            for i, suggestion in enumerate(recs['improvement_suggestions'], 1):\n",
        "                print(f\"   {i}. {suggestion}\")\n",
        "    \n",
        "    # 5. Limitaciones\n",
        "    if 'limitations' in conclusions and conclusions['limitations']:\n",
        "        print(\"\\n‚ö†Ô∏è LIMITACIONES IDENTIFICADAS\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        limitations = conclusions['limitations']\n",
        "        for limitation, description in limitations.items():\n",
        "            limitation_name = limitation.replace('_', ' ').title()\n",
        "            print(f\"   - {limitation_name}: {description}\")\n",
        "    \n",
        "    # 6. Trabajo futuro\n",
        "    if 'future_work' in conclusions and conclusions['future_work']:\n",
        "        print(\"\\nüöÄ TRABAJO FUTURO SUGERIDO\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        future_work = conclusions['future_work']\n",
        "        for area, description in future_work.items():\n",
        "            area_name = area.replace('_', ' ').title()\n",
        "            print(f\"   - {area_name}: {description}\")\n",
        "\n",
        "# Mostrar conclusiones din√°micas\n",
        "display_dynamic_conclusions(dynamic_conclusions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recomendaciones Finales\n",
        "\n",
        "### üéØ Configuraci√≥n Recomendada para Producci√≥n\n",
        "\n",
        "Bas√°ndose en los resultados de la evaluaci√≥n, se recomienda la siguiente configuraci√≥n:\n",
        "\n",
        "1. **Modelo de Embedding**: [Se actualizar√° con el mejor modelo identificado]\n",
        "2. **Modelo de Re-ranking**: [Se actualizar√° con recomendaci√≥n de re-ranking]\n",
        "3. **Configuraci√≥n de LLM**: [Se actualizar√° con configuraci√≥n √≥ptima]\n",
        "4. **Par√°metros de Qdrant**: [Se actualizar√° con configuraci√≥n recomendada]\n",
        "\n",
        "### üìä M√©tricas de Monitoreo\n",
        "\n",
        "Para el monitoreo en producci√≥n, se recomienda seguir estas m√©tricas:\n",
        "\n",
        "#### M√©tricas Cr√≠ticas\n",
        "- **Recall@5**: Debe mantenerse por encima del [valor] identificado\n",
        "- **Calidad General**: Debe mantenerse por encima de [valor]/5\n",
        "- **Tiempo de Respuesta**: Debe ser menor a [valor] segundos\n",
        "\n",
        "#### M√©tricas de Calidad\n",
        "- **Coherencia**: Monitorear degradaci√≥n en respuestas\n",
        "- **Relevancia**: Verificar que las respuestas sean pertinentes\n",
        "- **Fidelidad**: Asegurar que las respuestas sean fieles al contexto\n",
        "\n",
        "### üîÑ Proceso de Mejora Continua\n",
        "\n",
        "1. **Monitoreo Semanal**: Revisar m√©tricas de rendimiento\n",
        "2. **Evaluaci√≥n Mensual**: Ejecutar evaluaci√≥n completa con nuevas consultas\n",
        "3. **Actualizaci√≥n Trimestral**: Re-evaluar modelos y configuraciones\n",
        "4. **Feedback de Usuarios**: Incorporar feedback real para mejorar m√©tricas\n",
        "\n",
        "### üöÄ Pr√≥ximos Pasos\n",
        "\n",
        "#### Inmediatos (1-2 semanas)\n",
        "- [ ] Implementar configuraci√≥n recomendada en producci√≥n\n",
        "- [ ] Configurar monitoreo de m√©tricas cr√≠ticas\n",
        "- [ ] Establecer proceso de evaluaci√≥n continua\n",
        "\n",
        "#### Corto Plazo (1-3 meses)\n",
        "- [ ] Expandir dataset de evaluaci√≥n con m√°s consultas\n",
        "- [ ] Implementar pruebas A/B con diferentes configuraciones\n",
        "- [ ] Optimizar para menor costo computacional\n",
        "\n",
        "#### Mediano Plazo (3-6 meses)\n",
        "- [ ] Evaluar nuevos modelos de embedding y re-ranking\n",
        "- [ ] Implementar evaluaci√≥n autom√°tica de calidad\n",
        "- [ ] Desarrollar dashboard de monitoreo en tiempo real\n",
        "\n",
        "#### Largo Plazo (6+ meses)\n",
        "- [ ] Extender evaluaci√≥n a otros dominios legales\n",
        "- [ ] Implementar aprendizaje continuo del sistema\n",
        "- [ ] Desarrollar m√©tricas de satisfacci√≥n del usuario\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar conclusiones finales\n",
        "def save_final_conclusions(conclusions: Dict[str, Any], \n",
        "                          evaluation_name: str = \"final_conclusions\") -> str:\n",
        "    \"\"\"\n",
        "    Guarda las conclusiones finales del notebook\n",
        "    \"\"\"\n",
        "    # Preparar configuraci√≥n\n",
        "    config = {\n",
        "        'embedding_models': list(EMBEDDING_MODELS.keys()),\n",
        "        'reranking_models': list(RERANKING_MODELS.keys()),\n",
        "        'retrieval_metrics': RETRIEVAL_METRICS,\n",
        "        'llm_evaluation_criteria': LLM_EVALUATION_CRITERIA,\n",
        "        'qdrant_config': QDRANT_CONFIG,\n",
        "        'llm_config': LLM_CONFIG,\n",
        "        'dataset_size': len(dataset)\n",
        "    }\n",
        "    \n",
        "    # Preparar metadatos\n",
        "    metadata = {\n",
        "        'evaluation_type': 'final_conclusions',\n",
        "        'description': 'Conclusiones finales del notebook de evaluaci√≥n RAG',\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'total_queries': len(dataset),\n",
        "        'notebook_sections': 8,\n",
        "        'evaluation_components': ['retrieval', 'reranking', 'complete_pipeline', 'global_analysis', 'conclusions']\n",
        "    }\n",
        "    \n",
        "    # Guardar conclusiones\n",
        "    filepath = eval_manager.save_evaluation(\n",
        "        evaluation_name=evaluation_name,\n",
        "        config=config,\n",
        "        results={'final_conclusions': conclusions},\n",
        "        metadata=metadata\n",
        "    )\n",
        "    \n",
        "    print(f\"üíæ Conclusiones finales guardadas en: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "# Guardar conclusiones finales\n",
        "final_conclusions_filepath = save_final_conclusions(dynamic_conclusions)\n",
        "\n",
        "# Resumen final del notebook\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìã RESUMEN FINAL DEL NOTEBOOK\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\n‚úÖ NOTEBOOK DE EVALUACI√ìN RAG COMPLETADO\")\n",
        "print(f\"üìä Secciones implementadas: 8/8\")\n",
        "print(f\"üîß Componentes evaluados: Retrieval, Re-ranking, Pipeline Completo, An√°lisis Global\")\n",
        "print(f\"üìà M√©tricas implementadas: 15+ m√©tricas diferentes\")\n",
        "print(f\"üíæ Archivos generados: {len(eval_manager.list_evaluations())} evaluaciones guardadas\")\n",
        "\n",
        "print(f\"\\nüìÅ Archivos de resultados:\")\n",
        "print(f\"   - Retrieval: {retrieval_filepath}\")\n",
        "print(f\"   - Re-ranking: {reranking_filepath}\")\n",
        "print(f\"   - Pipeline Completo: {complete_rag_filepath}\")\n",
        "print(f\"   - An√°lisis Global: {global_analysis_filepath}\")\n",
        "print(f\"   - Conclusiones: {final_conclusions_filepath}\")\n",
        "\n",
        "print(f\"\\nüéØ Pr√≥ximos pasos recomendados:\")\n",
        "print(f\"   1. Revisar conclusiones din√°micas generadas\")\n",
        "print(f\"   2. Implementar configuraci√≥n recomendada\")\n",
        "print(f\"   3. Configurar monitoreo de m√©tricas\")\n",
        "print(f\"   4. Establecer proceso de evaluaci√≥n continua\")\n",
        "\n",
        "print(f\"\\nüöÄ ¬°Evaluaci√≥n RAG completada exitosamente!\")\n",
        "print(f\"   Este notebook proporciona una base s√≥lida para la optimizaci√≥n\")\n",
        "print(f\"   y monitoreo continuo del sistema RAG en producci√≥n.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Anexo: Estructura del Notebook\n",
        "\n",
        "### üìö Secciones Implementadas\n",
        "\n",
        "1. **Introducci√≥n**: Objetivos, dataset, tecnolog√≠as y sistema de persistencia\n",
        "2. **Configuraci√≥n de Modelos**: Diccionarios de configuraci√≥n para modelos y m√©tricas\n",
        "3. **Carga del Dataset**: Funciones de carga, validaci√≥n y visualizaci√≥n\n",
        "4. **Configuraci√≥n del Entorno**: EnvironmentManager para setup completo\n",
        "5. **Evaluaci√≥n de Retrievers**: M√©tricas de embedding + Qdrant\n",
        "6. **Evaluaci√≥n con Re-ranking**: Mejoras con cross-encoders\n",
        "7. **Evaluaci√≥n del Flujo Completo**: Pipeline RAG con LLM-as-a-judge\n",
        "8. **An√°lisis Comparativo Global**: Consolidaci√≥n y recomendaciones\n",
        "\n",
        "### üîß Componentes T√©cnicos\n",
        "\n",
        "#### Clases Principales\n",
        "- `EvaluationManager`: Sistema de persistencia de evaluaciones\n",
        "- `EnvironmentManager`: Configuraci√≥n centralizada del entorno\n",
        "- `RetrieverEvaluator`: Evaluaci√≥n de modelos de embedding\n",
        "- `RerankingEvaluator`: Evaluaci√≥n de modelos de re-ranking\n",
        "- `RAGPipelineEvaluator`: Evaluaci√≥n del pipeline completo\n",
        "- `GlobalComparativeAnalyzer`: An√°lisis comparativo global\n",
        "\n",
        "#### Funciones de Utilidad\n",
        "- Carga y validaci√≥n de datasets\n",
        "- Visualizaci√≥n de resultados\n",
        "- C√°lculo de m√©tricas agregadas\n",
        "- An√°lisis de correlaciones\n",
        "- Generaci√≥n de recomendaciones\n",
        "\n",
        "### üìä M√©tricas Implementadas\n",
        "\n",
        "#### Retrieval\n",
        "- Recall@k, Precision@k, nDCG@k, MRR, Hit Rate@k\n",
        "\n",
        "#### Calidad (LLM-as-a-judge)\n",
        "- Coherencia, Relevancia, Completitud, Fidelidad, Concisi√≥n\n",
        "\n",
        "#### An√°lisis\n",
        "- Correlaciones entre m√©tricas\n",
        "- Mejoras de re-ranking\n",
        "- Score combinado (retrieval + calidad)\n",
        "\n",
        "### üíæ Sistema de Persistencia\n",
        "\n",
        "- **Formato**: JSON con metadatos completos\n",
        "- **Ubicaci√≥n**: `evaluation_results/` (configurable)\n",
        "- **Estructura**: Configuraci√≥n + Resultados + Metadatos\n",
        "- **Versionado**: Timestamp autom√°tico\n",
        "- **Comparaci√≥n**: Funciones para comparar evaluaciones hist√≥ricas\n",
        "\n",
        "### üöÄ Caracter√≠sticas Avanzadas\n",
        "\n",
        "- **Evaluaci√≥n Autom√°tica**: Proceso completamente automatizado\n",
        "- **Visualizaciones Din√°micas**: Gr√°ficos adaptativos seg√∫n datos\n",
        "- **An√°lisis de Correlaciones**: Identificaci√≥n autom√°tica de patrones\n",
        "- **Recomendaciones Inteligentes**: Basadas en evidencia emp√≠rica\n",
        "- **Monitoreo Continuo**: Framework para evaluaci√≥n peri√≥dica\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
