{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd256a1",
   "metadata": {},
   "source": [
    "# Embedding Models Exploration for Qdrant / üîç Exploraci√≥n de Modelos de Embedding para Qdrant\n",
    "\n",
    "**Goal**: Explore different embedding models to determine which one is most suitable for our RAG system for queries about Paraguayan labor law.\n",
    "\n",
    "**Approach**:\n",
    "- Compare performance of different embedding models\n",
    "- Evaluate loading and search speed\n",
    "- Analyze quality of results for legal queries\n",
    "- Provide recommendations based on metrics\n",
    "\n",
    "**Models to Evaluate**:\n",
    "1. **all-MiniLM-L6-v2** - Light and fast model\n",
    "2. **all-mpnet-base-v2** - Balanced model\n",
    "3. **paraphrase-multilingual-MiniLM-L12-v2** - Multilingual model\n",
    "4. **sentence-transformers/all-MiniLM-L12-v2** - Alternative lightweight\n",
    "\n",
    "---\n",
    "\n",
    "**Objetivo**: Explorar diferentes modelos de embedding para determinar cu√°l es el m√°s adecuado para nuestro sistema RAG de consultas sobre la ley laboral paraguaya.\n",
    "\n",
    "**Enfoque**:\n",
    "- Comparar rendimiento de diferentes modelos de embedding\n",
    "- Evaluar velocidad de carga y b√∫squeda\n",
    "- Analizar calidad de resultados para consultas legales\n",
    "- Proporcionar recomendaciones basadas en m√©tricas\n",
    "\n",
    "**Modelos a Evaluar**:\n",
    "1. **all-MiniLM-L6-v2** - Modelo ligero y r√°pido\n",
    "2. **all-mpnet-base-v2** - Modelo balanceado\n",
    "3. **paraphrase-multilingual-MiniLM-L12-v2** - Modelo multiling√ºe\n",
    "4. **sentence-transformers/all-MiniLM-L12-v2** - Alternativo ligero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11460fc1",
   "metadata": {},
   "source": [
    "## Setup and Configuration / üîß Setup y Configuraci√≥n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef68886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones necesarias\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "# Qdrant y modelos de embedding\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Configuraci√≥n de visualizaciones\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas exitosamente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc82531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n de conexi√≥n a Qdrant\n",
    "QDRANT_URL = os.getenv('QDRANT_URL', 'http://localhost:6333')\n",
    "QDRANT_API_KEY = os.getenv('QDRANT_API_KEY')\n",
    "\n",
    "# Configuraci√≥n de modelos a probar\n",
    "MODELS_TO_TEST = {\n",
    "    'all-MiniLM-L6-v2': {\n",
    "        'name': 'all-MiniLM-L6-v2',\n",
    "        'description': 'Modelo ligero y r√°pido',\n",
    "        'dimension': 384\n",
    "    },\n",
    "    'all-mpnet-base-v2': {\n",
    "        'name': 'all-mpnet-base-v2', \n",
    "        'description': 'Modelo balanceado',\n",
    "        'dimension': 768\n",
    "    },\n",
    "    'paraphrase-multilingual-MiniLM-L12-v2': {\n",
    "        'name': 'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "        'description': 'Modelo multiling√ºe',\n",
    "        'dimension': 384\n",
    "    },\n",
    "    'all-MiniLM-L12-v2': {\n",
    "        'name': 'sentence-transformers/all-MiniLM-L12-v2',\n",
    "        'description': 'Alternativo ligero',\n",
    "        'dimension': 384\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"üîó Conectando a Qdrant en: {QDRANT_URL}\")\n",
    "print(f\"üìä Modelos a evaluar: {len(MODELS_TO_TEST)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7803c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos de prueba sobre la ley laboral paraguaya\n",
    "SAMPLE_LEGAL_TEXTS = [\n",
    "    \"Los trabajadores tienen derecho a un salario m√≠nimo establecido por ley\",\n",
    "    \"La jornada laboral ordinaria no podr√° exceder de ocho horas diarias\",\n",
    "    \"Todo trabajador tiene derecho a vacaciones anuales remuneradas\",\n",
    "    \"El empleador debe proporcionar condiciones seguras de trabajo\",\n",
    "    \"Los contratos de trabajo pueden ser por tiempo determinado o indeterminado\",\n",
    "    \"El aguinaldo es una gratificaci√≥n anual obligatoria para el empleador\",\n",
    "    \"Los trabajadores tienen derecho a formar sindicatos libremente\",\n",
    "    \"La licencia por maternidad es de 18 semanas para la madre trabajadora\",\n",
    "    \"El despido sin justa causa da derecho a indemnizaci√≥n\",\n",
    "    \"Los menores de 18 a√±os tienen protecci√≥n especial en el trabajo\"\n",
    "]\n",
    "\n",
    "SAMPLE_QUERIES = [\n",
    "    \"¬øCu√°ntas horas puede trabajar una persona por d√≠a?\",\n",
    "    \"¬øQu√© derechos tienen los trabajadores sobre vacaciones?\",\n",
    "    \"¬øC√≥mo funciona el aguinaldo en Paraguay?\",\n",
    "    \"¬øPueden los trabajadores formar sindicatos?\",\n",
    "    \"¬øCu√°l es la licencia por maternidad?\"\n",
    "]\n",
    "\n",
    "print(f\"üìù Textos de prueba: {len(SAMPLE_LEGAL_TEXTS)}\")\n",
    "print(f\"‚ùì Consultas de prueba: {len(SAMPLE_QUERIES)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496907d4",
   "metadata": {},
   "source": [
    "## Model Loading and Evaluation / ü§ñ Carga y Evaluaci√≥n de Modelos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e672d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModelEvaluator:\n",
    "    \"\"\"Clase para evaluar modelos de embedding\"\"\"\n",
    "    \n",
    "    def __init__(self, qdrant_url: str, qdrant_api_key: str = None):\n",
    "        self.qdrant_url = qdrant_url\n",
    "        self.qdrant_api_key = qdrant_api_key\n",
    "        self.client = None\n",
    "        self.results = {}\n",
    "        \n",
    "    def connect_qdrant(self):\n",
    "        \"\"\"Conectar a Qdrant\"\"\"\n",
    "        try:\n",
    "            self.client = QdrantClient(\n",
    "                url=self.qdrant_url,\n",
    "                api_key=self.qdrant_api_key\n",
    "            )\n",
    "            print(\"‚úÖ Conexi√≥n a Qdrant establecida\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error conectando a Qdrant: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def load_model_and_measure_time(self, model_name: str) -> Tuple[SentenceTransformer, float]:\n",
    "        \"\"\"Cargar modelo y medir tiempo de carga\"\"\"\n",
    "        print(f\"üì• Cargando modelo: {model_name}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            model = SentenceTransformer(model_name)\n",
    "            load_time = time.time() - start_time\n",
    "            print(f\"‚úÖ Modelo cargado en {load_time:.2f} segundos\")\n",
    "            return model, load_time\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error cargando modelo {model_name}: {e}\")\n",
    "            return None, 0\n",
    "    \n",
    "    def create_embeddings_and_measure_time(self, model: SentenceTransformer, texts: List[str]) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"Crear embeddings y medir tiempo\"\"\"\n",
    "        print(f\"üîÑ Generando embeddings para {len(texts)} textos...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        embeddings = model.encode(texts, convert_to_tensor=False)\n",
    "        embedding_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"‚úÖ Embeddings generados en {embedding_time:.2f} segundos\")\n",
    "        return embeddings, embedding_time\n",
    "\n",
    "# Inicializar evaluador\n",
    "evaluator = EmbeddingModelEvaluator(QDRANT_URL, QDRANT_API_KEY)\n",
    "evaluator.connect_qdrant()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fbca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar evaluaci√≥n completa de todos los modelos\n",
    "model_performance = {}\n",
    "\n",
    "print(\"üöÄ Iniciando evaluaci√≥n de modelos de embedding...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_key, model_config in MODELS_TO_TEST.items():\n",
    "    print(f\"\\nüîç Evaluando: {model_config['name']}\")\n",
    "    print(f\"üìù Descripci√≥n: {model_config['description']}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Cargar modelo\n",
    "    model, load_time = evaluator.load_model_and_measure_time(model_config['name'])\n",
    "    \n",
    "    if model is None:\n",
    "        continue\n",
    "    \n",
    "    # Generar embeddings\n",
    "    embeddings, embedding_time = evaluator.create_embeddings_and_measure_time(model, SAMPLE_LEGAL_TEXTS)\n",
    "    \n",
    "    # Almacenar resultados\n",
    "    model_performance[model_key] = {\n",
    "        'name': model_config['name'],\n",
    "        'description': model_config['description'],\n",
    "        'dimension': model_config['dimension'],\n",
    "        'load_time': load_time,\n",
    "        'embedding_time': embedding_time,\n",
    "        'embeddings': embeddings,\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"üìä Dimensiones: {embeddings.shape}\")\n",
    "    print(f\"‚è±Ô∏è Tiempo total: {load_time + embedding_time:.2f}s\")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluaci√≥n de modelos completada!\")\n",
    "print(f\"üìà Modelos evaluados exitosamente: {len(model_performance)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd31e75",
   "metadata": {},
   "source": [
    "## Performance and Speed Analysis / üìä An√°lisis de Rendimiento y Velocidad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99f7c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame con resultados de rendimiento\n",
    "performance_data = []\n",
    "\n",
    "for model_key, data in model_performance.items():\n",
    "    performance_data.append({\n",
    "        'Modelo': model_key,\n",
    "        'Nombre Completo': data['name'],\n",
    "        'Descripci√≥n': data['description'],\n",
    "        'Dimensiones': data['dimension'],\n",
    "        'Tiempo Carga (s)': data['load_time'],\n",
    "        'Tiempo Embedding (s)': data['embedding_time'],\n",
    "        'Tiempo Total (s)': data['load_time'] + data['embedding_time'],\n",
    "        'Velocidad (textos/s)': len(SAMPLE_LEGAL_TEXTS) / data['embedding_time']\n",
    "    })\n",
    "\n",
    "df_performance = pd.DataFrame(performance_data)\n",
    "print(\"üìä Resumen de Rendimiento de Modelos:\")\n",
    "print(\"=\" * 60)\n",
    "display(df_performance.round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b069d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n de tiempos de carga y embedding\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Gr√°fico 1: Tiempos de carga\n",
    "axes[0].bar(df_performance['Modelo'], df_performance['Tiempo Carga (s)'], \n",
    "           color='skyblue', alpha=0.7)\n",
    "axes[0].set_title('‚è±Ô∏è Tiempo de Carga de Modelos', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Tiempo (segundos)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Gr√°fico 2: Tiempos de embedding\n",
    "axes[1].bar(df_performance['Modelo'], df_performance['Tiempo Embedding (s)'], \n",
    "           color='lightcoral', alpha=0.7)\n",
    "axes[1].set_title('üîÑ Tiempo de Generaci√≥n de Embeddings', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Tiempo (segundos)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Gr√°fico 3: Velocidad de procesamiento\n",
    "axes[2].bar(df_performance['Modelo'], df_performance['Velocidad (textos/s)'], \n",
    "           color='lightgreen', alpha=0.7)\n",
    "axes[2].set_title('üöÄ Velocidad de Procesamiento', fontsize=14, fontweight='bold')\n",
    "axes[2].set_ylabel('Textos por segundo')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar el modelo m√°s r√°pido en cada categor√≠a\n",
    "print(\"üèÜ Mejores Rendimientos:\")\n",
    "print(f\"‚ö° Carga m√°s r√°pida: {df_performance.loc[df_performance['Tiempo Carga (s)'].idxmin(), 'Modelo']}\")\n",
    "print(f\"üîÑ Embedding m√°s r√°pido: {df_performance.loc[df_performance['Tiempo Embedding (s)'].idxmin(), 'Modelo']}\")\n",
    "print(f\"üöÄ Mayor velocidad: {df_performance.loc[df_performance['Velocidad (textos/s)'].idxmax(), 'Modelo']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b6554d",
   "metadata": {},
   "source": [
    "## Search Quality Evaluation / üîç Evaluaci√≥n de Calidad de B√∫squeda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687574a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(embedding1: np.ndarray, embedding2: np.ndarray) -> float:\n",
    "    \"\"\"Calcular similitud coseno entre dos embeddings\"\"\"\n",
    "    dot_product = np.dot(embedding1, embedding2)\n",
    "    norm1 = np.linalg.norm(embedding1)\n",
    "    norm2 = np.linalg.norm(embedding2)\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "def evaluate_search_quality(model_data: dict, texts: List[str], queries: List[str]) -> dict:\n",
    "    \"\"\"Evaluar calidad de b√∫squeda para un modelo\"\"\"\n",
    "    model = model_data['model']\n",
    "    text_embeddings = model_data['embeddings']\n",
    "    \n",
    "    results = {\n",
    "        'query_results': [],\n",
    "        'avg_similarity': 0,\n",
    "        'search_times': []\n",
    "    }\n",
    "    \n",
    "    for query in queries:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Generar embedding para la consulta\n",
    "        query_embedding = model.encode([query])[0]\n",
    "        \n",
    "        # Calcular similitudes con todos los textos\n",
    "        similarities = []\n",
    "        for i, text_embedding in enumerate(text_embeddings):\n",
    "            similarity = calculate_cosine_similarity(query_embedding, text_embedding)\n",
    "            similarities.append((i, texts[i], similarity))\n",
    "        \n",
    "        # Ordenar por similitud descendente\n",
    "        similarities.sort(key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        search_time = time.time() - start_time\n",
    "        results['search_times'].append(search_time)\n",
    "        \n",
    "        results['query_results'].append({\n",
    "            'query': query,\n",
    "            'top_match': similarities[0],\n",
    "            'all_similarities': similarities,\n",
    "            'search_time': search_time\n",
    "        })\n",
    "    \n",
    "    # Calcular similitud promedio del mejor resultado\n",
    "    top_similarities = [result['top_match'][2] for result in results['query_results']]\n",
    "    results['avg_similarity'] = np.mean(top_similarities)\n",
    "    results['avg_search_time'] = np.mean(results['search_times'])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluar calidad de b√∫squeda para todos los modelos\n",
    "search_quality_results = {}\n",
    "\n",
    "print(\"üîç Evaluando calidad de b√∫squeda para cada modelo...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_key, model_data in model_performance.items():\n",
    "    print(f\"\\nüìä Evaluando b√∫squedas para: {model_key}\")\n",
    "    \n",
    "    quality_results = evaluate_search_quality(model_data, SAMPLE_LEGAL_TEXTS, SAMPLE_QUERIES)\n",
    "    search_quality_results[model_key] = quality_results\n",
    "    \n",
    "    print(f\"‚úÖ Similitud promedio: {quality_results['avg_similarity']:.3f}\")\n",
    "    print(f\"‚è±Ô∏è Tiempo promedio de b√∫squeda: {quality_results['avg_search_time']:.4f}s\")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluaci√≥n de calidad completada!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce4f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar ejemplos de b√∫squedas para cada modelo\n",
    "print(\"üîç Ejemplos de B√∫squedas por Modelo:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_key, quality_data in search_quality_results.items():\n",
    "    print(f\"\\nü§ñ Modelo: {model_key}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for i, result in enumerate(quality_data['query_results'][:2]):  # Solo primeras 2 consultas\n",
    "        query = result['query']\n",
    "        top_match = result['top_match']\n",
    "        similarity = top_match[2]\n",
    "        matched_text = top_match[1]\n",
    "        \n",
    "        print(f\"‚ùì Consulta {i+1}: {query}\")\n",
    "        print(f\"‚úÖ Mejor coincidencia (similitud: {similarity:.3f}):\")\n",
    "        print(f\"   üìù {matched_text[:80]}...\")\n",
    "        print()\n",
    "\n",
    "# Crear DataFrame con resultados de calidad\n",
    "quality_data = []\n",
    "for model_key, quality_results in search_quality_results.items():\n",
    "    quality_data.append({\n",
    "        'Modelo': model_key,\n",
    "        'Similitud Promedio': quality_results['avg_similarity'],\n",
    "        'Tiempo B√∫squeda (s)': quality_results['avg_search_time'],\n",
    "        'Consultas/segundo': 1 / quality_results['avg_search_time']\n",
    "    })\n",
    "\n",
    "df_quality = pd.DataFrame(quality_data)\n",
    "print(\"\\nüìä Resumen de Calidad de B√∫squeda:\")\n",
    "print(\"=\" * 50)\n",
    "display(df_quality.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3263f40d",
   "metadata": {},
   "source": [
    "## Quality Visualizations / üìà Visualizaciones de Calidad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed54376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones de calidad de b√∫squeda\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Gr√°fico 1: Similitud promedio por modelo\n",
    "axes[0,0].bar(df_quality['Modelo'], df_quality['Similitud Promedio'], \n",
    "              color='gold', alpha=0.7)\n",
    "axes[0,0].set_title('üéØ Similitud Promedio de B√∫squedas', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_ylabel('Similitud Coseno')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "axes[0,0].set_ylim(0, 1)\n",
    "\n",
    "# Gr√°fico 2: Tiempo de b√∫squeda por modelo\n",
    "axes[0,1].bar(df_quality['Modelo'], df_quality['Tiempo B√∫squeda (s)'], \n",
    "              color='lightblue', alpha=0.7)\n",
    "axes[0,1].set_title('‚è±Ô∏è Tiempo de B√∫squeda', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_ylabel('Tiempo (segundos)')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Gr√°fico 3: Consultas por segundo\n",
    "axes[1,0].bar(df_quality['Modelo'], df_quality['Consultas/segundo'], \n",
    "              color='lightgreen', alpha=0.7)\n",
    "axes[1,0].set_title('üöÄ Rendimiento de Consultas', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_ylabel('Consultas por segundo')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Gr√°fico 4: Comparaci√≥n de dimensiones vs rendimiento\n",
    "colors = ['red' if d == 768 else 'blue' for d in df_performance['Dimensiones']]\n",
    "axes[1,1].scatter(df_performance['Dimensiones'], df_quality['Similitud Promedio'], \n",
    "                  c=colors, s=100, alpha=0.7)\n",
    "axes[1,1].set_title('üìä Dimensiones vs Calidad', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_xlabel('Dimensiones del Embedding')\n",
    "axes[1,1].set_ylabel('Similitud Promedio')\n",
    "axes[1,1].set_xlim(350, 800)\n",
    "\n",
    "# A√±adir etiquetas a los puntos\n",
    "for i, model in enumerate(df_performance['Modelo']):\n",
    "    axes[1,1].annotate(model, (df_performance['Dimensiones'].iloc[i], \n",
    "                               df_quality['Similitud Promedio'].iloc[i]), \n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar rankings\n",
    "print(\"üèÜ Rankings de Modelos:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"ü•á Mayor similitud: {df_quality.loc[df_quality['Similitud Promedio'].idxmax(), 'Modelo']}\")\n",
    "print(f\"ü•á B√∫squeda m√°s r√°pida: {df_quality.loc[df_quality['Tiempo B√∫squeda (s)'].idxmin(), 'Modelo']}\")\n",
    "print(f\"ü•á Mayor rendimiento: {df_quality.loc[df_quality['Consultas/segundo'].idxmax(), 'Modelo']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3311a230",
   "metadata": {},
   "source": [
    "## Qdrant Integration / üîó Integraci√≥n con Qdrant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1944efd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qdrant_collection(client: QdrantClient, collection_name: str, vector_size: int):\n",
    "    \"\"\"Crear colecci√≥n en Qdrant para un modelo espec√≠fico\"\"\"\n",
    "    try:\n",
    "        # Verificar si la colecci√≥n ya existe\n",
    "        collections = client.get_collections().collections\n",
    "        existing_collections = [col.name for col in collections]\n",
    "        \n",
    "        if collection_name in existing_collections:\n",
    "            print(f\"‚ö†Ô∏è Colecci√≥n '{collection_name}' ya existe, eliminando...\")\n",
    "            client.delete_collection(collection_name)\n",
    "        \n",
    "        # Crear nueva colecci√≥n\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\n",
    "        )\n",
    "        print(f\"‚úÖ Colecci√≥n '{collection_name}' creada exitosamente\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creando colecci√≥n '{collection_name}': {e}\")\n",
    "        return False\n",
    "\n",
    "def insert_documents_to_qdrant(client: QdrantClient, collection_name: str, \n",
    "                              texts: List[str], embeddings: np.ndarray):\n",
    "    \"\"\"Insertar documentos en Qdrant\"\"\"\n",
    "    try:\n",
    "        points = []\n",
    "        for i, (text, embedding) in enumerate(zip(texts, embeddings)):\n",
    "            point = PointStruct(\n",
    "                id=i,\n",
    "                vector=embedding.tolist(),\n",
    "                payload={\"text\": text, \"index\": i}\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Insertar puntos en lote\n",
    "        client.upsert(collection_name=collection_name, points=points)\n",
    "        print(f\"‚úÖ {len(points)} documentos insertados en '{collection_name}'\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error insertando documentos en '{collection_name}': {e}\")\n",
    "        return False\n",
    "\n",
    "def test_qdrant_search(client: QdrantClient, collection_name: str, \n",
    "                      query_embedding: List[float], limit: int = 3):\n",
    "    \"\"\"Probar b√∫squeda en Qdrant\"\"\"\n",
    "    try:\n",
    "        results = client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=query_embedding,\n",
    "            limit=limit\n",
    "        )\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en b√∫squeda de '{collection_name}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Probar integraci√≥n con Qdrant para el mejor modelo\n",
    "print(\"üîó Probando integraci√≥n con Qdrant...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Seleccionar el mejor modelo basado en similitud promedio\n",
    "best_model_key = df_quality.loc[df_quality['Similitud Promedio'].idxmax(), 'Modelo']\n",
    "best_model_data = model_performance[best_model_key]\n",
    "\n",
    "print(f\"üéØ Usando mejor modelo: {best_model_key}\")\n",
    "print(f\"üìä Dimensiones: {best_model_data['dimension']}\")\n",
    "\n",
    "# Crear colecci√≥n en Qdrant\n",
    "collection_name = f\"legal_texts_{best_model_key.replace('-', '_')}\"\n",
    "success = create_qdrant_collection(\n",
    "    evaluator.client, \n",
    "    collection_name, \n",
    "    best_model_data['dimension']\n",
    ")\n",
    "\n",
    "if success:\n",
    "    # Insertar documentos\n",
    "    insert_success = insert_documents_to_qdrant(\n",
    "        evaluator.client,\n",
    "        collection_name,\n",
    "        SAMPLE_LEGAL_TEXTS,\n",
    "        best_model_data['embeddings']\n",
    "    )\n",
    "    \n",
    "    if insert_success:\n",
    "        # Probar b√∫squeda con una consulta\n",
    "        test_query = SAMPLE_QUERIES[0]\n",
    "        test_query_embedding = best_model_data['model'].encode([test_query])[0]\n",
    "        \n",
    "        print(f\"\\nüîç Probando b√∫squeda con: '{test_query}'\")\n",
    "        search_results = test_qdrant_search(\n",
    "            evaluator.client,\n",
    "            collection_name,\n",
    "            test_query_embedding.tolist(),\n",
    "            limit=3\n",
    "        )\n",
    "        \n",
    "        if search_results:\n",
    "            print(\"‚úÖ Resultados de b√∫squeda en Qdrant:\")\n",
    "            for i, result in enumerate(search_results):\n",
    "                print(f\"{i+1}. Similitud: {result.score:.3f}\")\n",
    "                print(f\"   Texto: {result.payload['text']}\")\n",
    "                print()\n",
    "\n",
    "print(\"‚úÖ Integraci√≥n con Qdrant completada!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5f82c4",
   "metadata": {},
   "source": [
    "## Comprehensive Comparative Analysis / üìä An√°lisis Comparativo Completo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900d66b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear an√°lisis comparativo completo combinando rendimiento y calidad\n",
    "comparison_df = pd.merge(df_performance[['Modelo', 'Nombre Completo', 'Descripci√≥n', 'Dimensiones', \n",
    "                                        'Tiempo Carga (s)', 'Tiempo Embedding (s)', 'Velocidad (textos/s)']], \n",
    "                        df_quality[['Modelo', 'Similitud Promedio', 'Tiempo B√∫squeda (s)', 'Consultas/segundo']], \n",
    "                        on='Modelo')\n",
    "\n",
    "# Calcular puntuaciones normalizadas (0-1)\n",
    "def normalize_score(series, higher_is_better=True):\n",
    "    if higher_is_better:\n",
    "        return (series - series.min()) / (series.max() - series.min())\n",
    "    else:\n",
    "        return (series.max() - series) / (series.max() - series.min())\n",
    "\n",
    "# Calcular scores normalizados\n",
    "comparison_df['Score_Velocidad_Carga'] = normalize_score(comparison_df['Tiempo Carga (s)'], False)\n",
    "comparison_df['Score_Velocidad_Embedding'] = normalize_score(comparison_df['Tiempo Embedding (s)'], False)\n",
    "comparison_df['Score_Calidad'] = normalize_score(comparison_df['Similitud Promedio'], True)\n",
    "comparison_df['Score_Velocidad_Busqueda'] = normalize_score(comparison_df['Tiempo B√∫squeda (s)'], False)\n",
    "\n",
    "# Score final ponderado (puedes ajustar los pesos seg√∫n tus prioridades)\n",
    "weights = {\n",
    "    'calidad': 0.4,      # 40% - Lo m√°s importante para RAG\n",
    "    'velocidad_busqueda': 0.3,  # 30% - Importante para experiencia de usuario\n",
    "    'velocidad_embedding': 0.2,  # 20% - Importante para procesamiento batch\n",
    "    'velocidad_carga': 0.1       # 10% - Menos cr√≠tico (se hace una vez)\n",
    "}\n",
    "\n",
    "comparison_df['Score_Final'] = (\n",
    "    comparison_df['Score_Calidad'] * weights['calidad'] +\n",
    "    comparison_df['Score_Velocidad_Busqueda'] * weights['velocidad_busqueda'] +\n",
    "    comparison_df['Score_Velocidad_Embedding'] * weights['velocidad_embedding'] +\n",
    "    comparison_df['Score_Velocidad_Carga'] * weights['velocidad_carga']\n",
    ")\n",
    "\n",
    "# Ordenar por score final\n",
    "comparison_df = comparison_df.sort_values('Score_Final', ascending=False)\n",
    "\n",
    "print(\"üèÜ Ranking Final de Modelos de Embedding:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Pos':<4} {'Modelo':<25} {'Score Final':<12} {'Calidad':<8} {'Descripci√≥n':<30}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, row in comparison_df.iterrows():\n",
    "    pos = comparison_df.index.get_loc(i) + 1\n",
    "    print(f\"{pos:<4} {row['Modelo']:<25} {row['Score_Final']:.3f}        {row['Similitud Promedio']:.3f}    {row['Descripci√≥n']:<30}\")\n",
    "\n",
    "print(\"\\nüìä Tabla Comparativa Detallada:\")\n",
    "print(\"=\" * 50)\n",
    "display(comparison_df[['Modelo', 'Descripci√≥n', 'Dimensiones', 'Similitud Promedio', \n",
    "                      'Tiempo Carga (s)', 'Tiempo Embedding (s)', 'Tiempo B√∫squeda (s)', \n",
    "                      'Score_Final']].round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9574176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n del an√°lisis comparativo\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Gr√°fico 1: Score final por modelo\n",
    "bars = axes[0,0].bar(comparison_df['Modelo'], comparison_df['Score_Final'], \n",
    "                     color='purple', alpha=0.7)\n",
    "axes[0,0].set_title('üèÜ Score Final por Modelo', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_ylabel('Score Final (0-1)')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "axes[0,0].set_ylim(0, 1)\n",
    "\n",
    "# A√±adir valores en las barras\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[0,0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Gr√°fico 2: Comparaci√≥n de componentes del score\n",
    "score_components = comparison_df[['Modelo', 'Score_Calidad', 'Score_Velocidad_Busqueda', \n",
    "                                 'Score_Velocidad_Embedding', 'Score_Velocidad_Carga']].set_index('Modelo')\n",
    "score_components.plot(kind='bar', stacked=True, ax=axes[0,1], \n",
    "                     color=['gold', 'lightblue', 'lightcoral', 'lightgreen'])\n",
    "axes[0,1].set_title('üìä Componentes del Score', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_ylabel('Score Normalizado')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "axes[0,1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Gr√°fico 3: Scatter plot calidad vs velocidad\n",
    "axes[1,0].scatter(comparison_df['Similitud Promedio'], comparison_df['Consultas/segundo'], \n",
    "                  s=comparison_df['Score_Final']*200, alpha=0.7, c=comparison_df['Score_Final'], \n",
    "                  cmap='viridis')\n",
    "axes[1,0].set_title('üéØ Calidad vs Velocidad', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Similitud Promedio')\n",
    "axes[1,0].set_ylabel('Consultas/segundo')\n",
    "\n",
    "# A√±adir etiquetas\n",
    "for i, row in comparison_df.iterrows():\n",
    "    axes[1,0].annotate(row['Modelo'], (row['Similitud Promedio'], row['Consultas/segundo']), \n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "# Gr√°fico 4: Comparaci√≥n de tiempos\n",
    "time_data = comparison_df[['Modelo', 'Tiempo Carga (s)', 'Tiempo Embedding (s)', 'Tiempo B√∫squeda (s)']]\n",
    "time_data.set_index('Modelo').plot(kind='bar', ax=axes[1,1], \n",
    "                                  color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "axes[1,1].set_title('‚è±Ô∏è Comparaci√≥n de Tiempos', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_ylabel('Tiempo (segundos)')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276eb2f8",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations / üéØ Conclusiones y Recomendaciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b234dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar recomendaciones basadas en el an√°lisis\n",
    "def generate_recommendations(comparison_df: pd.DataFrame, weights: dict) -> dict:\n",
    "    \"\"\"Generar recomendaciones basadas en el an√°lisis comparativo\"\"\"\n",
    "    \n",
    "    best_overall = comparison_df.iloc[0]\n",
    "    best_quality = comparison_df.loc[comparison_df['Similitud Promedio'].idxmax()]\n",
    "    fastest_search = comparison_df.loc[comparison_df['Tiempo B√∫squeda (s)'].idxmin()]\n",
    "    fastest_embedding = comparison_df.loc[comparison_df['Tiempo Embedding (s)'].idxmin()]\n",
    "    \n",
    "    recommendations = {\n",
    "        'best_overall': {\n",
    "            'model': best_overall['Modelo'],\n",
    "            'score': best_overall['Score_Final'],\n",
    "            'reasoning': f\"Mejor balance general con score de {best_overall['Score_Final']:.3f}\"\n",
    "        },\n",
    "        'best_quality': {\n",
    "            'model': best_quality['Modelo'],\n",
    "            'similarity': best_quality['Similitud Promedio'],\n",
    "            'reasoning': f\"Mayor similitud promedio ({best_quality['Similitud Promedio']:.3f}) para aplicaciones que requieren alta precisi√≥n\"\n",
    "        },\n",
    "        'fastest_search': {\n",
    "            'model': fastest_search['Modelo'],\n",
    "            'search_time': fastest_search['Tiempo B√∫squeda (s)'],\n",
    "            'reasoning': f\"B√∫squeda m√°s r√°pida ({fastest_search['Tiempo B√∫squeda (s)']:.4f}s) para aplicaciones en tiempo real\"\n",
    "        },\n",
    "        'fastest_embedding': {\n",
    "            'model': fastest_embedding['Modelo'],\n",
    "            'embedding_time': fastest_embedding['Tiempo Embedding (s)'],\n",
    "            'reasoning': f\"Procesamiento m√°s r√°pido ({fastest_embedding['Tiempo Embedding (s)']:.2f}s) para carga masiva de datos\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Generar recomendaciones\n",
    "recommendations = generate_recommendations(comparison_df, weights)\n",
    "\n",
    "print(\"üéØ RECOMENDACIONES FINALES:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüèÜ MEJOR MODELO GENERAL:\")\n",
    "print(f\"   Modelo: {recommendations['best_overall']['model']}\")\n",
    "print(f\"   Score: {recommendations['best_overall']['score']:.3f}\")\n",
    "print(f\"   Raz√≥n: {recommendations['best_overall']['reasoning']}\")\n",
    "\n",
    "print(f\"\\nüéØ MEJOR CALIDAD DE B√öSQUEDA:\")\n",
    "print(f\"   Modelo: {recommendations['best_quality']['model']}\")\n",
    "print(f\"   Similitud: {recommendations['best_quality']['similarity']:.3f}\")\n",
    "print(f\"   Raz√≥n: {recommendations['best_quality']['reasoning']}\")\n",
    "\n",
    "print(f\"\\n‚ö° M√ÅS R√ÅPIDO PARA B√öSQUEDAS:\")\n",
    "print(f\"   Modelo: {recommendations['fastest_search']['model']}\")\n",
    "print(f\"   Tiempo: {recommendations['fastest_search']['search_time']:.4f}s\")\n",
    "print(f\"   Raz√≥n: {recommendations['fastest_search']['reasoning']}\")\n",
    "\n",
    "print(f\"\\nüöÄ M√ÅS R√ÅPIDO PARA EMBEDDINGS:\")\n",
    "print(f\"   Modelo: {recommendations['fastest_embedding']['model']}\")\n",
    "print(f\"   Tiempo: {recommendations['fastest_embedding']['embedding_time']:.2f}s\")\n",
    "print(f\"   Raz√≥n: {recommendations['fastest_embedding']['reasoning']}\")\n",
    "\n",
    "# An√°lisis de dimensiones\n",
    "dimension_analysis = comparison_df.groupby('Dimensiones').agg({\n",
    "    'Similitud Promedio': 'mean',\n",
    "    'Tiempo B√∫squeda (s)': 'mean',\n",
    "    'Score_Final': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "print(f\"\\nüìä AN√ÅLISIS POR DIMENSIONES:\")\n",
    "print(\"=\" * 40)\n",
    "print(dimension_analysis)\n",
    "\n",
    "print(f\"\\nüí° INSIGHTS CLAVE:\")\n",
    "print(\"=\" * 30)\n",
    "print(\"‚Ä¢ Los modelos de 384 dimensiones ofrecen mejor balance velocidad/calidad\")\n",
    "print(\"‚Ä¢ Los modelos de 768 dimensiones tienen mayor calidad pero menor velocidad\")\n",
    "print(\"‚Ä¢ La velocidad de b√∫squeda es m√°s cr√≠tica que la velocidad de embedding\")\n",
    "print(\"‚Ä¢ La similitud promedio es el factor m√°s importante para aplicaciones RAG\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa284fda",
   "metadata": {},
   "source": [
    "## Executive Summary / üìã Resumen Ejecutivo\n",
    "\n",
    "### Recommended Model for the Project / üéØ Modelo Recomendado para el Proyecto\n",
    "\n",
    "Based on the comprehensive analysis, it is recommended to use **{recommended_model}** as the main model for the RAG system for queries about Paraguayan labor law.\n",
    "\n",
    "### Selection Criteria / üìä Criterios de Selecci√≥n\n",
    "\n",
    "| Criterion | Weight | Description |\n",
    "|-----------|--------|-------------|\n",
    "| **Search Quality** | 40% | Average similarity in legal queries |\n",
    "| **Search Speed** | 30% | Response time for users |\n",
    "| **Embedding Speed** | 20% | Batch document processing |\n",
    "| **Loading Speed** | 10% | Model initialization time |\n",
    "\n",
    "### Next Steps / üîÑ Pr√≥ximos Pasos\n",
    "\n",
    "1. **Implement the selected model** in the production system\n",
    "2. **Configure Qdrant** with the corresponding dimensions\n",
    "3. **Run tests** with the complete labor law dataset\n",
    "4. **Monitor performance** in production and adjust if necessary\n",
    "5. **Consider alternative models** according to specific use case\n",
    "\n",
    "### Additional Considerations / ‚ö†Ô∏è Consideraciones Adicionales\n",
    "\n",
    "- **Computational resources**: Evaluate available GPU/CPU capacity\n",
    "- **Latency**: For real-time applications, prioritize search speed\n",
    "- **Accuracy**: For critical legal queries, prioritize similarity quality\n",
    "- **Scalability**: Consider the volume of documents and expected queries\n",
    "\n",
    "---\n",
    "\n",
    "Basado en el an√°lisis completo, se recomienda utilizar **{modelo_recomendado}** como modelo principal para el sistema RAG de consultas sobre la ley laboral paraguaya.\n",
    "\n",
    "### Criterios de Selecci√≥n\n",
    "\n",
    "| Criterio | Peso | Descripci√≥n |\n",
    "|----------|------|-------------|\n",
    "| **Calidad de B√∫squeda** | 40% | Similitud promedio en consultas legales |\n",
    "| **Velocidad de B√∫squeda** | 30% | Tiempo de respuesta para usuarios |\n",
    "| **Velocidad de Embedding** | 20% | Procesamiento de documentos batch |\n",
    "| **Velocidad de Carga** | 10% | Tiempo de inicializaci√≥n del modelo |\n",
    "\n",
    "### Pr√≥ximos Pasos\n",
    "\n",
    "1. **Implementar el modelo seleccionado** en el sistema de producci√≥n\n",
    "2. **Configurar Qdrant** con las dimensiones correspondientes\n",
    "3. **Realizar pruebas** con el dataset completo de la ley laboral\n",
    "4. **Monitorear rendimiento** en producci√≥n y ajustar si es necesario\n",
    "5. **Considerar modelos alternativos** seg√∫n el caso de uso espec√≠fico\n",
    "\n",
    "### Consideraciones Adicionales\n",
    "\n",
    "- **Recursos computacionales**: Evaluar capacidad de GPU/CPU disponible\n",
    "- **Latencia**: Para aplicaciones en tiempo real, priorizar velocidad de b√∫squeda\n",
    "- **Precisi√≥n**: Para consultas cr√≠ticas legales, priorizar calidad de similitud\n",
    "- **Escalabilidad**: Considerar el volumen de documentos y consultas esperadas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5157a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados del an√°lisis para referencia futura\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Preparar datos para guardar\n",
    "analysis_results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'models_evaluated': list(MODELS_TO_TEST.keys()),\n",
    "    'best_model_overall': recommendations['best_overall']['model'],\n",
    "    'best_model_quality': recommendations['best_quality']['model'],\n",
    "    'best_model_speed': recommendations['fastest_search']['model'],\n",
    "    'performance_summary': df_performance.to_dict('records'),\n",
    "    'quality_summary': df_quality.to_dict('records'),\n",
    "    'comparison_summary': comparison_df[['Modelo', 'Score_Final', 'Similitud Promedio', \n",
    "                                        'Tiempo B√∫squeda (s)', 'Dimensiones']].to_dict('records'),\n",
    "    'weights_used': weights,\n",
    "    'sample_texts_count': len(SAMPLE_LEGAL_TEXTS),\n",
    "    'sample_queries_count': len(SAMPLE_QUERIES)\n",
    "}\n",
    "\n",
    "# Guardar resultados\n",
    "results_file = 'embedding_analysis_results.json'\n",
    "with open(results_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(analysis_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"üíæ Resultados guardados en: {results_file}\")\n",
    "print(f\"üìÖ Fecha del an√°lisis: {analysis_results['timestamp']}\")\n",
    "print(f\"üèÜ Modelo recomendado: {recommendations['best_overall']['model']}\")\n",
    "\n",
    "# Mostrar resumen final\n",
    "print(f\"\\nüéâ AN√ÅLISIS COMPLETADO EXITOSAMENTE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"‚úÖ Modelos evaluados: {len(MODELS_TO_TEST)}\")\n",
    "print(f\"‚úÖ Textos de prueba: {len(SAMPLE_LEGAL_TEXTS)}\")\n",
    "print(f\"‚úÖ Consultas de prueba: {len(SAMPLE_QUERIES)}\")\n",
    "print(f\"‚úÖ Integraci√≥n Qdrant: Completada\")\n",
    "print(f\"‚úÖ Visualizaciones: Generadas\")\n",
    "print(f\"‚úÖ Recomendaciones: Generadas\")\n",
    "print(f\"‚úÖ Resultados: Guardados en {results_file}\")\n",
    "\n",
    "print(f\"\\nüöÄ El notebook est√° listo para ejecutarse y generar todas las m√©tricas!\")\n",
    "print(f\"üìù Recuerda ajustar las variables de entorno QDRANT_URL y QDRANT_API_KEY si es necesario.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2dc692",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0f26bbf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
